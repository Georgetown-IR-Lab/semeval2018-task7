<?xml version="1.0" encoding="UTF-8"?>
<doc>




<text id="L08-1450"><title>
A LAF/GrAF <entity id="L08-1450.1">based</entity> Encoding <entity id="L08-1450.2">Scheme</entity> for underspecified Representations of <entity id="L08-1450.3">syntactic</entity> Annotations.
</title><abstract><entity id="L08-1450.4">Data</entity> <entity id="L08-1450.5">models</entity> and encoding <entity id="L08-1450.6">formats</entity> for syntactically annotated <entity id="L08-1450.7">text</entity> <entity id="L08-1450.8">corpora</entity> need to <entity id="L08-1450.9">deal</entity> with <entity id="L08-1450.10">syntactic</entity> <entity id="L08-1450.11">ambiguity</entity>; underspecified <entity id="L08-1450.12">representations</entity> are particularly well suited for the <entity id="L08-1450.13">representation</entity> of ambiguous<entity id="L08-1450.14">data</entity> because they allow for high informational <entity id="L08-1450.15">efficiency</entity>. We discuss the <entity id="L08-1450.16">issue</entity> of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete <entity id="L08-1450.17">documentation</entity> of <entity id="L08-1450.18">linguistic analyses</entity>. The <entity id="L08-1450.19">main</entity> <entity id="L08-1450.20">topic</entity> of this article is a<entity id="L08-1450.21">data</entity> <entity id="L08-1450.22">model</entity> and an encoding <entity id="L08-1450.23">scheme</entity> <entity id="L08-1450.24">based</entity> on LAF/GrAF ( <entity id="L08-1450.25">Ide</entity> and  Romary, 2006 ; <entity id="L08-1450.26">Ide</entity> and  Suderman, 2007 ) which <entity id="L08-1450.27">provides</entity> a flexible <entity id="L08-1450.28">framework</entity> for encoding underspecified <entity id="L08-1450.29">representations</entity>. We show how a set of <entity id="L08-1450.30">dependency structures</entity> and a set of <entity id="L08-1450.31">TiGer</entity> graphs ( Brants et al., 2002 ) representing the readings of an ambiguous <entity id="L08-1450.32">sentence</entity> can be encoded, and we discuss <entity id="L08-1450.33">basic</entity> <entity id="L08-1450.34">issues</entity> in <entity id="L08-1450.35">querying</entity> <entity id="L08-1450.36">corpora</entity> which are encoded using the <entity id="L08-1450.37">framework</entity> presented here.
</abstract>


</text>

<text id="L08-1459"><title>
A <entity id="L08-1459.1">Study</entity> of Parentheticals in <entity id="L08-1459.2">Discourse</entity> <entity id="L08-1459.3">Corpora</entity> - Implications for NLG <entity id="L08-1459.4">Systems</entity></title><abstract>
This <entity id="L08-1459.5">paper</entity> presents a <entity id="L08-1459.6">corpus</entity> <entity id="L08-1459.7">study</entity> of parenthetical <entity id="L08-1459.8">constructions</entity> in two different <entity id="L08-1459.9">corpora</entity>: the Penn <entity id="L08-1459.10">Discourse</entity> Treebank (PDTB, (PDTB- Group, 2008 )) and the RST <entity id="L08-1459.11">Discourse</entity> Treebank ( Carlson et al., 2001 ). The <entity id="L08-1459.12">motivation</entity> for the <entity id="L08-1459.13">study</entity> is to <entity id="L08-1459.14">gain</entity> a better <entity id="L08-1459.15">understanding</entity> of the rhetorical <entity id="L08-1459.16">properties</entity> of parentheticals in <entity id="L08-1459.17">order</entity> to enable a <entity id="L08-1459.18">natural language generation system</entity> to produce parentheticals as <entity id="L08-1459.19">part</entity> of a rhetorically well-formed <entity id="L08-1459.20">output</entity>. We argue that there is a <entity id="L08-1459.21">correlation</entity> between <entity id="L08-1459.22">syntactic</entity> and rhetorical <entity id="L08-1459.23">types</entity> of parentheticals and establish two <entity id="L08-1459.24">main</entity> <entity id="L08-1459.25">categories</entity>: elaboration/<entity id="L08-1459.26">expansion-type</entity> <entity id="L08-1459.27">NP-modifier</entity> parentheticals and non-elaboration/<entity id="L08-1459.28">expansion-type</entity> VP- or S-<entity id="L08-1459.29">modifier</entity> parentheticals. We show several <entity id="L08-1459.30">strategies</entity> for <entity id="L08-1459.31">extracting</entity> these from the two <entity id="L08-1459.32">corpora</entity> and discuss how the seemingly contradictory <entity id="L08-1459.33">results</entity> obtained can be reconciled in light of the rhetorical and <entity id="L08-1459.34">syntactic</entity> <entity id="L08-1459.35">properties</entity> of parentheticals as well as the <entity id="L08-1459.36">decisions</entity> taken in the annotation <entity id="L08-1459.37">guidelines</entity>.
</abstract>


</text>

<text id="I05-2027"><title>
<entity id="I05-2027.1">Machine Learning Approach</entity> to Augmenting <entity id="I05-2027.2">News</entity> <entity id="I05-2027.3">Headline</entity> <entity id="I05-2027.4">Generation</entity>
</title><abstract>In this <entity id="I05-2027.5">paper</entity>, we present the HybridTrim <entity id="I05-2027.6">system</entity> which uses a <entity id="I05-2027.7">machine</entity> learning <entity id="I05-2027.8">technique</entity> to combine linguistic, <entity id="I05-2027.9">statistical</entity> and positional <entity id="I05-2027.10">information</entity> to identify <entity id="I05-2027.11">topic</entity> labels for <entity id="I05-2027.12">headlines</entity> in a <entity id="I05-2027.13">text</entity>. We compare our <entity id="I05-2027.14">system</entity> with the Topiary <entity id="I05-2027.15">system</entity> which, in <entity id="I05-2027.16">contrast</entity>, uses a <entity id="I05-2027.17">statistical</entity> <entity id="I05-2027.18">learning approach</entity> to finding <entity id="I05-2027.19">topic</entity> descriptors for <entity id="I05-2027.20">headlines</entity>. The Topiary <entity id="I05-2027.21">system</entity>, <entity id="I05-2027.22">developed</entity> at the <entity id="I05-2027.23">University</entity> of Maryland with BBN, was the top performing <entity id="I05-2027.24">headline</entity> <entity id="I05-2027.25">generation system</entity> at DUC 2004. Topiary-style <entity id="I05-2027.26">headlines</entity> consist of a <entity id="I05-2027.27">number</entity> of general <entity id="I05-2027.28">topic</entity> labels followed by a compressed <entity id="I05-2027.29">version</entity> of the lead <entity id="I05-2027.30">sentence</entity> of a <entity id="I05-2027.31">news</entity> story. The Topiary <entity id="I05-2027.32">system</entity> uses a <entity id="I05-2027.33">statistical</entity> <entity id="I05-2027.34">learning approach</entity> to finding <entity id="I05-2027.35">topic</entity> labels. The <entity id="I05-2027.36">performance</entity> of these <entity id="I05-2027.37">systems</entity> is <entity id="I05-2027.38">evaluated</entity> using the <entity id="I05-2027.39">ROUGE</entity> <entity id="I05-2027.40">evaluation</entity> <entity id="I05-2027.41">suite</entity> on the DUC 2004 <entity id="I05-2027.42">news</entity> stories <entity id="I05-2027.43">collection</entity>.
</abstract>


</text>

<text id="N03-1014"><title>
Inducing History Representations For Broad <entity id="N03-1014.1">Coverage</entity> <entity id="N03-1014.2">Statistical</entity> <entity id="N03-1014.3">Parsing</entity></title><abstract>
We present a <entity id="N03-1014.4">neural network</entity> <entity id="N03-1014.5">method</entity> for inducing <entity id="N03-1014.6">representations</entity> of <entity id="N03-1014.7">parse</entity> histories and using these history <entity id="N03-1014.8">representations</entity> to estimate the <entity id="N03-1014.9">probabilities</entity> needed by a <entity id="N03-1014.10">statistical</entity> left-corner <entity id="N03-1014.11">parser</entity>. The <entity id="N03-1014.12">resulting</entity> <entity id="N03-1014.13">statistical</entity> <entity id="N03-1014.14">parser</entity> achieves <entity id="N03-1014.15">performance</entity> (89.1% F-measure) on the <entity id="N03-1014.16">Penn Treebank</entity> which is only 0.6% below the best <entity id="N03-1014.17">current</entity> <entity id="N03-1014.18">parser</entity> for this <entity id="N03-1014.19">task</entity>, despite using a smaller <entity id="N03-1014.20">vocabulary</entity> <entity id="N03-1014.21">size</entity> and less prior <entity id="N03-1014.22">linguistic knowledge</entity>. Crucial to this <entity id="N03-1014.23">success</entity> is the use of structurally determined soft <entity id="N03-1014.24">biases</entity> in inducing the <entity id="N03-1014.25">representation</entity> of the <entity id="N03-1014.26">parse</entity> history, and no use of hard <entity id="N03-1014.27">independence</entity> <entity id="N03-1014.28">assumptions</entity>.
</abstract>


</text>

<text id="N03-2021"><title><entity id="N03-2021.1">Precision</entity> And <entity id="N03-2021.2">Recall</entity> Of <entity id="N03-2021.3">Machine Translation</entity></title>
<abstract><entity id="N03-2021.4">Machine translation</entity> can be <entity id="N03-2021.5">evaluated</entity> using <entity id="N03-2021.6">precision</entity>, <entity id="N03-2021.7">recall</entity>, and the F-measure. These <entity id="N03-2021.8">standard</entity> measures have significantly higher <entity id="N03-2021.9">correlation</entity> with human <entity id="N03-2021.10">judgments</entity> than recently <entity id="N03-2021.11">proposed</entity> <entity id="N03-2021.12">alternatives</entity>. More importantly, the <entity id="N03-2021.13">standard</entity> measures have an intuitive <entity id="N03-2021.14">interpretation</entity>, which can facilitate <entity id="N03-2021.15">insights</entity> into how <entity id="N03-2021.16">MT systems</entity> might be <entity id="N03-2021.17">improved</entity>. The relevant <entity id="N03-2021.18">software</entity> is publicly available.
</abstract>


</text>

<text id="N06-1042"><title><entity id="N06-1042.1">Learning</entity> Morphological <entity id="N06-1042.2">Disambiguation</entity> Rules For Turkish
</title><abstract>
In this <entity id="N06-1042.3">paper</entity>, we present a <entity id="N06-1042.4">rule</entity> <entity id="N06-1042.5">based</entity> <entity id="N06-1042.6">model</entity> for morphological <entity id="N06-1042.7">disambiguation</entity> of Turkish. The <entity id="N06-1042.8">rules</entity> are <entity id="N06-1042.9">generated</entity> by a novel <entity id="N06-1042.10">decision</entity> <entity id="N06-1042.11">list</entity> learning <entity id="N06-1042.12">algorithm</entity> using supervised <entity id="N06-1042.13">training</entity>. Morphological <entity id="N06-1042.14">ambiguity</entity> (e.g. lives = live+s or life+s) is a <entity id="N06-1042.15">challenging</entity> <entity id="N06-1042.16">problem</entity> for agglutinative <entity id="N06-1042.17">languages</entity> like Turkish where close to half of the <entity id="N06-1042.18">words</entity> in running <entity id="N06-1042.19">text</entity> are morphologically ambiguous. Furthermore, it is possible for a <entity id="N06-1042.20">word</entity> to take an unlimited <entity id="N06-1042.21">number</entity> of <entity id="N06-1042.22">suffixes</entity>, therefore the <entity id="N06-1042.23">number</entity> of possible morphological <entity id="N06-1042.24">tags</entity> is unlimited. We attempted to cope with these <entity id="N06-1042.25">problems</entity> by <entity id="N06-1042.26">training</entity> a separate <entity id="N06-1042.27">model</entity> for each of the 126 <entity id="N06-1042.28">morphological features</entity> recognized by the morphological <entity id="N06-1042.29">analyzer</entity>. The <entity id="N06-1042.30">resulting</entity> <entity id="N06-1042.31">decision</entity> <entity id="N06-1042.32">lists</entity> independently vote on each of the potential <entity id="N06-1042.33">parses</entity> of a <entity id="N06-1042.34">word</entity> and the final <entity id="N06-1042.35">parse</entity> is selected <entity id="N06-1042.36">based</entity> on our <entity id="N06-1042.37">confidence</entity> on these votes. The <entity id="N06-1042.38">accuracy</entity> of our <entity id="N06-1042.39">model</entity> (96%) is slightly above the best previously <entity id="N06-1042.40">reported</entity> <entity id="N06-1042.41">results</entity> which use <entity id="N06-1042.42">statistical models</entity>. For <entity id="N06-1042.43">comparison</entity>, when we <entity id="N06-1042.44">train</entity> a single <entity id="N06-1042.45">decision</entity> <entity id="N06-1042.46">list</entity> on full <entity id="N06-1042.47">tags</entity> instead of using separate <entity id="N06-1042.48">models</entity> on each <entity id="N06-1042.49">feature</entity> we get 91% <entity id="N06-1042.50">accuracy</entity>.
</abstract>


</text>

<text id="M92-1018"><title>
SRA SOLOMON: MUC-4 <entity id="M92-1018.1">Test</entity> <entity id="M92-1018.2">Results</entity> And <entity id="M92-1018.3">Analysis</entity></title><abstract>
In this <entity id="M92-1018.4">paper</entity>, we <entity id="M92-1018.5">report</entity> SRA's <entity id="M92-1018.6">results</entity> on the MUC-4 <entity id="M92-1018.7">task</entity> and describe how we <entity id="M92-1018.8">trained</entity> our <entity id="M92-1018.9">natural language processing system</entity> for MUC-4. We also <entity id="M92-1018.10">report</entity> on what worked, what didn't work, and lessons learned. Our MUC-4 <entity id="M92-1018.11">system</entity> embeds the SOLOMON <entity id="M92-1018.12">knowledge-based</entity> NLP shell which is <entity id="M92-1018.13">designed</entity> for both and We are currently using SOLOMON for a Spanish and <entity id="M92-1018.14">Japanese</entity> <entity id="M92-1018.15">text</entity> <entity id="M92-1018.16">understanding</entity> <entity id="M92-1018.17">project</entity> in a different <entity id="M92-1018.18">domain</entity>. Although this was our first year participating in MUC, we have built and are currently building other<entity id="M92-1018.19">data</entity> <entity id="M92-1018.20">extraction systems</entity>.
</abstract>


</text>

<text id="E95-1014"><title><entity id="E95-1014.1">Corpus-</entity><entity id="E95-1014.2">Based</entity> <entity id="E95-1014.3">Method</entity> For <entity id="E95-1014.4">Automatic</entity> <entity id="E95-1014.5">Identification</entity> Of <entity id="E95-1014.6">Support</entity> Verbs For Nominalizations
</title><abstract>
Nominalization is a highly productive <entity id="E95-1014.7">phenomena</entity> in most <entity id="E95-1014.8">languages</entity>. The <entity id="E95-1014.9">process</entity> of nominalization ejects a <entity id="E95-1014.10">verb</entity> from its <entity id="E95-1014.11">syntactic</entity> <entity id="E95-1014.12">role</entity> into a nominal position. The original <entity id="E95-1014.13">verb</entity> is often replaced by a semantically emptied <entity id="E95-1014.14">support</entity> <entity id="E95-1014.15">verb</entity> (e.g., make a <entity id="E95-1014.16">proposal</entity>).
</abstract>


</text>

<text id="A00-1030"><title>
Aggressive <entity id="A00-1030.1">Morphology</entity> For <entity id="A00-1030.2">Robust</entity> <entity id="A00-1030.3">Lexical</entity> <entity id="A00-1030.4">Coverage</entity></title><abstract>
This <entity id="A00-1030.5">paper</entity> describes an <entity id="A00-1030.6">approach</entity> to <entity id="A00-1030.7">providing</entity> <entity id="A00-1030.8">lexical information</entity> for <entity id="A00-1030.9">natural language processing</entity> in unrestricted <entity id="A00-1030.10">domains</entity>. A <entity id="A00-1030.11">system</entity> of approximately 1200 morphological <entity id="A00-1030.12">rules</entity> is used to extend a <entity id="A00-1030.13">core</entity> <entity id="A00-1030.14">lexicon</entity> of 39,000 <entity id="A00-1030.15">words</entity> to <entity id="A00-1030.16">provide</entity> <entity id="A00-1030.17">lexical</entity> <entity id="A00-1030.18">coverage</entity> that exceeds that of a <entity id="A00-1030.19">lexicon</entity> of 80,000 <entity id="A00-1030.20">words</entity> or 150,000 <entity id="A00-1030.21">word</entity> <entity id="A00-1030.22">forms</entity>. The morphological <entity id="A00-1030.23">system</entity> is described, and <entity id="A00-1030.24">lexical</entity> <entity id="A00-1030.25">coverage</entity> is <entity id="A00-1030.26">evaluated</entity> for random <entity id="A00-1030.27">words</entity> chosen from a previously unanalyzed <entity id="A00-1030.28">corpus</entity>.
</abstract>


</text>

<text id="A00-2029"><title>
Predicting <entity id="A00-2029.1">Automatic Speech Recognition</entity> <entity id="A00-2029.2">Performance</entity> Using Prosodic Cues
</title><abstract>
In spoken <entity id="A00-2029.3">dialogue systems</entity>, it is important for a <entity id="A00-2029.4">system</entity> to know how likely a <entity id="A00-2029.5">speech recognition</entity> <entity id="A00-2029.6">hypothesis</entity> is to be correct, so it can reprompt for fresh <entity id="A00-2029.7">input</entity>, or, in <entity id="A00-2029.8">cases</entity> where many <entity id="A00-2029.9">errors</entity> have occurred, change its <entity id="A00-2029.10">interaction</entity> <entity id="A00-2029.11">strategy</entity> or switch the caller to a human attendant. We have discovered prosodie <entity id="A00-2029.12">features</entity> which more accurately predict when a <entity id="A00-2029.13">recognition</entity> <entity id="A00-2029.14">hypothesis</entity> contains a <entity id="A00-2029.15">word</entity> <entity id="A00-2029.16">error</entity> than the acoustic <entity id="A00-2029.17">confidence</entity> score <entity id="A00-2029.18">thresholds</entity> traditionally used in <entity id="A00-2029.19">automatic speech recognition</entity>. We present analytic <entity id="A00-2029.20">results</entity> indicating that there are significant prosodie <entity id="A00-2029.21">differences</entity> between correctly and incorrectly recognized turns.
</abstract>


</text>

<text id="H92-1099"><title><entity id="H92-1099.1">Evaluating</entity> The Use Of Prosodic <entity id="H92-1099.2">Information</entity> In <entity id="H92-1099.3">Speech Recognition</entity> And <entity id="H92-1099.4">Understanding</entity></title><abstract>
The <entity id="H92-1099.5">goal</entity> of this <entity id="H92-1099.6">project</entity> is to investigate the use of different <entity id="H92-1099.7">levels</entity> of prosodie <entity id="H92-1099.8">information</entity> in <entity id="H92-1099.9">speech recognition</entity> and <entity id="H92-1099.10">understanding</entity>. In particular, the <entity id="H92-1099.11">current</entity> <entity id="H92-1099.12">focus</entity> of the work is the use of prosodie <entity id="H92-1099.13">phrase</entity> <entity id="H92-1099.14">boundary</entity> <entity id="H92-1099.15">information</entity> in <entity id="H92-1099.16">parsing</entity>. The <entity id="H92-1099.17">research</entity> involves determining a <entity id="H92-1099.18">representation</entity> of prosodie <entity id="H92-1099.19">information</entity> suitable for use in a <entity id="H92-1099.20">speech understanding system</entity>, <entity id="H92-1099.21">developing</entity> reliable <entity id="H92-1099.22">algorithms</entity> for <entity id="H92-1099.23">detection</entity> of the prosodie <entity id="H92-1099.24">cues</entity> in <entity id="H92-1099.25">speech</entity>, investigating <entity id="H92-1099.26">architectures</entity> for integrating prosodie <entity id="H92-1099.27">cues</entity> in a <entity id="H92-1099.28">parser</entity>, and <entity id="H92-1099.29">evaluating</entity> the potential <entity id="H92-1099.30">improvements</entity> of <entity id="H92-1099.31">prosody</entity> in the <entity id="H92-1099.32">context</entity> of the SRI <entity id="H92-1099.33">Spoken Language System</entity>. This <entity id="H92-1099.34">research</entity> is sponsored jointly by DARPA and NSF.
</abstract>


</text>

<text id="H93-1048"><title><entity id="H93-1048.1">Prediction</entity> Of Lexicalized <entity id="H93-1048.2">Tree</entity> Fragments In <entity id="H93-1048.3">Text</entity></title><abstract>
There is a <entity id="H93-1048.4">mismatch</entity> between the <entity id="H93-1048.5">distribution</entity> of <entity id="H93-1048.6">information</entity> in <entity id="H93-1048.7">text</entity>, and a <entity id="H93-1048.8">variety</entity> of grammatical <entity id="H93-1048.9">formalisms</entity> for describing it, <entity id="H93-1048.10">including</entity> ngrams, <entity id="H93-1048.11">context-free</entity> grammars, and <entity id="H93-1048.12">dependency grammars</entity>. Rather than adding <entity id="H93-1048.13">probabilities</entity> to existing grammars, it is <entity id="H93-1048.14">proposed</entity> to collect the <entity id="H93-1048.15">distributions</entity> of flexibly <entity id="H93-1048.16">sized</entity> <entity id="H93-1048.17">partial</entity> <entity id="H93-1048.18">trees</entity>. These can be used to enhance an ngram <entity id="H93-1048.19">model</entity>, and in analogical <entity id="H93-1048.20">parsing</entity>.
</abstract>


</text>

<text id="H93-1060"><title>
The COMLEX <entity id="H93-1060.1">Syntax</entity> <entity id="H93-1060.2">Project</entity></title><abstract>
"<entity id="H93-1060.3">Developing</entity> more shareable <entity id="H93-1060.4">resources</entity> to <entity id="H93-1060.5">support</entity> <entity id="H93-1060.6">natural language</entity> <entity id="H93-1060.7">analysis</entity> will make it easier and cheaper to create new <entity id="H93-1060.8">language processing</entity> <entity id="H93-1060.9">applications</entity> and to <entity id="H93-1060.10">support</entity> <entity id="H93-1060.11">research</entity> in <entity id="H93-1060.12">computational linguistics</entity>. One <entity id="H93-1060.13">natural</entity> <entity id="H93-1060.14">candidate</entity> for such a <entity id="H93-1060.15">resource</entity> is a <entity id="H93-1060.16">broad-coverage</entity> <entity id="H93-1060.17">dictionary</entity>, since the work <entity id="H93-1060.18">required</entity> to create such a <entity id="H93-1060.19">dictionary</entity> is large but there is general <entity id="H93-1060.20">agreement</entity> on at least some of the <entity id="H93-1060.21">information</entity> to be <entity id="H93-1060.22">recorded</entity> for each <entity id="H93-1060.23">word</entity>. The Linguistic <entity id="H93-1060.24">Data</entity> Consortium has begun an <entity id="H93-1060.25">effort</entity> to create several such <entity id="H93-1060.26">lexical resources</entity>, under the rubric ""COMLEX"" (<entity id="H93-1060.27">COMmon</entity> <entity id="H93-1060.28">LEXicon</entity>); one of these <entity id="H93-1060.29">projects</entity> is the COMLEX <entity id="H93-1060.30">Syntax</entity> <entity id="H93-1060.31">Project</entity>. The <entity id="H93-1060.32">goal</entity> of the COMLEX <entity id="H93-1060.33">Syntax</entity> <entity id="H93-1060.34">Project</entity> is to create a <entity id="H93-1060.35">moderately-broad-coverage</entity> shareable <entity id="H93-1060.36">dictionary</entity> containing the <entity id="H93-1060.37">syntactic features</entity> of <entity id="H93-1060.38">English</entity> <entity id="H93-1060.39">words</entity>, intended for <entity id="H93-1060.40">automatic</entity> <entity id="H93-1060.41">language analysis</entity>. We are initially aiming for a <entity id="H93-1060.42">dictionary</entity> of 35,000 to 40,000 <entity id="H93-1060.43">base</entity> <entity id="H93-1060.44">forms</entity>, although this of course may be enlarged if the initial <entity id="H93-1060.45">effort</entity> is positively received. The <entity id="H93-1060.46">dictionary</entity> should <entity id="H93-1060.47">include</entity> detailed <entity id="H93-1060.48">syntactic</entity> <entity id="H93-1060.49">specifications</entity>, particularly for subcategorization; our intent is to <entity id="H93-1060.50">provide</entity> sufficient <entity id="H93-1060.51">detail</entity> so that the <entity id="H93-1060.52">information</entity> <entity id="H93-1060.53">required</entity> by a <entity id="H93-1060.54">number</entity> of major <entity id="H93-1060.55">English</entity> <entity id="H93-1060.56">analyzers</entity> can be automatically derived from the <entity id="H93-1060.57">information</entity> we <entity id="H93-1060.58">provide</entity>. As with other Linguistic <entity id="H93-1060.59">Data</entity> Consortium <entity id="H93-1060.60">resources</entity>, our intent is to <entity id="H93-1060.61">provide</entity> a <entity id="H93-1060.62">lexicon</entity> available without <entity id="H93-1060.63">license</entity> <entity id="H93-1060.64">constraint</entity> to all Consortium members. Finally, our <entity id="H93-1060.65">goal</entity> is to <entity id="H93-1060.66">provide</entity> an initial <entity id="H93-1060.67">lexicon</entity> relatively quickly 
 within about a year, funding permitting. This implies a certain <entity id="H93-1060.68">flexibility</entity>, where some of the <entity id="H93-1060.69">features</entity> will probably be changed and refined as the <entity id="H93-1060.70">coding</entity> is taking place. "
</abstract>


</text>

<text id="A97-1008"><title>
An <entity id="A97-1008.1">Evaluation</entity> Of <entity id="A97-1008.2">Strategies</entity> For Selective <entity id="A97-1008.3">Utterance</entity> <entity id="A97-1008.4">Verification</entity> For Spoken <entity id="A97-1008.5">Natural Language</entity> <entity id="A97-1008.6">Dialog</entity></title><abstract>
As with human-human <entity id="A97-1008.7">interaction</entity>, spoken <entity id="A97-1008.8">human-computer</entity> <entity id="A97-1008.9">dialog</entity> will contain <entity id="A97-1008.10">situations</entity> where there is miscommunication. In <entity id="A97-1008.11">experimental</entity> trials consisting of eight different <entity id="A97-1008.12">users</entity>, 141 <entity id="A97-1008.13">problem-solving</entity> <entity id="A97-1008.14">dialogs</entity>, and 2840 <entity id="A97-1008.15">user</entity> <entity id="A97-1008.16">utterances</entity>, the Circuit Fix-It Shop <entity id="A97-1008.17">natural language</entity> <entity id="A97-1008.18">dialog system</entity> misinterpreted 18.5% of <entity id="A97-1008.19">user</entity> <entity id="A97-1008.20">utterances</entity>. These miscommunications created various <entity id="A97-1008.21">problems</entity> for the <entity id="A97-1008.22">dialog</entity> <entity id="A97-1008.23">interaction</entity>, ranging from repetitive <entity id="A97-1008.24">dialog</entity> to experimenter intervention to occasional failure of the <entity id="A97-1008.25">dialog</entity>. One <entity id="A97-1008.26">natural</entity> <entity id="A97-1008.27">strategy</entity> for reducing the <entity id="A97-1008.28">impact</entity> of miscommunication is selective <entity id="A97-1008.29">verification</entity> of the <entity id="A97-1008.30">user</entity>'s <entity id="A97-1008.31">utterances</entity>. This <entity id="A97-1008.32">paper</entity> <entity id="A97-1008.33">reports</entity> on both <entity id="A97-1008.34">context-independent</entity> and <entity id="A97-1008.35">context-dependent</entity> <entity id="A97-1008.36">strategies</entity> for <entity id="A97-1008.37">utterance</entity> <entity id="A97-1008.38">verification</entity> that show that the use of <entity id="A97-1008.39">dialog</entity> <entity id="A97-1008.40">context</entity> is crucial for intelligent <entity id="A97-1008.41">selection</entity> of which <entity id="A97-1008.42">utterances</entity> to verify.
</abstract>


</text>

<text id="W97-1206"><title><entity id="W97-1206.1">Computing</entity> Prosodic Properties In A <entity id="W97-1206.2">Data-</entity>To-<entity id="W97-1206.3">Speech</entity> <entity id="W97-1206.4">System</entity></title><abstract>
We <entity id="W97-1206.5">propose</entity> a set of <entity id="W97-1206.6">rules</entity> for the <entity id="W97-1206.7">computation</entity> of <entity id="W97-1206.8">prosody</entity> which are <entity id="W97-1206.9">implemented</entity> in an existing generic <entity id="W97-1206.10">Data-to-</entity><entity id="W97-1206.11">Speech</entity> <entity id="W97-1206.12">system</entity>. The <entity id="W97-1206.13">rules</entity> make crucial use of both <entity id="W97-1206.14">sentence-internal</entity> and <entity id="W97-1206.15">sentence-external</entity> <entity id="W97-1206.16">semantic</entity> and <entity id="W97-1206.17">syntactic information</entity> <entity id="W97-1206.18">provided</entity> by the <entity id="W97-1206.19">system</entity>. In a <entity id="W97-1206.20">Text-to-</entity><entity id="W97-1206.21">Speech</entity> <entity id="W97-1206.22">system</entity>, this <entity id="W97-1206.23">information</entity> would have to be obtained through <entity id="W97-1206.24">text analysis</entity>, but in <entity id="W97-1206.25">Data-to-</entity><entity id="W97-1206.26">Speech</entity> it is readily available, and its reliable and detailed character makes it possible to <entity id="W97-1206.27">compute</entity> the prosodie <entity id="W97-1206.28">properties</entity> of <entity id="W97-1206.29">generated</entity> <entity id="W97-1206.30">sentences</entity> in a sophisticated way. This in turn allows for a close <entity id="W97-1206.31">control</entity> of prosodie <entity id="W97-1206.32">realization</entity>, <entity id="W97-1206.33">resulting</entity> in <entity id="W97-1206.34">natural-sounding</entity> <entity id="W97-1206.35">intonation</entity>.
</abstract>


</text>

<text id="W99-0305"><title>
Standardisation <entity id="W99-0305.1">Efforts</entity> On The <entity id="W99-0305.2">Level</entity> Of <entity id="W99-0305.3">Dialogue Act</entity> In The <entity id="W99-0305.4">MATE</entity> <entity id="W99-0305.5">Project</entity></title><abstract>
This <entity id="W99-0305.6">paper</entity> describes the state of the art of <entity id="W99-0305.7">coding</entity> <entity id="W99-0305.8">schemes</entity> for <entity id="W99-0305.9">dialogue acts</entity> and the <entity id="W99-0305.10">efforts</entity> to establish a <entity id="W99-0305.11">standard</entity> in this <entity id="W99-0305.12">field</entity>. We present a <entity id="W99-0305.13">review</entity> and <entity id="W99-0305.14">comparison</entity> of currently available <entity id="W99-0305.15">schemes</entity> and <entity id="W99-0305.16">outline</entity> the <entity id="W99-0305.17">comparison</entity> <entity id="W99-0305.18">problems</entity> we had <entity id="W99-0305.19">due</entity> to <entity id="W99-0305.20">domain</entity>, <entity id="W99-0305.21">task</entity>, and <entity id="W99-0305.22">language</entity> <entity id="W99-0305.23">dependencies</entity> of <entity id="W99-0305.24">schemes</entity>. We discuss <entity id="W99-0305.25">solution</entity> <entity id="W99-0305.26">strategies</entity> which have in mind the reusability of <entity id="W99-0305.27">corpora</entity>. Reusability is a crucial point because production and annotation of <entity id="W99-0305.28">corpora</entity> is very <entity id="W99-0305.29">time</entity> and <entity id="W99-0305.30">cost</entity> consuming but the <entity id="W99-0305.31">current</entity> broad <entity id="W99-0305.32">variety</entity> of <entity id="W99-0305.33">schemes</entity> makes reusability of annotated <entity id="W99-0305.34">corpora</entity> very hard. The work of this <entity id="W99-0305.35">paper</entity> takes place in the <entity id="W99-0305.36">framework</entity> of the European Union funded <entity id="W99-0305.37">MATE</entity> <entity id="W99-0305.38">project</entity>. <entity id="W99-0305.39">MATE</entity> aims to <entity id="W99-0305.40">develop</entity> general methodological <entity id="W99-0305.41">guidelines</entity> for the <entity id="W99-0305.42">creation</entity>, annotation, <entity id="W99-0305.43">retrieval</entity> and <entity id="W99-0305.44">analysis</entity> of annotated <entity id="W99-0305.45">corpora</entity>.
</abstract>


</text>

<text id="W00-1002"><title>
ADAM - An <entity id="W00-1002.1">Architecture</entity> For XML-Based <entity id="W00-1002.2">Dialogue</entity> Annotation On Multiple Levels
</title><abstract>
In this <entity id="W00-1002.3">paper</entity> annotation modularity and use of annotation meta-schemes are identified as <entity id="W00-1002.4">basic</entity> <entity id="W00-1002.5">requirements</entity> for achieving actual <entity id="W00-1002.6">corpora</entity> reusability. We discuss these <entity id="W00-1002.7">concepts</entity> and the way they are <entity id="W00-1002.8">implemented</entity> in the architectural <entity id="W00-1002.9">framework</entity> of the ADAM <entity id="W00-1002.10">corpus</entity>, which is a <entity id="W00-1002.11">corpus</entity> of 450 Italian spontaneous <entity id="W00-1002.12">dialogues</entity>. The <entity id="W00-1002.13">design</entity> of ADAM <entity id="W00-1002.14">architecture</entity> is compatible with as many <entity id="W00-1002.15">practices</entity> of <entity id="W00-1002.16">dialogue</entity> annotation as possible, as well as <entity id="W00-1002.17">approaches</entity> to annotation at different <entity id="W00-1002.18">levels</entity>.
</abstract>


</text>

<text id="W00-1213"><title>
Annotating <entity id="W00-1213.1">Information</entity> <entity id="W00-1213.2">Structures</entity> In <entity id="W00-1213.3">Chinese</entity> <entity id="W00-1213.4">Texts</entity> Using HowNet
</title><abstract>
This <entity id="W00-1213.5">paper</entity> <entity id="W00-1213.6">reported</entity> our work on annotating <entity id="W00-1213.7">Chinese</entity> <entity id="W00-1213.8">texts</entity> with <entity id="W00-1213.9">information structures</entity> derived from HowNet. An <entity id="W00-1213.10">information structure</entity> consists of two <entity id="W00-1213.11">components</entity>: HowNet <entity id="W00-1213.12">definitions</entity> and <entity id="W00-1213.13">dependency relations</entity>. It is the <entity id="W00-1213.14">unit</entity> of <entity id="W00-1213.15">representation</entity> of the meaning of <entity id="W00-1213.16">texts</entity>. This work is <entity id="W00-1213.17">part</entity> of a multi-sentential <entity id="W00-1213.18">approach</entity> to <entity id="W00-1213.19">Chinese</entity> <entity id="W00-1213.20">text</entity> <entity id="W00-1213.21">understanding</entity>. An <entity id="W00-1213.22">overview</entity> of HowNet and <entity id="W00-1213.23">information structure</entity> are described in this <entity id="W00-1213.24">paper</entity>.
</abstract>


</text>

<text id="W02-0221"><title><entity id="W02-0221.1">Training</entity> A <entity id="W02-0221.2">Dialogue Act</entity> Tagger For Human-Human And Human-<entity id="W02-0221.3">Computer</entity> Travel Dialogues
</title><abstract>
While <entity id="W02-0221.4">dialogue acts</entity> <entity id="W02-0221.5">provide</entity> a useful <entity id="W02-0221.6">schema</entity> for characterizing <entity id="W02-0221.7">dialogue</entity> <entity id="W02-0221.8">behaviors</entity> in <entity id="W02-0221.9">human-computer</entity> and human-human <entity id="W02-0221.10">dialogues</entity>, their <entity id="W02-0221.11">utility</entity> is <entity id="W02-0221.12">limited</entity> by the huge <entity id="W02-0221.13">effort</entity> involved in <entity id="W02-0221.14">hand-labelling</entity> <entity id="W02-0221.15">dialogues</entity> with a <entity id="W02-0221.16">dialogue act</entity> labelling <entity id="W02-0221.17">scheme</entity>. In this work, we examine whether it is possible to fully automate the <entity id="W02-0221.18">tagging</entity> <entity id="W02-0221.19">task</entity> with the <entity id="W02-0221.20">goal</entity> of enabling rapid <entity id="W02-0221.21">creation</entity> of <entity id="W02-0221.22">corpora</entity> for <entity id="W02-0221.23">evaluating</entity> spoken <entity id="W02-0221.24">dialogue systems</entity> and comparing them to human-human <entity id="W02-0221.25">dialogues</entity>. We <entity id="W02-0221.26">report</entity> <entity id="W02-0221.27">results</entity> for <entity id="W02-0221.28">training</entity> and <entity id="W02-0221.29">testing</entity> an <entity id="W02-0221.30">automatic</entity> <entity id="W02-0221.31">classifier</entity> to label the <entity id="W02-0221.32">information</entity> provider's <entity id="W02-0221.33">utterances</entity> in spoken <entity id="W02-0221.34">human-computer</entity> and human-human <entity id="W02-0221.35">dialogues</entity> with <entity id="W02-0221.36">DATE</entity> (<entity id="W02-0221.37">Dialogue Act</entity> <entity id="W02-0221.38">Tagging</entity> for <entity id="W02-0221.39">Evaluation</entity>) <entity id="W02-0221.40">dialogue act</entity> <entity id="W02-0221.41">tags</entity>. We <entity id="W02-0221.42">train</entity> and <entity id="W02-0221.43">test</entity> the <entity id="W02-0221.44">DATE</entity> tagger on various <entity id="W02-0221.45">combinations</entity> of the DARPA Communicator June-2000 and October-2001 <entity id="W02-0221.46">human-computer</entity> <entity id="W02-0221.47">corpora</entity>, and the CMU human-human <entity id="W02-0221.48">corpus</entity> in the travel planning <entity id="W02-0221.49">domain</entity>. Our <entity id="W02-0221.50">results</entity> show that we can achieve high <entity id="W02-0221.51">accuracies</entity> on the humancomputer data, and surprisingly, that the <entity id="W02-0221.52">human-computer</entity><entity id="W02-0221.53">data</entity> <entity id="W02-0221.54">improves</entity> <entity id="W02-0221.55">accuracy</entity> on the human-human data, when only small <entity id="W02-0221.56">amounts</entity> of human-human <entity id="W02-0221.57">training</entity><entity id="W02-0221.58">data</entity> are available.
</abstract>


</text>

<text id="W02-1504"><title><entity id="W02-1504.1">Machine Translation</entity> As A Testbed For Multilingual <entity id="W02-1504.2">Analysis</entity></title><abstract>
We <entity id="W02-1504.3">propose</entity> that <entity id="W02-1504.4">machine translation</entity> (MT) is a useful <entity id="W02-1504.5">application</entity> for <entity id="W02-1504.6">evaluating</entity> and deriving the <entity id="W02-1504.7">development</entity> of NL <entity id="W02-1504.8">components</entity>, especially in a <entity id="W02-1504.9">wide-coverage</entity> <entity id="W02-1504.10">analysis</entity> <entity id="W02-1504.11">system</entity>. Given the <entity id="W02-1504.12">architecture</entity> of our <entity id="W02-1504.13">MT system</entity>, which is a <entity id="W02-1504.14">transfer</entity> <entity id="W02-1504.15">system</entity> <entity id="W02-1504.16">based</entity> on linguistic <entity id="W02-1504.17">modules</entity>, correct <entity id="W02-1504.18">analysis</entity> is expected to be a prerequisite for correct <entity id="W02-1504.19">translation</entity>, suggesting a <entity id="W02-1504.20">correlation</entity> between the two, given relatively mature <entity id="W02-1504.21">transfer</entity> and <entity id="W02-1504.22">generation</entity> <entity id="W02-1504.23">components</entity>. We show through <entity id="W02-1504.24">error analysis</entity> that there is indeed a strong <entity id="W02-1504.25">correlation</entity> between the <entity id="W02-1504.26">quality</entity> of the <entity id="W02-1504.27">translated</entity> <entity id="W02-1504.28">output</entity> and the subjectively determined goodness of the <entity id="W02-1504.29">analysis</entity>. We use this <entity id="W02-1504.30">correlation</entity> as a guide for <entity id="W02-1504.31">development</entity> of a coordinated parallel <entity id="W02-1504.32">analysis</entity> <entity id="W02-1504.33">effort</entity> in 7 <entity id="W02-1504.34">languages</entity>.
</abstract>


</text>

<text id="W03-0421"><title>
A <entity id="W03-0421.1">Simple</entity> <entity id="W03-0421.2">Named</entity> <entity id="W03-0421.3">Entity</entity> <entity id="W03-0421.4">Extractor</entity> Using AdaBoost
</title><abstract>
This <entity id="W03-0421.5">paper</entity> presents a <entity id="W03-0421.6">Named</entity> <entity id="W03-0421.7">Entity</entity> <entity id="W03-0421.8">Extraction</entity> (NEE) <entity id="W03-0421.9">system</entity> for the CoNLL-2003 shared <entity id="W03-0421.10">task</entity> <entity id="W03-0421.11">competition</entity>. As in the past year edition ( Carreras et al., 2002a ), we have <entity id="W03-0421.12">approached</entity> the <entity id="W03-0421.13">task</entity> by treating the two <entity id="W03-0421.14">main</entity> sub-tasks of the <entity id="W03-0421.15">problem</entity>, <entity id="W03-0421.16">recognition</entity> (NER) and <entity id="W03-0421.17">classification</entity> (NEC), sequentially and independently with separate <entity id="W03-0421.18">modules</entity>. Both <entity id="W03-0421.19">modules</entity> are <entity id="W03-0421.20">machine</entity> learning <entity id="W03-0421.21">based</entity> <entity id="W03-0421.22">systems</entity>, which make use of binary and multiclass AdaBoost <entity id="W03-0421.23">classifiers</entity>. <entity id="W03-0421.24">Named</entity> <entity id="W03-0421.25">Entity</entity> <entity id="W03-0421.26">recognition</entity> is <entity id="W03-0421.27">performed</entity> as a greedy <entity id="W03-0421.28">sequence</entity> <entity id="W03-0421.29">tagging</entity> <entity id="W03-0421.30">procedure</entity> under the well-known BIO labelling <entity id="W03-0421.31">scheme</entity>. This <entity id="W03-0421.32">tagging</entity> <entity id="W03-0421.33">process</entity> makes use of three binary <entity id="W03-0421.34">classifiers</entity> <entity id="W03-0421.35">trained</entity> to be <entity id="W03-0421.36">experts</entity></abstract>


</text>

<text id="W03-0802"><title>
WHAT: An XSLT-Based <entity id="W03-0802.1">Infrastructure</entity> For The <entity id="W03-0802.2">Integration</entity> Of <entity id="W03-0802.3">Natural Language Processing</entity> Components
</title><abstract>
The idea of the Whiteboard <entity id="W03-0802.4">project</entity> is to integrate deep and shallow <entity id="W03-0802.5">natural language processing</entity> <entity id="W03-0802.6">components</entity> in <entity id="W03-0802.7">order</entity> to <entity id="W03-0802.8">benefit</entity> from their synergy. The <entity id="W03-0802.9">project</entity> came up with the first fully integrated hybrid <entity id="W03-0802.10">system</entity> consisting of a fast HPSG <entity id="W03-0802.11">parser</entity> that utilizes tokenization, PoS, <entity id="W03-0802.12">morphology</entity>, <entity id="W03-0802.13">lexical</entity>, <entity id="W03-0802.14">named</entity> <entity id="W03-0802.15">entity</entity>, <entity id="W03-0802.16">phrase</entity> <entity id="W03-0802.17">chunk</entity> and (for German) topological <entity id="W03-0802.18">sentence</entity> <entity id="W03-0802.19">field</entity> <entity id="W03-0802.20">analyses</entity> from shallow <entity id="W03-0802.21">components</entity>. This <entity id="W03-0802.22">integration</entity> <entity id="W03-0802.23">increases</entity> <entity id="W03-0802.24">robustness</entity>, directs the <entity id="W03-0802.25">search space</entity> and hence reduces <entity id="W03-0802.26">processing</entity> <entity id="W03-0802.27">time</entity> of the deep <entity id="W03-0802.28">parser</entity>. In this <entity id="W03-0802.29">paper</entity>, we <entity id="W03-0802.30">focus</entity> on one of the central <entity id="W03-0802.31">integration</entity> facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), <entity id="W03-0802.32">report</entity> on the <entity id="W03-0802.33">benefits</entity> of XSLT-based NLP <entity id="W03-0802.34">component</entity> <entity id="W03-0802.35">integration</entity>, and present <entity id="W03-0802.36">examples</entity> of XSL <entity id="W03-0802.37">transformation</entity> of shallow and deep annotations used in the integrated <entity id="W03-0802.38">architecture</entity>. The <entity id="W03-0802.39">infrastructure</entity> is open, portable and well suited for, but not restricted to the <entity id="W03-0802.40">development</entity> of hybrid NLP <entity id="W03-0802.41">architectures</entity> as well as <entity id="W03-0802.42">NLP applications</entity>.
</abstract>


</text>

<text id="W99-0907"><title>
Detecting Sub-<entity id="W99-0907.1">Topic</entity> <entity id="W99-0907.2">Correspondence</entity> Through Bipartite <entity id="W99-0907.3">Term</entity> Clustering
</title><abstract>
This <entity id="W99-0907.4">paper</entity> addresses a novel <entity id="W99-0907.5">task</entity> of detecting <entity id="W99-0907.6">sub-topic</entity> <entity id="W99-0907.7">correspondence</entity> in a <entity id="W99-0907.8">pair</entity> of <entity id="W99-0907.9">text</entity> <entity id="W99-0907.10">fragments</entity>, enhancing <entity id="W99-0907.11">common</entity> <entity id="W99-0907.12">notions</entity> of <entity id="W99-0907.13">text</entity> <entity id="W99-0907.14">similarity</entity>. This <entity id="W99-0907.15">task</entity> is addressed by coupling corresponding <entity id="W99-0907.16">term</entity> subsets through bipartite <entity id="W99-0907.17">clustering</entity>. The <entity id="W99-0907.18">paper</entity> presents a <entity id="W99-0907.19">cost-based</entity> <entity id="W99-0907.20">clustering</entity> <entity id="W99-0907.21">scheme</entity> and compares it with a bipartite <entity id="W99-0907.22">version</entity> of the <entity id="W99-0907.23">single-link</entity> <entity id="W99-0907.24">method</entity>, <entity id="W99-0907.25">providing</entity> illustrating <entity id="W99-0907.26">results</entity>.
</abstract>


</text>

<text id="W08-0327"><title>
Can we Relearn an RBMT <entity id="W08-0327.1">System</entity>?
</title><abstract>
This <entity id="W08-0327.2">paper</entity> describes SYSTRAN <entity id="W08-0327.3">submissions</entity> for the shared <entity id="W08-0327.4">task</entity> of the third <entity id="W08-0327.5">Workshop</entity> on <entity id="W08-0327.6">Statistical Machine Translation</entity> at ACL. Our <entity id="W08-0327.7">main</entity> <entity id="W08-0327.8">contribution</entity> consists in a French-<entity id="W08-0327.9">English</entity> <entity id="W08-0327.10">statistical model</entity> <entity id="W08-0327.11">trained</entity> without the use of any human-translated <entity id="W08-0327.12">parallel corpus</entity>. In <entity id="W08-0327.13">substitution</entity>, we <entity id="W08-0327.14">translated</entity> a monolingual <entity id="W08-0327.15">corpus</entity> with SYSTRAN <entity id="W08-0327.16">rule-based</entity> <entity id="W08-0327.17">translation</entity> <entity id="W08-0327.18">engine</entity> to produce the <entity id="W08-0327.19">parallel corpus</entity>. The <entity id="W08-0327.20">results</entity> are <entity id="W08-0327.21">provided</entity> herein, along with a measure of <entity id="W08-0327.22">error analysis</entity>.
</abstract>


</text>

<text id="J95-1004"><title>
An <entity id="J95-1004.1">Automatic</entity> <entity id="J95-1004.2">Procedure</entity> For <entity id="J95-1004.3">Topic-</entity><entity id="J95-1004.4">Focus</entity> <entity id="J95-1004.5">Identification</entity></title><abstract>
The dichotomy of <entity id="J95-1004.6">topic</entity> and <entity id="J95-1004.7">focus</entity>, <entity id="J95-1004.8">based</entity>, in the Praguean Functional Generative <entity id="J95-1004.9">Description</entity>, on the <entity id="J95-1004.10">scale</entity> of communicative dynamism, is relevant not only for a possible placement of the <entity id="J95-1004.11">sentence</entity> in a <entity id="J95-1004.12">context</entity>, but also for its <entity id="J95-1004.13">semantic interpretation</entity>. An <entity id="J95-1004.14">automatic</entity> <entity id="J95-1004.15">identification</entity> of <entity id="J95-1004.16">topic</entity> and <entity id="J95-1004.17">focus</entity> may use the <entity id="J95-1004.18">input</entity> <entity id="J95-1004.19">information</entity> on <entity id="J95-1004.20">word</entity> <entity id="J95-1004.21">order</entity>, on the systemic <entity id="J95-1004.22">ordering</entity> of <entity id="J95-1004.23">kinds</entity> of complementations (reflected by the underlying <entity id="J95-1004.24">order</entity> of the <entity id="J95-1004.25">items</entity> <entity id="J95-1004.26">included</entity> in the <entity id="J95-1004.27">focus</entity>), on definiteness, and on <entity id="J95-1004.28">lexical</entity> <entity id="J95-1004.29">semantic properties</entity> of <entity id="J95-1004.30">words</entity>. An <entity id="J95-1004.31">algorithm</entity> for the <entity id="J95-1004.32">analysis</entity> of <entity id="J95-1004.33">English</entity> <entity id="J95-1004.34">sentences</entity> has been <entity id="J95-1004.35">implemented</entity> and is discussed and illustrated on several <entity id="J95-1004.36">examples</entity>.
</abstract>


</text>

<text id="J96-3003"><title>
Efficient Multilingual <entity id="J96-3003.1">Phoneme-</entity>To-Grapheme <entity id="J96-3003.2">Conversion</entity> <entity id="J96-3003.3">Based</entity> On HMM
</title>
<abstract><entity id="J96-3003.4">Grapheme-to-phoneme</entity> <entity id="J96-3003.5">conversion</entity> (GTPC) has been achieved in most European <entity id="J96-3003.6">languages</entity> by <entity id="J96-3003.7">dictionary</entity> look-up or using <entity id="J96-3003.8">rules</entity>. The <entity id="J96-3003.9">application</entity> of these <entity id="J96-3003.10">methods</entity>, however, in the reverse <entity id="J96-3003.11">process</entity>, (i.e., in <entity id="J96-3003.12">phoneme-to-grapheme</entity> <entity id="J96-3003.13">conversion</entity> [PTGC]) creates serious <entity id="J96-3003.14">problems</entity>, especially in inflectionally rich <entity id="J96-3003.15">languages</entity>. In this <entity id="J96-3003.16">paper</entity> the PTGC <entity id="J96-3003.17">problem</entity> is <entity id="J96-3003.18">approached</entity> from a completely different <entity id="J96-3003.19">point of view</entity>. Instead of <entity id="J96-3003.20">rules</entity> or a <entity id="J96-3003.21">dictionary</entity>, the <entity id="J96-3003.22">statistics</entity> of <entity id="J96-3003.23">language</entity> connecting <entity id="J96-3003.24">pronunciation</entity> to <entity id="J96-3003.25">spelling</entity> are exploited. The <entity id="J96-3003.26">novelty</entity> lies in <entity id="J96-3003.27">modeling</entity> the <entity id="J96-3003.28">natural language</entity> intraword <entity id="J96-3003.29">features</entity> using the <entity id="J96-3003.30">theory</entity> of hidden <entity id="J96-3003.31">Markov models</entity> (HMM) and <entity id="J96-3003.32">performing</entity> the <entity id="J96-3003.33">conversion</entity> using the Viterbi <entity id="J96-3003.34">algorithm</entity>. The PTGC <entity id="J96-3003.35">system</entity> has been established and <entity id="J96-3003.36">tested</entity> on various multilingual <entity id="J96-3003.37">corpora</entity>. Initially, the <entity id="J96-3003.38">first-order</entity> HMM and the <entity id="J96-3003.39">common</entity> Viterbi <entity id="J96-3003.40">algorithm</entity> were used to obtain a single <entity id="J96-3003.41">transcription</entity> for each <entity id="J96-3003.42">word</entity>. Afterwards, the <entity id="J96-3003.43">second-order</entity> HMM and the N-best <entity id="J96-3003.44">algorithm</entity> <entity id="J96-3003.45">adapted</entity> to PTGC were <entity id="J96-3003.46">implemented</entity> to <entity id="J96-3003.47">provide</entity> one or more <entity id="J96-3003.48">transcriptions</entity> for each <entity id="J96-3003.49">word</entity> <entity id="J96-3003.50">input</entity> (homophones). This <entity id="J96-3003.51">system</entity> gave an average score of more than 99% correctly transcribed <entity id="J96-3003.52">words</entity> (overall <entity id="J96-3003.53">success</entity> in the first four <entity id="J96-3003.54">candidates</entity>) for most of the seven <entity id="J96-3003.55">languages</entity> it was <entity id="J96-3003.56">tested</entity> on (Dutch, <entity id="J96-3003.57">English</entity>, French, German, <entity id="J96-3003.58">Greek</entity>, Italian, and Spanish). The <entity id="J96-3003.59">system</entity> can be <entity id="J96-3003.60">adapted</entity> to almost any <entity id="J96-3003.61">language</entity> with little <entity id="J96-3003.62">effort</entity> and can be <entity id="J96-3003.63">implemented</entity> in hardware to serve in <entity id="J96-3003.64">real-time</entity> <entity id="J96-3003.65">speech recognition systems</entity>.
</abstract>


</text>

<text id="P80-1007"><title>
Should <entity id="P80-1007.1">Computers</entity> Write <entity id="P80-1007.2">Spoken Language</entity>?
</title><abstract></abstract>


</text>

<text id="P98-2171"><title>
From <entity id="P98-2171.1">Information Structure</entity> to <entity id="P98-2171.2">Intonation</entity>: A Phonological <entity id="P98-2171.3">Interface</entity> for <entity id="P98-2171.4">Concept-to-</entity><entity id="P98-2171.5">Speech</entity></title><abstract>
The <entity id="P98-2171.6">paper</entity> describes an <entity id="P98-2171.7">interface</entity> between <entity id="P98-2171.8">generator</entity> and synthesizer of the German <entity id="P98-2171.9">language</entity> <entity id="P98-2171.10">concept-to-speech</entity> <entity id="P98-2171.11">system</entity> VieCtoS. It discusses <entity id="P98-2171.12">phenomena</entity> in German <entity id="P98-2171.13">intonation</entity> that depend on the <entity id="P98-2171.14">interaction</entity> between grammatical <entity id="P98-2171.15">dependencies</entity> (<entity id="P98-2171.16">projection</entity> of <entity id="P98-2171.17">information structure</entity> into <entity id="P98-2171.18">syntax</entity>) and prosodie <entity id="P98-2171.19">context</entity> (<entity id="P98-2171.20">performance-related</entity> <entity id="P98-2171.21">modifications</entity> to <entity id="P98-2171.22">intonation</entity> <entity id="P98-2171.23">patterns</entity>). Phonological <entity id="P98-2171.24">processing</entity> in our <entity id="P98-2171.25">system</entity> comprises segmental as well as suprasegmental <entity id="P98-2171.26">dimensions</entity> such as syllabification, <entity id="P98-2171.27">modification</entity> of <entity id="P98-2171.28">word</entity> stress positions, and a symbolic encoding of <entity id="P98-2171.29">intonation</entity>. Phonological <entity id="P98-2171.30">phenomena</entity> often touch upon more than one of these <entity id="P98-2171.31">dimensions</entity>, so that mutual accessibility of the<entity id="P98-2171.32">data</entity> <entity id="P98-2171.33">structures</entity> on each <entity id="P98-2171.34">dimension</entity> had to be ensured. We present a linear <entity id="P98-2171.35">representation</entity> of the multidimensional phonological<entity id="P98-2171.36">data</entity> <entity id="P98-2171.37">based</entity> on a straightforward linearization <entity id="P98-2171.38">convention</entity>, which suffices to bring this conceptually multilinear<entity id="P98-2171.39">data</entity> set under the <entity id="P98-2171.40">scope</entity> of the well-known <entity id="P98-2171.41">processing</entity> <entity id="P98-2171.42">techniques</entity> for <entity id="P98-2171.43">two-level</entity> <entity id="P98-2171.44">morphology</entity>.
</abstract>


</text>

<text id="P98-2175"><title>
An Intelligent Multi-<entity id="P98-2175.1">Dictionary</entity> <entity id="P98-2175.2">Environment</entity></title><abstract>
An open, extendible <entity id="P98-2175.3">multi-dictionary</entity> <entity id="P98-2175.4">system</entity> is introduced in the <entity id="P98-2175.5">paper</entity>. It <entity id="P98-2175.6">supports</entity> the <entity id="P98-2175.7">translator</entity> in <entity id="P98-2175.8">accessing</entity> adequate <entity id="P98-2175.9">entries</entity> of various bi- and monolingual <entity id="P98-2175.10">dictionaries</entity> and <entity id="P98-2175.11">translation</entity> <entity id="P98-2175.12">examples</entity> from <entity id="P98-2175.13">parallel corpora</entity>. Simultaneously an unlimited <entity id="P98-2175.14">number</entity> of <entity id="P98-2175.15">dictionaries</entity> can be held open, thus by a single interrogation <entity id="P98-2175.16">step</entity>, all the <entity id="P98-2175.17">dictionaries</entity> (<entity id="P98-2175.18">translations</entity>, <entity id="P98-2175.19">explanations</entity>, <entity id="P98-2175.20">synonyms</entity>, etc.) can be <entity id="P98-2175.21">surveyed</entity>. The <entity id="P98-2175.22">implemented</entity> <entity id="P98-2175.23">system</entity> (<entity id="P98-2175.24">called</entity> MoBiDic) knows morphological <entity id="P98-2175.25">rules</entity> of the <entity id="P98-2175.26">dictionaries</entity>' <entity id="P98-2175.27">languages</entity>. Thus, never the actual (inflected) <entity id="P98-2175.28">words</entity>, but always their <entity id="P98-2175.29">lemmas</entity> - that is, the right <entity id="P98-2175.30">dictionary</entity> <entity id="P98-2175.31">entries</entity> - are looked up. MoBiDic has an open, multimedial <entity id="P98-2175.32">architecture</entity>, thus it is suitable for handling not only textual, but speaking or picture <entity id="P98-2175.33">dictionaries</entity>, as well. The same <entity id="P98-2175.34">system</entity> is also able to find <entity id="P98-2175.35">words</entity> and <entity id="P98-2175.36">expressions</entity> in <entity id="P98-2175.37">corpora</entity>, dynamically <entity id="P98-2175.38">providing</entity> the <entity id="P98-2175.39">translators</entity> with <entity id="P98-2175.40">examples</entity> from their earlier <entity id="P98-2175.41">translations</entity> or other <entity id="P98-2175.42">translators</entity>' works. MoBiDic has been <entity id="P98-2175.43">designed</entity> for <entity id="P98-2175.44">translator</entity> workgroups, where the <entity id="P98-2175.45">translators</entity>' own glossaries (built also with the <entity id="P98-2175.46">help</entity> of the <entity id="P98-2175.47">system</entity>) may also be disseminated among the members of the group, with different <entity id="P98-2175.48">access</entity> rights, if needed. The <entity id="P98-2175.49">system</entity> has a TCP/IP-based <entity id="P98-2175.50">client-server</entity> <entity id="P98-2175.51">implementation</entity> for various <entity id="P98-2175.52">platforms</entity> and available with a gradually <entity id="P98-2175.53">increasing</entity> <entity id="P98-2175.54">number</entity> of <entity id="P98-2175.55">dictionaries</entity> for numerous <entity id="P98-2175.56">language pairs</entity>.
</abstract>


</text>

<text id="I08-1001"><title>
A Lemmatization <entity id="I08-1001.1">Method</entity> for Modern Mongolian and its <entity id="I08-1001.2">Application</entity> to <entity id="I08-1001.3">Information Retrieval</entity></title><abstract>
In Modern Mongolian, a <entity id="I08-1001.4">content word</entity> can be inflected when concatenated with <entity id="I08-1001.5">suffixes</entity>. <entity id="I08-1001.6">Identifying</entity> the original <entity id="I08-1001.7">forms</entity> of <entity id="I08-1001.8">content words</entity> is crucial for <entity id="I08-1001.9">natural language processing</entity> and <entity id="I08-1001.10">information retrieval</entity>. We <entity id="I08-1001.11">propose</entity> a lemmatization <entity id="I08-1001.12">method</entity> for Modern Mongolian and <entity id="I08-1001.13">apply</entity> our <entity id="I08-1001.14">method</entity> to <entity id="I08-1001.15">indexing</entity> for <entity id="I08-1001.16">information retrieval</entity>. We use technical <entity id="I08-1001.17">abstracts</entity> to show the <entity id="I08-1001.18">effectiveness</entity> of our <entity id="I08-1001.19">method</entity> experimentally.
</abstract>


</text>

<text id="W04-2003"><title>
A <entity id="W04-2003.1">Robust</entity> And Hybrid Deep-<entity id="W04-2003.2">Linguistic Theory</entity> <entity id="W04-2003.3">Applied</entity> To Large-<entity id="W04-2003.4">Scale</entity> <entity id="W04-2003.5">Parsing</entity></title><abstract>
Modern <entity id="W04-2003.6">statistical</entity> <entity id="W04-2003.7">parsers</entity> are <entity id="W04-2003.8">robust</entity> and quite fast, but their <entity id="W04-2003.9">output</entity> is relatively shallow when compared to formal grammar <entity id="W04-2003.10">parsers</entity>. We suggest to extend <entity id="W04-2003.11">statistical</entity> <entity id="W04-2003.12">approaches</entity> to a more <entity id="W04-2003.13">deep-linguistic analysis</entity> while at the same <entity id="W04-2003.14">time</entity> keeping the <entity id="W04-2003.15">speed</entity> and low <entity id="W04-2003.16">complexity</entity> of a <entity id="W04-2003.17">statistical</entity> <entity id="W04-2003.18">parser</entity>. The <entity id="W04-2003.19">resulting</entity> <entity id="W04-2003.20">parsing</entity> <entity id="W04-2003.21">architecture</entity> suggested, <entity id="W04-2003.22">implemented</entity> and <entity id="W04-2003.23">evaluated</entity> here is highly robust and hybrid on a number of <entity id="W04-2003.24">levels</entity>, combining <entity id="W04-2003.25">statistical</entity> and <entity id="W04-2003.26">rule-based approaches</entity>, <entity id="W04-2003.27">constituency</entity> and <entity id="W04-2003.28">dependency grammar</entity>, shallow and deep <entity id="W04-2003.29">processing</entity>, full and near-full <entity id="W04-2003.30">parsing</entity>. With its <entity id="W04-2003.31">parsing</entity> <entity id="W04-2003.32">speed</entity> of about 300,000 <entity id="W04-2003.33">words</entity> per hour and state-of-the-art <entity id="W04-2003.34">performance</entity> the <entity id="W04-2003.35">parser</entity> is reliable for a <entity id="W04-2003.36">number</entity> of <entity id="W04-2003.37">large-scale</entity> <entity id="W04-2003.38">applications</entity> discussed in the article.
</abstract>


</text>

<text id="W04-2306"><title>
Semi-<entity id="W04-2306.1">Automatic</entity> <entity id="W04-2306.2">Generation</entity> Of <entity id="W04-2306.3">Dialogue</entity> <entity id="W04-2306.4">Applications</entity> In The GEMINI <entity id="W04-2306.5">Project</entity></title><abstract>
GEMINI (Generic <entity id="W04-2306.6">Environment</entity> for Multilingual Interactive <entity id="W04-2306.7">Natural</entity> Interfaces) is an EC funded <entity id="W04-2306.8">research project</entity>, which has two <entity id="W04-2306.9">main</entity> <entity id="W04-2306.10">objectives</entity>: First, the <entity id="W04-2306.11">development</entity> of a flexible <entity id="W04-2306.12">platform</entity> able to produce <entity id="W04-2306.13">user-friendly</entity> interactive multilingual and multi-modal <entity id="W04-2306.14">dialogue</entity> <entity id="W04-2306.15">interfaces</entity> to <entity id="W04-2306.16">databases</entity> with a <entity id="W04-2306.17">minimum</entity> of human <entity id="W04-2306.18">effort</entity>, and, second, the <entity id="W04-2306.19">demonstration</entity> of the <entity id="W04-2306.20">platform</entity>'s <entity id="W04-2306.21">efficiency</entity> through the <entity id="W04-2306.22">development</entity> of two different <entity id="W04-2306.23">applications</entity> <entity id="W04-2306.24">based</entity> on this <entity id="W04-2306.25">platform</entity>: EG-Banking, a voice-portal for <entity id="W04-2306.26">high-quality</entity> <entity id="W04-2306.27">interactions</entity> for <entity id="W04-2306.28">bank</entity> <entity id="W04-2306.29">customers</entity>, and CitizenCare, an e-government <entity id="W04-2306.30">platform</entity> <entity id="W04-2306.31">framework</entity> for citizen-to-administration <entity id="W04-2306.32">interaction</entity> which are available for spoken and web-based <entity id="W04-2306.33">user</entity> <entity id="W04-2306.34">interaction</entity>.
</abstract>


</text>

<text id="W04-2501"><title><entity id="W04-2501.1">Strategies</entity> For Advanced <entity id="W04-2501.2">Question Answering</entity></title>
<abstract><entity id="W04-2501.3">Progress</entity> in <entity id="W04-2501.4">Question Answering</entity> can be achieved by (1) combining multiple <entity id="W04-2501.5">strategies</entity> that optimally resolve different <entity id="W04-2501.6">question</entity> <entity id="W04-2501.7">classes</entity> of various <entity id="W04-2501.8">degrees</entity> of <entity id="W04-2501.9">complexity</entity>; (2) enhancing the <entity id="W04-2501.10">precision</entity> of <entity id="W04-2501.11">question</entity> <entity id="W04-2501.12">interpretation</entity> and answer <entity id="W04-2501.13">extraction</entity>; and (3) <entity id="W04-2501.14">question</entity> <entity id="W04-2501.15">decomposition</entity> and answer <entity id="W04-2501.16">fusion</entity>. In this <entity id="W04-2501.17">paper</entity> we also present the <entity id="W04-2501.18">impact</entity> of <entity id="W04-2501.19">modeling</entity> the <entity id="W04-2501.20">user</entity> <entity id="W04-2501.21">background</entity> on Q/A and discuss the pragmatics pf <entity id="W04-2501.22">processing</entity> <entity id="W04-2501.23">negation</entity> in Q/A.
</abstract>


</text>

<text id="W05-0617"><title><entity id="W05-0617.1">Morphology</entity> <entity id="W05-0617.2">Induction</entity> From <entity id="W05-0617.3">Term</entity> Clusters
</title><abstract>
We address the <entity id="W05-0617.4">problem</entity> of learning a morphological automaton directly from a monolingual <entity id="W05-0617.5">text</entity> <entity id="W05-0617.6">corpus</entity> without recourse to additional <entity id="W05-0617.7">resources</entity>. Like previous work in this <entity id="W05-0617.8">area</entity>, our <entity id="W05-0617.9">approach</entity> exploits orthographic <entity id="W05-0617.10">regularities</entity> in a <entity id="W05-0617.11">search</entity> for possible morphological segmentation points. Instead of affixes, however, we <entity id="W05-0617.12">search</entity> for affix <entity id="W05-0617.13">transformation</entity> <entity id="W05-0617.14">rules</entity> that express <entity id="W05-0617.15">correspondences</entity> between <entity id="W05-0617.16">term</entity> <entity id="W05-0617.17">clusters</entity> induced from the<entity id="W05-0617.18">data</entity>. This <entity id="W05-0617.19">focuses</entity> the <entity id="W05-0617.20">system</entity> on substrings having <entity id="W05-0617.21">syntactic function</entity>, and <entity id="W05-0617.22">yields</entity> <entity id="W05-0617.23">cluster-to-cluster</entity> <entity id="W05-0617.24">transformation</entity> <entity id="W05-0617.25">rules</entity> which enable the <entity id="W05-0617.26">system</entity> to <entity id="W05-0617.27">process</entity> unknown morphological <entity id="W05-0617.28">forms</entity> of known <entity id="W05-0617.29">words</entity> accurately. A <entity id="W05-0617.30">stem-weighting</entity> <entity id="W05-0617.31">algorithm</entity> <entity id="W05-0617.32">based</entity> on Hubs and Authorities is used to clarify ambiguous segmentation points. We <entity id="W05-0617.33">evaluate</entity> our <entity id="W05-0617.34">approach</entity> using the CELEX <entity id="W05-0617.35">database</entity>.
</abstract>


</text>

<text id="W05-0802"><title><entity id="W05-0802.1">Cross</entity> <entity id="W05-0802.2">Language</entity> <entity id="W05-0802.3">Text Categorization</entity> By Acquiring Multilingual <entity id="W05-0802.4">Domain</entity> <entity id="W05-0802.5">Models</entity> From Comparable <entity id="W05-0802.6">Corpora</entity></title><abstract>
In a multilingual <entity id="W05-0802.7">scenario</entity>, the classical monolingual <entity id="W05-0802.8">text categorization</entity> <entity id="W05-0802.9">problem</entity> can be reformulated as a <entity id="W05-0802.10">cross</entity> <entity id="W05-0802.11">language</entity> TC <entity id="W05-0802.12">English</entity> Italian). <entity id="W05-0802.13">English</entity>), Italian).
</abstract>


</text>

<text id="W05-1514"><title><entity id="W05-1514.1">Chunk</entity> <entity id="W05-1514.2">Parsing</entity> Revisited
</title><abstract><entity id="W05-1514.3">Chunk</entity> <entity id="W05-1514.4">parsing</entity> is conceptually appealing but its <entity id="W05-1514.5">performance</entity> has not been satisfactory for practical use. In this <entity id="W05-1514.6">paper</entity> we show that <entity id="W05-1514.7">chunk</entity> <entity id="W05-1514.8">parsing</entity> can <entity id="W05-1514.9">perform</entity> significantly better than previously <entity id="W05-1514.10">reported</entity> by using a <entity id="W05-1514.11">simple</entity> <entity id="W05-1514.12">sliding-window</entity> <entity id="W05-1514.13">method</entity> and <entity id="W05-1514.14">maximum entropy</entity> <entity id="W05-1514.15">classifiers</entity> for <entity id="W05-1514.16">phrase</entity> <entity id="W05-1514.17">recognition</entity> in each <entity id="W05-1514.18">level</entity> of <entity id="W05-1514.19">chunking</entity>. <entity id="W05-1514.20">Experimental</entity> <entity id="W05-1514.21">results</entity> with the <entity id="W05-1514.22">Penn Treebank</entity> <entity id="W05-1514.23">corpus</entity> show that our <entity id="W05-1514.24">chunk</entity> <entity id="W05-1514.25">parser</entity> can give <entity id="W05-1514.26">high-precision</entity> <entity id="W05-1514.27">parsing</entity> <entity id="W05-1514.28">outputs</entity> with very high <entity id="W05-1514.29">speed</entity> (14 msec/<entity id="W05-1514.30">sentence</entity>). We also present a <entity id="W05-1514.31">parsing</entity> <entity id="W05-1514.32">method</entity> for <entity id="W05-1514.33">searching</entity> the best <entity id="W05-1514.34">parse</entity> by considering the <entity id="W05-1514.35">probabilities</entity> <entity id="W05-1514.36">output</entity> by the <entity id="W05-1514.37">maximum entropy</entity> <entity id="W05-1514.38">classifiers</entity>, and show that the <entity id="W05-1514.39">search</entity> <entity id="W05-1514.40">method</entity> can further <entity id="W05-1514.41">improve</entity> the <entity id="W05-1514.42">parsing</entity> <entity id="W05-1514.43">accuracy</entity>.
</abstract>


</text>

<text id="W06-0205"><title><entity id="W06-0205.1">Automatic</entity> <entity id="W06-0205.2">Knowledge Representation</entity> Using A Graph-<entity id="W06-0205.3">Based</entity> <entity id="W06-0205.4">Algorithm</entity> For <entity id="W06-0205.5">Language-</entity>Independent <entity id="W06-0205.6">Lexical</entity> Chaining
</title><abstract><entity id="W06-0205.7">Lexical Chains</entity> are powerful <entity id="W06-0205.8">representations</entity> of <entity id="W06-0205.9">documents</entity>. In particular, they have successfully been used in the <entity id="W06-0205.10">field</entity> of <entity id="W06-0205.11">Automatic</entity> <entity id="W06-0205.12">Text Summarization</entity>. However, until now, <entity id="W06-0205.13">Lexical</entity> Chaining <entity id="W06-0205.14">algorithms</entity> have only been <entity id="W06-0205.15">proposed</entity> for <entity id="W06-0205.16">English</entity>. In this <entity id="W06-0205.17">paper</entity>, we <entity id="W06-0205.18">propose</entity> a greedy <entity id="W06-0205.19">Language-</entity>Independent <entity id="W06-0205.20">algorithm</entity> that automatically <entity id="W06-0205.21">extracts</entity> <entity id="W06-0205.22">Lexical Chains</entity> from <entity id="W06-0205.23">texts</entity>. For that <entity id="W06-0205.24">purpose</entity>, we build a hierarchical <entity id="W06-0205.25">lexico-semantic knowledge</entity> <entity id="W06-0205.26">base</entity> from a <entity id="W06-0205.27">collection</entity> of <entity id="W06-0205.28">texts</entity> by using the Pole-<entity id="W06-0205.29">Based</entity> Overlapping <entity id="W06-0205.30">Clustering Algorithm</entity>. As a consequence, our <entity id="W06-0205.31">methodology</entity> can be <entity id="W06-0205.32">applied</entity> to any <entity id="W06-0205.33">language</entity> and <entity id="W06-0205.34">proposes</entity> a <entity id="W06-0205.35">solution</entity> to <entity id="W06-0205.36">language-dependent</entity> <entity id="W06-0205.37">Lexical</entity> Chainers.
</abstract>


</text>

<text id="W06-1002"><title>
The <entity id="W06-1002.1">Role</entity> Of <entity id="W06-1002.2">Lexical</entity> Resources In CJK <entity id="W06-1002.3">Natural Language</entity> <entity id="W06-1002.4">Processing</entity></title><abstract>
The <entity id="W06-1002.5">role</entity> of <entity id="W06-1002.6">lexical resources</entity> is often understated in NLP <entity id="W06-1002.7">research</entity>. The <entity id="W06-1002.8">complexity</entity> of <entity id="W06-1002.9">Chinese</entity>, <entity id="W06-1002.10">Japanese</entity> and Korean (CJK) poses special <entity id="W06-1002.11">challenges</entity> to <entity id="W06-1002.12">developers</entity> of NLP <entity id="W06-1002.13">tools</entity>, especially in the <entity id="W06-1002.14">area</entity> of <entity id="W06-1002.15">word segmentation</entity> (WS), <entity id="W06-1002.16">information retrieval</entity> (IR), <entity id="W06-1002.17">named</entity> <entity id="W06-1002.18">entity</entity> <entity id="W06-1002.19">extraction</entity> (NER), and <entity id="W06-1002.20">machine translation</entity> (MT). These <entity id="W06-1002.21">difficulties</entity> are exacerbated by the <entity id="W06-1002.22">lack</entity> of comprehensive <entity id="W06-1002.23">lexical resources</entity>, especially for proper <entity id="W06-1002.24">nouns</entity>, and the <entity id="W06-1002.25">lack</entity> of a standardized orthography, especially in <entity id="W06-1002.26">Japanese</entity>. This <entity id="W06-1002.27">paper</entity> summarizes some of the major linguistic <entity id="W06-1002.28">issues</entity> in the <entity id="W06-1002.29">development</entity> <entity id="W06-1002.30">NLP applications</entity> that are dependent on <entity id="W06-1002.31">lexical resources</entity>, and discusses the central <entity id="W06-1002.32">role</entity> such <entity id="W06-1002.33">resources</entity> should play in enhancing the <entity id="W06-1002.34">accuracy</entity> of NLP <entity id="W06-1002.35">tools</entity>.
</abstract>


</text>

<text id="W06-1006"><title>
Multilingual <entity id="W06-1006.1">Collocation</entity> <entity id="W06-1006.2">Extraction</entity>: <entity id="W06-1006.3">Issues</entity> And Solutions
</title><abstract>
Although traditionally seen as a <entity id="W06-1006.4">language-independent</entity> <entity id="W06-1006.5">task</entity>, <entity id="W06-1006.6">collocation</entity> <entity id="W06-1006.7">extraction</entity> relies nowadays more and more on the linguistic preprocessing of <entity id="W06-1006.8">texts</entity> (e.g., lemmatization, <entity id="W06-1006.9">POS tagging</entity>, <entity id="W06-1006.10">chunking</entity> or <entity id="W06-1006.11">parsing</entity>) prior to the <entity id="W06-1006.12">application</entity> of <entity id="W06-1006.13">statistical</entity> measures. This <entity id="W06-1006.14">paper</entity> <entity id="W06-1006.15">provides</entity> a <entity id="W06-1006.16">language-oriented</entity> <entity id="W06-1006.17">review</entity> of the existing <entity id="W06-1006.18">extraction</entity> work. It points out several <entity id="W06-1006.19">language-specific</entity> <entity id="W06-1006.20">issues</entity> related to <entity id="W06-1006.21">extraction</entity> and <entity id="W06-1006.22">proposes</entity> a <entity id="W06-1006.23">strategy</entity> for coping with them. It then describes a hybrid <entity id="W06-1006.24">extraction system</entity> <entity id="W06-1006.25">based</entity> on a multilingual <entity id="W06-1006.26">parser</entity>. Finally, it presents a <entity id="W06-1006.27">case-study</entity> on the <entity id="W06-1006.28">performance</entity> of an <entity id="W06-1006.29">association</entity> measure across a <entity id="W06-1006.30">number</entity> of <entity id="W06-1006.31">languages</entity>.
</abstract>


</text>

<text id="W06-1303"><title><entity id="W06-1303.1">Building</entity> Effective <entity id="W06-1303.2">Question Answering</entity> Characters
</title><abstract>
In this <entity id="W06-1303.3">paper</entity>, we describe <entity id="W06-1303.4">methods</entity> for <entity id="W06-1303.5">building</entity> and <entity id="W06-1303.6">evaluation</entity> of limited <entity id="W06-1303.7">domain</entity> <entity id="W06-1303.8">question-answering</entity> characters. Several <entity id="W06-1303.9">classification</entity> <entity id="W06-1303.10">techniques</entity> are <entity id="W06-1303.11">tested</entity>, <entity id="W06-1303.12">including</entity> <entity id="W06-1303.13">text classification</entity> using <entity id="W06-1303.14">support vector machines</entity>, <entity id="W06-1303.15">language-model</entity> <entity id="W06-1303.16">based</entity> <entity id="W06-1303.17">retrieval</entity>, and <entity id="W06-1303.18">cross-language</entity> <entity id="W06-1303.19">information retrieval</entity> <entity id="W06-1303.20">techniques</entity>, with the latter having the highest <entity id="W06-1303.21">success</entity> <entity id="W06-1303.22">rate</entity>. We also <entity id="W06-1303.23">evaluated</entity> the <entity id="W06-1303.24">effect</entity> of <entity id="W06-1303.25">speech recognition</entity> <entity id="W06-1303.26">errors</entity> on <entity id="W06-1303.27">performance</entity> with <entity id="W06-1303.28">users</entity>, finding that <entity id="W06-1303.29">retrieval</entity> is <entity id="W06-1303.30">robust</entity> until <entity id="W06-1303.31">recognition</entity> <entity id="W06-1303.32">reaches</entity> over 50% WER.
</abstract>


</text>

<text id="I08-2123"><title>
A <entity id="I08-2123.1">Co-occurrence</entity> <entity id="I08-2123.2">Graph-based Approach</entity> for Personal <entity id="I08-2123.3">Name</entity> <entity id="I08-2123.4">Alias</entity> <entity id="I08-2123.5">Extraction</entity> from Anchor <entity id="I08-2123.6">Texts</entity></title><abstract>
A person may have multiple <entity id="I08-2123.7">name</entity> <entity id="I08-2123.8">aliases</entity> on the Web. <entity id="I08-2123.9">Identifying</entity> <entity id="I08-2123.10">aliases</entity> of a <entity id="I08-2123.11">name</entity> is important for various <entity id="I08-2123.12">tasks</entity> such as <entity id="I08-2123.13">information retrieval</entity>, <entity id="I08-2123.14">sentiment analysis</entity> and <entity id="I08-2123.15">name</entity> <entity id="I08-2123.16">disambiguation</entity>. We introduce the <entity id="I08-2123.17">notion</entity> of a <entity id="I08-2123.18">word</entity> <entity id="I08-2123.19">co-occurrence</entity> graph to represent the mutual <entity id="I08-2123.20">relations</entity> between <entity id="I08-2123.21">words</entity> that appear in <entity id="I08-2123.22">anchor texts</entity>. <entity id="I08-2123.23">Words</entity> in <entity id="I08-2123.24">anchor texts</entity> are represented as <entity id="I08-2123.25">nodes</entity> in the <entity id="I08-2123.26">co-occurrence</entity> graph and an <entity id="I08-2123.27">edge</entity> is <entity id="I08-2123.28">formed</entity> between <entity id="I08-2123.29">nodes</entity> which <entity id="I08-2123.30">link</entity> to the same url. For a given personal <entity id="I08-2123.31">name</entity>, its <entity id="I08-2123.32">neighboring</entity> <entity id="I08-2123.33">nodes</entity> in the graph are considered as <entity id="I08-2123.34">candidates</entity> of its <entity id="I08-2123.35">aliases</entity>. We formalize <entity id="I08-2123.36">alias</entity> <entity id="I08-2123.37">identification</entity> as a <entity id="I08-2123.38">problem</entity> of ranking <entity id="I08-2123.39">nodes</entity> in this graph with <entity id="I08-2123.40">respect</entity> to a given <entity id="I08-2123.41">name</entity>. We integrate various ranking scores through <entity id="I08-2123.42">support vector machines</entity> to <entity id="I08-2123.43">leverage</entity> a <entity id="I08-2123.44">robust</entity> ranking <entity id="I08-2123.45">function</entity> and use it to <entity id="I08-2123.46">extract</entity> <entity id="I08-2123.47">aliases</entity> for a given <entity id="I08-2123.48">name</entity>. <entity id="I08-2123.49">Experimental</entity> <entity id="I08-2123.50">results</entity> on a dataset of <entity id="I08-2123.51">Japanese</entity> celebrities show that the <entity id="I08-2123.52">proposed</entity> <entity id="I08-2123.53">method</entity> outperforms all baselines, <entity id="I08-2123.54">displaying</entity> a MRR score of 0.562.
</abstract>


</text>

<text id="W06-2801"><title><entity id="W06-2801.1">Text</entity> Linkage In The Wiki Medium - A Comparative <entity id="W06-2801.2">Study</entity></title><abstract>
We analyze four different <entity id="W06-2801.3">types</entity> of <entity id="W06-2801.4">document</entity> <entity id="W06-2801.5">networks</entity> with <entity id="W06-2801.6">respect</entity> to their small world <entity id="W06-2801.7">characteristics</entity>. These <entity id="W06-2801.8">characteristics</entity> allow distinguishing <entity id="W06-2801.9">wiki-based systems</entity> from <entity id="W06-2801.10">citation</entity> and more traditional <entity id="W06-2801.11">text-based</entity> <entity id="W06-2801.12">networks</entity> augmented by hyperlinks. The <entity id="W06-2801.13">study</entity> <entity id="W06-2801.14">provides</entity> <entity id="W06-2801.15">evidence</entity> that a more appropriate <entity id="W06-2801.16">network</entity> <entity id="W06-2801.17">model</entity> is needed which better reflects the specifics of wiki <entity id="W06-2801.18">systems</entity>. It puts emphasize on their topological <entity id="W06-2801.19">differences</entity> as a <entity id="W06-2801.20">result</entity> of <entity id="W06-2801.21">wiki-related</entity> <entity id="W06-2801.22">linking</entity> compared to other <entity id="W06-2801.23">text-based</entity> <entity id="W06-2801.24">networks</entity>.
</abstract>


</text>

<text id="W06-2802"><title>
Errors In Wikis
</title><abstract>
This <entity id="W06-2802.1">discussion</entity> <entity id="W06-2802.2">document</entity> <entity id="W06-2802.3">concerns</entity> the <entity id="W06-2802.4">challenges</entity> to <entity id="W06-2802.5">assessments</entity> of <entity id="W06-2802.6">reliability</entity> posed by wikis and the potential for <entity id="W06-2802.7">language processing</entity> <entity id="W06-2802.8">techniques</entity> for aiding readers to decide whether to trust particular <entity id="W06-2802.9">text</entity>.
</abstract>


</text>

<text id="W06-3115"><title>
NTT <entity id="W06-3115.1">System</entity> <entity id="W06-3115.2">Description</entity> For The WMT2006 <entity id="W06-3115.3">Shared Task</entity></title><abstract>
"We present two <entity id="W06-3115.4">translation systems</entity> <entity id="W06-3115.5">experimented</entity> for the <entity id="W06-3115.6">shared-task</entity> of ""<entity id="W06-3115.7">Workshop</entity> on <entity id="W06-3115.8">Statistical Machine Translation</entity>,"" a <entity id="W06-3115.9">phrase-based model</entity> and a hierarchical <entity id="W06-3115.10">phrase-based model</entity>. The former uses a phrasal <entity id="W06-3115.11">unit</entity> for <entity id="W06-3115.12">translation</entity>, whereas the latter is conceptualized as a synchronous-CFG in which <entity id="W06-3115.13">phrases</entity> are hierarchically combined using non-terminals. <entity id="W06-3115.14">Experiments</entity> showed that the hierarchical <entity id="W06-3115.15">phrase-based model</entity> <entity id="W06-3115.16">performed</entity> very comparable to the <entity id="W06-3115.17">phrase-based model</entity>. We also <entity id="W06-3115.18">report</entity> a <entity id="W06-3115.19">phrase</entity>/<entity id="W06-3115.20">rule</entity> <entity id="W06-3115.21">extraction</entity> <entity id="W06-3115.22">technique</entity> differentiating tokenization of <entity id="W06-3115.23">corpora</entity>. "
</abstract>


</text>

<text id="W06-1710"><title>
Web <entity id="W06-1710.1">Corpus</entity> Mining By <entity id="W06-1710.2">Instance</entity> Of Wikipedia
</title><abstract>
In this <entity id="W06-1710.3">paper</entity> we present an <entity id="W06-1710.4">approach</entity> to <entity id="W06-1710.5">structure</entity> learning in the <entity id="W06-1710.6">area</entity> of <entity id="W06-1710.7">web documents</entity>. This is done in <entity id="W06-1710.8">order</entity> to <entity id="W06-1710.9">approach</entity> the <entity id="W06-1710.10">goal</entity> of webgenre <entity id="W06-1710.11">tagging</entity> in the <entity id="W06-1710.12">area</entity> of web <entity id="W06-1710.13">corpus</entity> <entity id="W06-1710.14">linguistics</entity>. A central <entity id="W06-1710.15">outcome</entity> of the <entity id="W06-1710.16">paper</entity> is that purely <entity id="W06-1710.17">structure</entity> oriented <entity id="W06-1710.18">approaches</entity> to <entity id="W06-1710.19">web document</entity> <entity id="W06-1710.20">classification</entity> <entity id="W06-1710.21">provide</entity> an <entity id="W06-1710.22">information gain</entity> which may be utilized in combined <entity id="W06-1710.23">approaches</entity> of web <entity id="W06-1710.24">content</entity> and <entity id="W06-1710.25">structure</entity> <entity id="W06-1710.26">analysis</entity>.
</abstract>


</text>

<text id="W06-1906"><title>
BRUJA: <entity id="W06-1906.1">Question</entity> <entity id="W06-1906.2">Classification</entity> For Spanish Using <entity id="W06-1906.3">Machine</entity> Translation and An <entity id="W06-1906.4">English</entity> <entity id="W06-1906.5">Classifier</entity></title>
<abstract><entity id="W06-1906.6">Question</entity> <entity id="W06-1906.7">Classification</entity> is an important <entity id="W06-1906.8">task</entity> in <entity id="W06-1906.9">Question Answering</entity> <entity id="W06-1906.10">Systems</entity>. This <entity id="W06-1906.11">paper</entity> presents a Spanish <entity id="W06-1906.12">Question</entity> <entity id="W06-1906.13">Classifier</entity> <entity id="W06-1906.14">based</entity> on <entity id="W06-1906.15">machine learning</entity>, <entity id="W06-1906.16">automatic</entity> online <entity id="W06-1906.17">translators</entity> and different <entity id="W06-1906.18">language</entity> <entity id="W06-1906.19">features</entity>. Our <entity id="W06-1906.20">system</entity> works with <entity id="W06-1906.21">English</entity> <entity id="W06-1906.22">collections</entity> and bilingual <entity id="W06-1906.23">questions</entity> (<entity id="W06-1906.24">English</entity>/Spanish). We have <entity id="W06-1906.25">tested</entity> two Spanish-<entity id="W06-1906.26">English</entity> online <entity id="W06-1906.27">translators</entity> to identify the lost of <entity id="W06-1906.28">precision</entity>. We have made <entity id="W06-1906.29">experiments</entity> using <entity id="W06-1906.30">lexical</entity>, <entity id="W06-1906.31">syntactic</entity> and <entity id="W06-1906.32">semantic features</entity> to <entity id="W06-1906.33">test</entity> which ones made a better <entity id="W06-1906.34">performance</entity>. The obtained <entity id="W06-1906.35">results</entity> show that our <entity id="W06-1906.36">system</entity> makes good <entity id="W06-1906.37">classifications</entity>, over a 80% in <entity id="W06-1906.38">terms</entity> of <entity id="W06-1906.39">accuracy</entity> using the original <entity id="W06-1906.40">English</entity> <entity id="W06-1906.41">questions</entity> and over a 65% using Spanish <entity id="W06-1906.42">questions</entity> and <entity id="W06-1906.43">machine translation systems</entity>. Our <entity id="W06-1906.44">conclusion</entity> about the <entity id="W06-1906.45">features</entity> is that a <entity id="W06-1906.46">lexical</entity>, <entity id="W06-1906.47">syntactic</entity> and <entity id="W06-1906.48">semantic features</entity> <entity id="W06-1906.49">combination</entity> obtains the best <entity id="W06-1906.50">result</entity>.
</abstract>


</text>

<text id="I08-4033"><title>
Achilles: NiCT/ATR <entity id="I08-4033.1">Chinese</entity> Morphological <entity id="I08-4033.2">Analyzer</entity> for the Fourth Sighan Bakeoff
</title><abstract>
We created a new <entity id="I08-4033.3">Chinese</entity> morphological <entity id="I08-4033.4">analyzer</entity>, Achilles , by integrating <entity id="I08-4033.5">rule-based</entity>, <entity id="I08-4033.6">dictionary-based</entity>, and <entity id="I08-4033.7">statistical</entity> <entity id="I08-4033.8">machine</entity> learning <entity id="I08-4033.9">method</entity>, <entity id="I08-4033.10">conditional random fields</entity> (CRF). The <entity id="I08-4033.11">rule-based method</entity> is used to recognize regular <entity id="I08-4033.12">expressions</entity>: <entity id="I08-4033.13">numbers</entity>, <entity id="I08-4033.14">time</entity> and alphabets. The <entity id="I08-4033.15">dictionary-based method</entity> is used to find <entity id="I08-4033.16">in-vocabulary</entity> (IV) <entity id="I08-4033.17">words</entity> while <entity id="I08-4033.18">out-of-vocabulary</entity> (OOV) <entity id="I08-4033.19">words</entity> are detected by the CRFs. At last, <entity id="I08-4033.20">confidence</entity> measure <entity id="I08-4033.21">based</entity> <entity id="I08-4033.22">approach</entity> is used to weigh all the <entity id="I08-4033.23">results</entity> and <entity id="I08-4033.24">output</entity> the best ones. Achilles was used and <entity id="I08-4033.25">evaluated</entity> in the bakeoff. We participated the closed tracks of <entity id="I08-4033.26">word segmentation</entity> and <entity id="I08-4033.27">part-of-speech</entity> <entity id="I08-4033.28">tagging</entity> for all the <entity id="I08-4033.29">provided</entity> <entity id="I08-4033.30">corpus</entity>. In <entity id="I08-4033.31">spite</entity> of an unexpected file encoding <entity id="I08-4033.32">errors</entity>, the <entity id="I08-4033.33">system</entity> exhibited a top <entity id="I08-4033.34">level</entity> <entity id="I08-4033.35">performance</entity>. A higher <entity id="I08-4033.36">word segmentation</entity> <entity id="I08-4033.37">accuracy</entity> for the <entity id="I08-4033.38">corpus</entity> ckip and ncc were achieved. We are <entity id="I08-4033.39">ranked</entity> at the fifth and eighth position out of all 19 and 26 <entity id="I08-4033.40">submissions</entity> respectively for the two <entity id="I08-4033.41">corpus</entity>. Achilles uses a <entity id="I08-4033.42">feature</entity> combined <entity id="I08-4033.43">approach</entity> for <entity id="I08-4033.44">part-of-speech</entity> <entity id="I08-4033.45">tagging</entity>. Our <entity id="I08-4033.46">post-evaluation results</entity> prove the <entity id="I08-4033.47">effectiveness</entity> of this <entity id="I08-4033.48">approach</entity> for <entity id="I08-4033.49">POS tagging</entity>.
</abstract>


</text>

<text id="P07-1078"><title>
Self-<entity id="P07-1078.1">Training</entity> for <entity id="P07-1078.2">Enhancement</entity> and <entity id="P07-1078.3">Domain Adaptation</entity> of <entity id="P07-1078.4">Statistical</entity> Parsers <entity id="P07-1078.5">Trained</entity> on Small Datasets
</title><abstract>
Creating large <entity id="P07-1078.6">amounts</entity> of annotated <entity id="P07-1078.7">data</entity> to <entity id="P07-1078.8">train</entity> <entity id="P07-1078.9">statistical</entity> PCFG <entity id="P07-1078.10">parsers</entity> is expensive, and the <entity id="P07-1078.11">performance</entity> of such <entity id="P07-1078.12">parsers</entity> declines when <entity id="P07-1078.13">training</entity> and <entity id="P07-1078.14">test</entity><entity id="P07-1078.15">data</entity> are taken from different <entity id="P07-1078.16">domains</entity>. In this <entity id="P07-1078.17">paper</entity> we use <entity id="P07-1078.18">self-training</entity> in <entity id="P07-1078.19">order</entity> to <entity id="P07-1078.20">improve</entity> the <entity id="P07-1078.21">quality</entity> of a <entity id="P07-1078.22">parser</entity> and to <entity id="P07-1078.23">adapt</entity> it to a different <entity id="P07-1078.24">domain</entity>, using only small <entity id="P07-1078.25">amounts</entity> of manually annotated <entity id="P07-1078.26">seed</entity><entity id="P07-1078.27">data</entity>. We <entity id="P07-1078.28">report</entity> significant <entity id="P07-1078.29">improvement</entity> both when the <entity id="P07-1078.30">seed</entity> and <entity id="P07-1078.31">test</entity><entity id="P07-1078.32">data</entity> are in the same <entity id="P07-1078.33">domain</entity> and in the <entity id="P07-1078.34">out-of-domain adaptation</entity> <entity id="P07-1078.35">scenario</entity>. In particular, we achieve 50% <entity id="P07-1078.36">reduction</entity> in annotation <entity id="P07-1078.37">cost</entity> for the <entity id="P07-1078.38">in-domain</entity> <entity id="P07-1078.39">case</entity>, <entity id="P07-1078.40">yielding</entity> an <entity id="P07-1078.41">improvement</entity> of 66% over previous work, and a 20-33% <entity id="P07-1078.42">reduction</entity> for the <entity id="P07-1078.43">domain adaptation</entity> <entity id="P07-1078.44">case</entity>. This is the first <entity id="P07-1078.45">time</entity> that <entity id="P07-1078.46">self-training</entity> with small labeled datasets is <entity id="P07-1078.47">applied</entity> successfully to these <entity id="P07-1078.48">tasks</entity>. We were also able to formulate a <entity id="P07-1078.49">characterization</entity> of when <entity id="P07-1078.50">self-training</entity> is valuable.
</abstract>


</text>

<text id="W05-1104"><title>
Designing an Extensible API for Integrating <entity id="W05-1104.1">Language Modeling</entity> and <entity id="W05-1104.2">Realization</entity></title><abstract>
"We present an extensible API for integrating <entity id="W05-1104.3">language modeling</entity> and <entity id="W05-1104.4">realization</entity>, describing its <entity id="W05-1104.5">design</entity> and efficient <entity id="W05-1104.6">implementation</entity> in the OpenCCG <entity id="W05-1104.7">surface</entity> realizer. With OpenCCG, <entity id="W05-1104.8">language models</entity> may be used to select <entity id="W05-1104.9">realizations</entity> with preferred <entity id="W05-1104.10">word</entity> <entity id="W05-1104.11">orders</entity>, promote <entity id="W05-1104.12">alignment</entity> with a conversational partner, avoid repetitive <entity id="W05-1104.13">language</entity> use, and <entity id="W05-1104.14">increase</entity> the <entity id="W05-1104.15">speed</entity> of the best-first anytime <entity id="W05-1104.16">search</entity>. The API enables a <entity id="W05-1104.17">variety</entity> of <entity id="W05-1104.18">n-gram models</entity> to be easily combined and used in <entity id="W05-1104.19">conjunction</entity> with appropriate <entity id="W05-1104.20">edge</entity> pruning <entity id="W05-1104.21">strategies</entity>. The <entity id="W05-1104.22">n-gram models</entity> may be of any <entity id="W05-1104.23">order</entity>, operate in reverse (""right-to-left""), and selectively replace certain <entity id="W05-1104.24">words</entity> with their <entity id="W05-1104.25">semantic classes</entity>. Factored <entity id="W05-1104.26">language models</entity> with <entity id="W05-1104.27">generalized</entity> backoff may also be employed, over <entity id="W05-1104.28">words</entity> represented as bundles of <entity id="W05-1104.29">factors</entity> such as <entity id="W05-1104.30">form</entity>, <entity id="W05-1104.31">pitch</entity> accent, <entity id="W05-1104.32">stem</entity>, <entity id="W05-1104.33">part of speech</entity>, supertag, and <entity id="W05-1104.34">semantic class</entity>. "
</abstract>


</text>

<text id="L08-1237"><title>
A Semantically Annotated Swedish Medical <entity id="L08-1237.1">Corpus</entity></title>
<abstract>
With the <entity id="L08-1237.2">information</entity> overload in the life <entity id="L08-1237.3">sciences</entity> there is an <entity id="L08-1237.4">increasing</entity> need for annotated <entity id="L08-1237.5">corpora</entity>, particularly with biological and biomedical <entity id="L08-1237.6">entities</entity>, which is the driving force for <entity id="L08-1237.7">data-driven</entity> <entity id="L08-1237.8">language processing</entity> <entity id="L08-1237.9">applications</entity> and the empirical <entity id="L08-1237.10">approach</entity> to <entity id="L08-1237.11">language</entity> <entity id="L08-1237.12">study</entity>. Inspired by the work in the GENIA <entity id="L08-1237.13">Corpus</entity>, MEDLEX <entity id="L08-1237.14">Corpus</entity>.
</abstract>


</text>

<text id="L08-1265"><title><entity id="L08-1265.1">Ontology</entity> <entity id="L08-1265.2">Search</entity> with the OntoSelect <entity id="L08-1265.3">Ontology</entity> <entity id="L08-1265.4">Library</entity></title>
<abstract>
OntoSelect is a dynamic web-based <entity id="L08-1265.5">ontology</entity> <entity id="L08-1265.6">library</entity> that harvests, analyzes and organizes <entity id="L08-1265.7">ontologies</entity> published on the <entity id="L08-1265.8">Semantic</entity> Web. OntoSelect allows <entity id="L08-1265.9">searching</entity> as well as browsing of <entity id="L08-1265.10">ontologies</entity> according to <entity id="L08-1265.11">size</entity> (<entity id="L08-1265.12">number</entity> of <entity id="L08-1265.13">classes</entity>, <entity id="L08-1265.14">properties</entity>), <entity id="L08-1265.15">representation</entity> <entity id="L08-1265.16">format</entity> (DAML, RDFS, OWL), connectedness (score over the <entity id="L08-1265.17">number</entity> of <entity id="L08-1265.18">included</entity> and referring <entity id="L08-1265.19">ontologies</entity>) and <entity id="L08-1265.20">human languages</entity> used for <entity id="L08-1265.21">class-</entity> and <entity id="L08-1265.22">object</entity> <entity id="L08-1265.23">property-labels</entity>. <entity id="L08-1265.24">Ontology</entity> <entity id="L08-1265.25">search</entity> in OntoSelect is <entity id="L08-1265.26">based</entity> on a combined measure of <entity id="L08-1265.27">coverage</entity>, <entity id="L08-1265.28">structure</entity> connectedness.
</abstract>


</text>

<text id="C88-1071"><title><entity id="C88-1071.1">Speech Recognition</entity> And The <entity id="C88-1071.2">Frequency</entity> Of Recently Used <entity id="C88-1071.3">Words</entity>: A Modified <entity id="C88-1071.4">Markov Model</entity> For <entity id="C88-1071.5">Natural Language</entity></title>
<abstract><entity id="C88-1071.6">Speech recognition systems</entity> incorporate a <entity id="C88-1071.7">language model</entity> which, at each stage of the <entity id="C88-1071.8">recognition</entity> <entity id="C88-1071.9">task</entity>, assigns a <entity id="C88-1071.10">probability</entity> of <entity id="C88-1071.11">occurrence</entity> to each <entity id="C88-1071.12">word</entity> in the <entity id="C88-1071.13">vocabulary</entity>. A <entity id="C88-1071.14">class</entity> of Markov <entity id="C88-1071.15">language models</entity> identified by Jelinek has achieved consider able <entity id="C88-1071.16">success</entity> in this <entity id="C88-1071.17">domain</entity>. A <entity id="C88-1071.18">modification</entity> of the Markov <entity id="C88-1071.19">approach</entity>, which assigns higher <entity id="C88-1071.20">probabilities</entity> to recently used <entity id="C88-1071.21">words</entity>, is <entity id="C88-1071.22">proposed</entity> and <entity id="C88-1071.23">tested</entity> against a pure <entity id="C88-1071.24">Markov model</entity>. <entity id="C88-1071.25">Parameter</entity> <entity id="C88-1071.26">calculation</entity> and <entity id="C88-1071.27">comparison</entity> of the two <entity id="C88-1071.28">models</entity> both involve use of the LOB <entity id="C88-1071.29">Corpus</entity> of <entity id="C88-1071.30">tagged</entity> modern <entity id="C88-1071.31">English</entity>.
</abstract>


</text>

<text id="A00-3001"><title>
Experimenting With The <entity id="A00-3001.1">Interaction</entity> Between <entity id="A00-3001.2">Aggregation</entity> And <entity id="A00-3001.3">Text</entity> Structuring
</title>
<abstract>
In <entity id="A00-3001.4">natural language generation</entity>, different <entity id="A00-3001.5">generation</entity> <entity id="A00-3001.6">tasks</entity> often interact with each other in a <entity id="A00-3001.7">complex</entity> way, which is hard to capture in the <entity id="A00-3001.8">pipeline</entity> <entity id="A00-3001.9">architecture</entity> described by Reiter ( Reiter, 1994 ). This <entity id="A00-3001.10">paper</entity> <entity id="A00-3001.11">focuses</entity> on the <entity id="A00-3001.12">interaction</entity> between a specific <entity id="A00-3001.13">type</entity> of <entity id="A00-3001.14">aggregation</entity> and <entity id="A00-3001.15">text</entity> planning, in particular, maintaining local <entity id="A00-3001.16">coherence</entity>, and tries to explore what <entity id="A00-3001.17">preferences</entity> exist among the <entity id="A00-3001.18">factors</entity> related to the two <entity id="A00-3001.19">tasks</entity>. The <entity id="A00-3001.20">evaluation result</entity> shows that it is these <entity id="A00-3001.21">preferences</entity> that decide the <entity id="A00-3001.22">quality</entity> of the <entity id="A00-3001.23">generated</entity> <entity id="A00-3001.24">text</entity> and capturing them properly in a <entity id="A00-3001.25">generation system</entity> could lead to coherent <entity id="A00-3001.26">text</entity>.
</abstract>



</text>

<text id="H93-1065"><title>
Quantitative <entity id="H93-1065.1">Modeling</entity> Of Segmental <entity id="H93-1065.2">Duration</entity></title>
<abstract>
In <entity id="H93-1065.3">natural</entity> <entity id="H93-1065.4">speech</entity>, <entity id="H93-1065.5">durations</entity> of phonetic <entity id="H93-1065.6">segments</entity> are strongly dependent on contextual <entity id="H93-1065.7">factors</entity>. Quantitative <entity id="H93-1065.8">descriptions</entity> of these contextual <entity id="H93-1065.9">effects</entity> have <entity id="H93-1065.10">applications</entity> in <entity id="H93-1065.11">text-to-speech</entity> <entity id="H93-1065.12">synthesis</entity> and in <entity id="H93-1065.13">automatic speech  recognition</entity>. In this <entity id="H93-1065.14">paper</entity>, we describe a speaker-dependent <entity id="H93-1065.15">system</entity> for predicting segmental <entity id="H93-1065.16">duration</entity> from <entity id="H93-1065.17">text</entity>, with emphasis on the <entity id="H93-1065.18">statistical methods</entity> used for its <entity id="H93-1065.19">construction</entity>. We also <entity id="H93-1065.20">report</entity> <entity id="H93-1065.21">results</entity> of a subjective listening <entity id="H93-1065.22">experiment</entity> <entity id="H93-1065.23">evaluating</entity> an <entity id="H93-1065.24">implementation</entity> of this <entity id="H93-1065.25">system</entity> for <entity id="H93-1065.26">text-to-speech</entity> <entity id="H93-1065.27">synthesis</entity> <entity id="H93-1065.28">purposes</entity>.
</abstract>


</text>

<text id="C00-1068"><title>
Flexible Mixed-<entity id="C00-1068.1">Initiative</entity> <entity id="C00-1068.2">Dialogue Management</entity> Using <entity id="C00-1068.3">Concept-</entity><entity id="C00-1068.4">Level</entity> <entity id="C00-1068.5">Confidence</entity> Measures Of <entity id="C00-1068.6">Speech</entity> Recognizer <entity id="C00-1068.7">Output</entity></title>
<abstract>
We present a <entity id="C00-1068.8">method</entity> to realize flexible <entity id="C00-1068.9">mixed-initiative</entity> <entity id="C00-1068.10">dialogue</entity>, in which the <entity id="C00-1068.11">system</entity> can make effective <entity id="C00-1068.12">confirmation</entity> and guidance using <entity id="C00-1068.13">concept-level</entity> <entity id="C00-1068.14">confidence</entity> measures (CMs) derived from <entity id="C00-1068.15">speech</entity> recognizer <entity id="C00-1068.16">output</entity> in <entity id="C00-1068.17">order</entity> to handle <entity id="C00-1068.18">speech recognition</entity> <entity id="C00-1068.19">errors</entity>. We define two <entity id="C00-1068.20">concept-level</entity> CMs, which are on <entity id="C00-1068.21">content-words</entity> and on <entity id="C00-1068.22">semantic-attributes</entity>, using 10-best <entity id="C00-1068.23">outputs</entity> of the <entity id="C00-1068.24">speech</entity> recognizer and <entity id="C00-1068.25">parsing</entity> with <entity id="C00-1068.26">phrase-level</entity> grammars. <entity id="C00-1068.27">Content-word</entity> CM is useful for selecting plausible <entity id="C00-1068.28">interpretations</entity>. Less confident <entity id="C00-1068.29">interpretations</entity> are given to <entity id="C00-1068.30">confirmation</entity> <entity id="C00-1068.31">process</entity>. The <entity id="C00-1068.32">strategy</entity> <entity id="C00-1068.33">improved</entity> the <entity id="C00-1068.34">interpretation</entity> <entity id="C00-1068.35">accuracy</entity> by 11.5%. Moreover, the <entity id="C00-1068.36">semantic-attribute</entity> CM is used to estimate <entity id="C00-1068.37">user</entity>'s intention and <entity id="C00-1068.38">generates</entity> <entity id="C00-1068.39">system-initiative</entity> guidances even when successful <entity id="C00-1068.40">interpretation</entity> is not obtained.
</abstract>



</text>

<text id="W93-0109"><title>
The <entity id="W93-0109.1">Automatic</entity> <entity id="W93-0109.2">Acquisition</entity> Of Frequencies Of <entity id="W93-0109.3">Verb</entity> Subcategorization Frames From Tagged <entity id="W93-0109.4">Corpora</entity></title>
<abstract>
We describe a <entity id="W93-0109.5">mechanism</entity> for automatically acquiring <entity id="W93-0109.6">verb</entity> subcategorization <entity id="W93-0109.7">frames</entity> and their <entity id="W93-0109.8">frequencies</entity> in a large <entity id="W93-0109.9">corpus</entity>. A <entity id="W93-0109.10">tagged</entity> <entity id="W93-0109.11">corpus</entity> is first partially parsed to identify <entity id="W93-0109.12">noun phrases</entity> and then a linear grammar is used to estimate the appropriate subcategorization <entity id="W93-0109.13">frame</entity> for each <entity id="W93-0109.14">verb</entity> token in the <entity id="W93-0109.15">corpus</entity>. In an <entity id="W93-0109.16">experiment</entity> involving the <entity id="W93-0109.17">identification</entity> of six fixed subcategorization <entity id="W93-0109.18">frames</entity>, our <entity id="W93-0109.19">current</entity> <entity id="W93-0109.20">system</entity> showed more than 80% <entity id="W93-0109.21">accuracy</entity>. In <entity id="W93-0109.22">addition</entity>, a new <entity id="W93-0109.23">statistical</entity> <entity id="W93-0109.24">approach</entity> substantially <entity id="W93-0109.25">improves</entity> the <entity id="W93-0109.26">accuracy</entity> of the <entity id="W93-0109.27">frequency</entity> <entity id="W93-0109.28">estimation</entity>.
</abstract>



</text>

<text id="C90-3053"><title>
The <entity id="C90-3053.1">Semantic Representation</entity> Of Spatial Configurations: A Conceptual <entity id="C90-3053.2">Motivation</entity> For <entity id="C90-3053.3">Generation</entity> In <entity id="C90-3053.4">Machine Translation</entity></title>
<abstract>
This <entity id="C90-3053.5">paper</entity> <entity id="C90-3053.6">deals</entity> with the <entity id="C90-3053.7">automatic</entity> <entity id="C90-3053.8">translation</entity> of <entity id="C90-3053.9">prepositions</entity>, which are highly polysemous. Moreover, the same real <entity id="C90-3053.10">situation</entity> is often expressed by different <entity id="C90-3053.11">prepositions</entity> in different <entity id="C90-3053.12">languages</entity>. We proceed from the <entity id="C90-3053.13">hypothesis</entity> that different <entity id="C90-3053.14">usage</entity> <entity id="C90-3053.15">patterns</entity> are <entity id="C90-3053.16">due</entity> to different conceptualizations of the same real <entity id="C90-3053.17">situation</entity>. Following cognitive <entity id="C90-3053.18">principles</entity> of spatial conceptualization, we <entity id="C90-3053.19">design</entity> a <entity id="C90-3053.20">semantic interpretation</entity> <entity id="C90-3053.21">process</entity> for spatial <entity id="C90-3053.22">relations</entity> in which our <entity id="C90-3053.23">translation system</entity> uses <entity id="C90-3053.24">semantic features</entity> derived from a <entity id="C90-3053.25">semantic</entity> sort hierarchy. Thus we can differentiate subtle <entity id="C90-3053.26">distinctions</entity> between spatially significant <entity id="C90-3053.27">configurations</entity>.
</abstract>



</text>

<text id="W97-0210"><title>
Investigating Complementary <entity id="W97-0210.1">Methods</entity> For <entity id="W97-0210.2">Verb</entity> <entity id="W97-0210.3">Sense</entity> Pruning
</title>
<abstract>
We present an <entity id="W97-0210.4">approach</entity> for <entity id="W97-0210.5">tagging</entity> <entity id="W97-0210.6">verb</entity> <entity id="W97-0210.7">sense</entity> that combines a <entity id="W97-0210.8">domain-independent</entity> <entity id="W97-0210.9">method</entity> <entity id="W97-0210.10">based</entity> on subcategorization and <entity id="W97-0210.11">alternations</entity> with a <entity id="W97-0210.12">domain-dependent</entity> <entity id="W97-0210.13">method</entity> utilizing statistically <entity id="W97-0210.14">extracted</entity> <entity id="W97-0210.15">verb</entity> <entity id="W97-0210.16">clusters</entity>. Initial <entity id="W97-0210.17">results</entity> indicate that <entity id="W97-0210.18">verb</entity> senses can be pruned for highly polysemous <entity id="W97-0210.19">verbs</entity> by up to 74% by the first <entity id="W97-0210.20">method</entity> and by up to 85% by the second <entity id="W97-0210.21">method</entity>.
</abstract>



</text>

<text id="W02-0817"><title>
Building A <entity id="W02-0817.1">Sense</entity> Tagged <entity id="W02-0817.2">Corpus</entity> With Open Mind <entity id="W02-0817.3">Word</entity> <entity id="W02-0817.4">Expert</entity></title>
<abstract>
Open Mind <entity id="W02-0817.5">Word</entity> <entity id="W02-0817.6">Expert</entity> is an <entity id="W02-0817.7">implemented</entity> <entity id="W02-0817.8">active learning</entity> <entity id="W02-0817.9">system</entity> for collecting <entity id="W02-0817.10">word sense</entity> <entity id="W02-0817.11">tagging</entity> from the general public over the Web. It is available at http://teach-computers.org. We expect the <entity id="W02-0817.12">system</entity> to <entity id="W02-0817.13">yield</entity> a large <entity id="W02-0817.14">volume</entity> of <entity id="W02-0817.15">high-quality</entity> <entity id="W02-0817.16">training</entity> <entity id="W02-0817.17">data</entity> at a much lower <entity id="W02-0817.18">cost</entity> than the traditional <entity id="W02-0817.19">method</entity> of hiring lexicographers. We thus <entity id="W02-0817.20">propose</entity> a Senseval-3 <entity id="W02-0817.21">lexical</entity> <entity id="W02-0817.22">sample</entity> activity where the <entity id="W02-0817.23">training</entity> <entity id="W02-0817.24">data</entity> is collected via Open Mind <entity id="W02-0817.25">Word</entity> <entity id="W02-0817.26">Expert</entity>. If successful, the <entity id="W02-0817.27">collection</entity> <entity id="W02-0817.28">process</entity> can be extended to create the definitive <entity id="W02-0817.29">corpus</entity> of <entity id="W02-0817.30">word sense</entity> <entity id="W02-0817.31">information</entity>.
</abstract>



</text>

<text id="W98-1126"><title><entity id="W98-1126.1">Mapping</entity> Collocational Properties Into <entity id="W98-1126.2">Machine Learning</entity> <entity id="W98-1126.3">Features</entity></title>
<abstract>
This <entity id="W98-1126.4">paper</entity> investigates <entity id="W98-1126.5">interactions</entity> between collocational <entity id="W98-1126.6">properties</entity> and <entity id="W98-1126.7">methods</entity> for organizing them into <entity id="W98-1126.8">features</entity> for <entity id="W98-1126.9">machine learning</entity>. In <entity id="W98-1126.10">experiments</entity> <entity id="W98-1126.11">performing</entity> an <entity id="W98-1126.12">event</entity> <entity id="W98-1126.13">categorization</entity> <entity id="W98-1126.14">task</entity>, Wiebe et al. (1997a) found that different <entity id="W98-1126.15">organizations</entity> are best for different <entity id="W98-1126.16">properties</entity>. This <entity id="W98-1126.17">paper</entity> presents a <entity id="W98-1126.18">statistical</entity> <entity id="W98-1126.19">analysis</entity> of the <entity id="W98-1126.20">results</entity> across different <entity id="W98-1126.21">machine</entity> learning <entity id="W98-1126.22">algorithms</entity>. In the <entity id="W98-1126.23">experiments</entity>, the <entity id="W98-1126.24">relationship</entity> between <entity id="W98-1126.25">property</entity> and <entity id="W98-1126.26">organization</entity> was strikingly consistent across <entity id="W98-1126.27">algorithms</entity>. This prompted further <entity id="W98-1126.28">analysis</entity> of this <entity id="W98-1126.29">relationship</entity>, and an <entity id="W98-1126.30">investigation</entity> of <entity id="W98-1126.31">criteria</entity> for recognizing beneficial ways to <entity id="W98-1126.32">include</entity> collocational <entity id="W98-1126.33">properties</entity> in <entity id="W98-1126.34">machine</entity> learning <entity id="W98-1126.35">experiments</entity>. While many <entity id="W98-1126.36">types</entity> of collocational <entity id="W98-1126.37">properties</entity> and <entity id="W98-1126.38">methods</entity> of organizing them into <entity id="W98-1126.39">features</entity> have been used in NLP, systematic <entity id="W98-1126.40">investigations</entity> of their <entity id="W98-1126.41">interaction</entity> are rare.
</abstract>



</text>

<text id="L08-1203">
<title>
KnoFusius: a New <entity id="L08-1203.1">Knowledge</entity> <entity id="L08-1203.2">Fusion</entity> <entity id="L08-1203.3">System</entity> for <entity id="L08-1203.4">Interpretation</entity> of <entity id="L08-1203.5">Gene</entity> <entity id="L08-1203.6">Expression</entity> <entity id="L08-1203.7">Data</entity></title>
<abstract>
This <entity id="L08-1203.8">paper</entity> introduces a new <entity id="L08-1203.9">architecture</entity> that aims at combining molecular <entity id="L08-1203.10">biology</entity> <entity id="L08-1203.11">data</entity> with <entity id="L08-1203.12">information</entity> automatically <entity id="L08-1203.13">extracted</entity> from relevant <entity id="L08-1203.14">scientific literature</entity> ( using <entity id="L08-1203.15">text</entity> mining <entity id="L08-1203.16">techniques</entity> on PubMed <entity id="L08-1203.17">abstracts</entity> and fulltext <entity id="L08-1203.18">papers</entity> ) to <entity id="L08-1203.19">help</entity> biomedical <entity id="L08-1203.20">experts</entity> to interpret <entity id="L08-1203.21">experimental</entity> <entity id="L08-1203.22">results</entity> in <entity id="L08-1203.23">hand</entity> . The infrastructural <entity id="L08-1203.24">level</entity> bears on <entity id="L08-1203.25">semantic-web</entity> <entity id="L08-1203.26">technologies</entity> and <entity id="L08-1203.27">standards</entity> that facilitate the actual <entity id="L08-1203.28">fusion</entity> of the <entity id="L08-1203.29">multi-source</entity> <entity id="L08-1203.30">knowledge</entity> .
</abstract>


</text>

<text id="L08-1204">
<title>
Modelling <entity id="L08-1204.1">Word</entity> <entity id="L08-1204.2">Similarity</entity> : an <entity id="L08-1204.3">Evaluation</entity> of <entity id="L08-1204.4">Automatic</entity> Synonymy <entity id="L08-1204.5">Extraction</entity> <entity id="L08-1204.6">Algorithms</entity> .
</title>
<abstract><entity id="L08-1204.7">Vector-based models</entity> of <entity id="L08-1204.8">lexical semantics</entity> retrieve semantically related <entity id="L08-1204.9">words</entity> automatically from large <entity id="L08-1204.10">corpora</entity> by exploiting the <entity id="L08-1204.11">property</entity> that <entity id="L08-1204.12">words</entity> with a similar meaning tend to occur in similar <entity id="L08-1204.13">contexts</entity> . Despite their <entity id="L08-1204.14">increasing</entity> <entity id="L08-1204.15">popularity</entity> , it is unclear which <entity id="L08-1204.16">kind</entity> of <entity id="L08-1204.17">semantic similarity</entity> they actually capture and for which <entity id="L08-1204.18">kind</entity> of <entity id="L08-1204.19">words</entity> . In this <entity id="L08-1204.20">paper</entity> , we use three <entity id="L08-1204.21">vector-based models</entity> to retrieve semantically related <entity id="L08-1204.22">words</entity> for a set of Dutch <entity id="L08-1204.23">nouns</entity> and we analyse whether three linguistic  <entity id="L08-1204.24">properties</entity> of the <entity id="L08-1204.25">nouns</entity> <entity id="L08-1204.26">influence</entity> the <entity id="L08-1204.27">results</entity> . In particular, we compare <entity id="L08-1204.28">results</entity> from a <entity id="L08-1204.29">dependency-based model</entity>  with those from a 1st and 2nd <entity id="L08-1204.30">order</entity> <entity id="L08-1204.31">bag-of-words</entity> <entity id="L08-1204.32">model</entity> and we examine the <entity id="L08-1204.33">effect</entity> of the <entity id="L08-1204.34">nouns</entity> ' <entity id="L08-1204.35">frequency</entity> , <entity id="L08-1204.36">semantic</entity> speficity and <entity id="L08-1204.37">semantic class</entity> . We find that all three <entity id="L08-1204.38">models</entity> find more <entity id="L08-1204.39">synonyms</entity> for <entity id="L08-1204.40">high-frequency</entity> <entity id="L08-1204.41">nouns</entity> and those belonging to <entity id="L08-1204.42">abstract</entity> <entity id="L08-1204.43">semantic</entity> classses. <entity id="L08-1204.44">Semantic</entity> specificty does not have a clear <entity id="L08-1204.45">influence</entity> .
</abstract>


</text>

<text id="L08-1205">
<title>
Childrens Oral Reading <entity id="L08-1205.1">Corpus</entity> (CHOREC): <entity id="L08-1205.2">Description</entity> and <entity id="L08-1205.3">Assessment</entity> of Annotator <entity id="L08-1205.4">Agreement</entity></title>
<abstract>
Within the <entity id="L08-1205.5">scope</entity> of the <entity id="L08-1205.6">SPACE</entity> <entity id="L08-1205.7">project</entity> , the CHildren's Oral REading <entity id="L08-1205.8">Corpus</entity> (CHOREC) is <entity id="L08-1205.9">developed</entity> . This <entity id="L08-1205.10">database</entity> contains <entity id="L08-1205.11">recorded</entity> , transcribed and annotated read <entity id="L08-1205.12">speech </entity> (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading <entity id="L08-1205.13">difficulties</entity> .  <entity id="L08-1205.14">Analyses</entity> of inter- and intra-annotator <entity id="L08-1205.15">agreement</entity> are carried out in <entity id="L08-1205.16">order</entity> to investigate the  <entity id="L08-1205.17">consistency</entity>  with which reading <entity id="L08-1205.18">errors</entity> are detected, orthographic and phonetic <entity id="L08-1205.19">transcriptions</entity> are made, and reading <entity id="L08-1205.20">errors</entity> and reading <entity id="L08-1205.21">strategies</entity> are labeled. <entity id="L08-1205.22">Percentage</entity> <entity id="L08-1205.23">agreement</entity> scores and <entity id="L08-1205.24">kappa</entity> values both show that <entity id="L08-1205.25">agreement</entity> between annotations, and therefore the <entity id="L08-1205.26">quality</entity> of the annotations, is high. Taken all double or <entity id="L08-1205.27">triple</entity> annotations (for 10% resp. 30% of the <entity id="L08-1205.28">corpus</entity> ) together, % <entity id="L08-1205.29">agreement</entity> varies between 86.4% and 98.6%, whereas <entity id="L08-1205.30">kappa</entity> varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School <entity id="L08-1205.31">type</entity> and reading  <entity id="L08-1205.32">type</entity> seem to account for systematic <entity id="L08-1205.33">differences</entity> in % <entity id="L08-1205.34">agreement</entity> , but these <entity id="L08-1205.35">differences</entity> disappear when <entity id="L08-1205.36">kappa</entity> values are calculated that correct for chance <entity id="L08-1205.37">agreement</entity> . To conclude, an  <entity id="L08-1205.38">analysis</entity> of the annotation <entity id="L08-1205.39">differences</entity>  with <entity id="L08-1205.40">respect</entity> to the '*s' label (i.e. a label that is used to annotate undistinguishable spelling <entity id="L08-1205.41">behaviour</entity> ), <entity id="L08-1205.42">phoneme</entity> labels, reading <entity id="L08-1205.43">strategy</entity> and <entity id="L08-1205.44">error</entity> labels is given.
</abstract>


</text>

<text id="L08-1206">
<title>
A Bilingual  <entity id="L08-1206.1">Corpus</entity> of Inter-linked <entity id="L08-1206.2">Events</entity></title>
<abstract>
This <entity id="L08-1206.3">paper</entity> describes the <entity id="L08-1206.4">creation</entity>  of a bilingual <entity id="L08-1206.5">corpus</entity>  of inter-linked <entity id="L08-1206.6">events</entity>  for Italian and <entity id="L08-1206.7">English</entity> . Linkage is accomplished through the Inter- <entity id="L08-1206.8">Lingual</entity> <entity id="L08-1206.9">Index</entity> (ILI) that <entity id="L08-1206.10">links</entity> ItalWordNet with WordNet. The <entity id="L08-1206.11">availability</entity> of this <entity id="L08-1206.12">resource</entity> , on the one <entity id="L08-1206.13">hand</entity> , enables  contrastive <entity id="L08-1206.14">analysis</entity>  of the linguistic <entity id="L08-1206.15">phenomena</entity>  surrounding <entity id="L08-1206.16">events</entity> in both <entity id="L08-1206.17">languages</entity> , and on the other <entity id="L08-1206.18">hand</entity> , can be used to <entity id="L08-1206.19">perform</entity> multilingual temporal <entity id="L08-1206.20">analysis</entity> of <entity id="L08-1206.21">texts</entity> . In <entity id="L08-1206.22">addition</entity> to describing the <entity id="L08-1206.23">methodology</entity> for <entity id="L08-1206.24">construction</entity> of the inter-linked <entity id="L08-1206.25">corpus</entity> and the <entity id="L08-1206.26">analysis</entity> of the <entity id="L08-1206.27">data</entity> collected, we demonstrate that the ILI could potentially be used to <entity id="L08-1206.28">bootstrap</entity> the <entity id="L08-1206.29">creation</entity> of comparable <entity id="L08-1206.30">corpora</entity> by exporting <entity id="L08-1206.31">layers</entity> of annotation for <entity id="L08-1206.32">words</entity> that have the same <entity id="L08-1206.33">sense</entity> .
</abstract>


</text>

<text id="L08-1207">
<title>
New Resources for <entity id="L08-1207.1">Document Classification</entity> , <entity id="L08-1207.2">Analysis</entity> and <entity id="L08-1207.3">Translation</entity> Technologies
</title>
<abstract>
The <entity id="L08-1207.4">goal</entity> of the DARPA MADCAT (Multilingual <entity id="L08-1207.5">Automatic</entity> <entity id="L08-1207.6">Document Classification</entity> <entity id="L08-1207.7">Analysis</entity> and <entity id="L08-1207.8">Translation</entity> ) <entity id="L08-1207.9">Program</entity> is to automatically convert <entity id="L08-1207.10">foreign language</entity> <entity id="L08-1207.11">text</entity> images into <entity id="L08-1207.12">English</entity> <entity id="L08-1207.13">transcripts</entity> , for use by humans and downstream <entity id="L08-1207.14">applications</entity> . The first <entity id="L08-1207.15">phase</entity> the <entity id="L08-1207.16">program</entity> <entity id="L08-1207.17">focuses</entity> on <entity id="L08-1207.18">translation</entity> of handwritten Arabic <entity id="L08-1207.19">documents</entity> . Linguistic <entity id="L08-1207.20">Data</entity> Consortium (LDC) is creating publicly available <entity id="L08-1207.21">linguistic resources</entity> for MADCAT <entity id="L08-1207.22">technologies</entity> , on a <entity id="L08-1207.23">scale</entity> and <entity id="L08-1207.24">richness</entity> not previously available. <entity id="L08-1207.25">Corpora</entity> will consist of existing LDC <entity id="L08-1207.26">corpora</entity> and <entity id="L08-1207.27">data</entity> donations from MADCAT partners, plus new <entity id="L08-1207.28">data</entity> <entity id="L08-1207.29">collection</entity> to <entity id="L08-1207.30">provide</entity> <entity id="L08-1207.31">high quality</entity> material for <entity id="L08-1207.32">evaluation</entity> and to address strategic <entity id="L08-1207.33">gaps</entity> (for <entity id="L08-1207.34">genre</entity> , <entity id="L08-1207.35">dialect</entity> , image <entity id="L08-1207.36">quality</entity> , etc.) in the existing <entity id="L08-1207.37">resources</entity> . <entity id="L08-1207.38">Training</entity> and <entity id="L08-1207.39">test</entity> <entity id="L08-1207.40">data</entity> <entity id="L08-1207.41">properties</entity> will expand over <entity id="L08-1207.42">time</entity> to encompass a wide range of <entity id="L08-1207.43">topics</entity> and <entity id="L08-1207.44">genres</entity> : letters, diaries, <entity id="L08-1207.45">training</entity> <entity id="L08-1207.46">manuals</entity> , brochures, signs, ledgers, memos, <entity id="L08-1207.47">instructions</entity> , post
cards and <entity id="L08-1207.48">forms</entity> among others. <entity id="L08-1207.49">Data</entity> will be ground truthed, with line, <entity id="L08-1207.50">word</entity> and token segmentation and zoning, and <entity id="L08-1207.51">translations</entity> and <entity id="L08-1207.52">word alignments</entity> will be produced for a subset. <entity id="L08-1207.53">Evaluation</entity> <entity id="L08-1207.54">data</entity> will be carefully selected from the available <entity id="L08-1207.55">data</entity> pools and <entity id="L08-1207.56">high quality</entity> <entity id="L08-1207.57">references</entity> will be produced, which can be used to compare MADCAT <entity id="L08-1207.58">system</entity> <entity id="L08-1207.59">performance</entity> against the human-produced <entity id="L08-1207.60">gold standard</entity> .
</abstract>


</text>

<text id="L08-1208">
<title>
Approximating <entity id="L08-1208.1">Learning</entity> Curves for Active- <entity id="L08-1208.2">Learning-</entity> Driven Annotation
</title>
<abstract><entity id="L08-1208.3">Active learning</entity> (AL) is getting more and more popular as a <entity id="L08-1208.4">methodology</entity> to considerably reduce the annotation <entity id="L08-1208.5">effort</entity> when <entity id="L08-1208.6">building</entity> <entity id="L08-1208.7">training</entity> material for <entity id="L08-1208.8">statistical</entity> <entity id="L08-1208.9">learning methods</entity> for various <entity id="L08-1208.10">NLP tasks</entity> . A crucial <entity id="L08-1208.11">issue</entity> rarely addressed, however, is when to actually stop the annotation <entity id="L08-1208.12">process</entity> to profit from the <entity id="L08-1208.13">savings</entity> in <entity id="L08-1208.14">efforts</entity> . This <entity id="L08-1208.15">question</entity> is tightly related to estimating the <entity id="L08-1208.16">classifier</entity> <entity id="L08-1208.17">performance</entity> after a certain <entity id="L08-1208.18">amount</entity> of <entity id="L08-1208.19">data</entity> has already been annotated. While learning curves are the <entity id="L08-1208.20">default</entity> means to monitor the <entity id="L08-1208.21">progress</entity> of the annotation <entity id="L08-1208.22">process</entity> in <entity id="L08-1208.23">terms</entity> of <entity id="L08-1208.24">classifier</entity> <entity id="L08-1208.25">performance</entity> , this <entity id="L08-1208.26">requires</entity> a labeled <entity id="L08-1208.27">gold standard</entity> which - in realistic annotation settings, at least - is often unavailable. We here <entity id="L08-1208.28">propose</entity> a <entity id="L08-1208.29">method</entity> for committee-based AL to approximate the progression of the learning curve <entity id="L08-1208.30">based</entity> on the <entity id="L08-1208.31">disagreement</entity> among the committee members. This  <entity id="L08-1208.32">method</entity> relies on a separate, unlabeled <entity id="L08-1208.33">corpus</entity> and is thus well suited for <entity id="L08-1208.34">situations</entity> where a labeled <entity id="L08-1208.35">gold standard</entity> is not available or would be too expensive to obtain. Considering <entity id="L08-1208.36">named</entity> <entity id="L08-1208.37">entity</entity> <entity id="L08-1208.38">recognition</entity> as a <entity id="L08-1208.39">test</entity> <entity id="L08-1208.40">case</entity> we <entity id="L08-1208.41">provide</entity> empirical <entity id="L08-1208.42">evidence</entity> that this <entity id="L08-1208.43">approach</entity> works well under <entity id="L08-1208.44">simulation</entity> as well as under real-world annotation conditions.
</abstract>


</text>

<text id="L08-1209">
<title><entity id="L08-1209.1">Lexicon</entity> Schemas and <entity id="L08-1209.2">Related</entity> <entity id="L08-1209.3">Data</entity> <entity id="L08-1209.4">Models</entity> : when Standards Meet Users
</title>
<abstract><entity id="L08-1209.5">Lexicon</entity> <entity id="L08-1209.6">schemas</entity> and their use are discussed in this <entity id="L08-1209.7">paper</entity> from the <entity id="L08-1209.8">perspective</entity> of lexicographers and <entity id="L08-1209.9">field</entity> <entity id="L08-1209.10">linguists</entity> . A <entity id="L08-1209.11">variety</entity> of <entity id="L08-1209.12">lexicon</entity> <entity id="L08-1209.13">schemas</entity> have been <entity id="L08-1209.14">developed</entity> , with <entity id="L08-1209.15">goals</entity> ranging from <entity id="L08-1209.16">computational</entity> lexicography (DATR) through <entity id="L08-1209.17">archiving</entity> (LIFT, TEI) to standardization (LMF, FSR). A <entity id="L08-1209.18">number</entity> of <entity id="L08-1209.19">requirements</entity> for <entity id="L08-1209.20">lexicon</entity> <entity id="L08-1209.21">schemas</entity> are given. The <entity id="L08-1209.22">lexicon</entity> <entity id="L08-1209.23">schemas</entity> are introduced and compared to each other in <entity id="L08-1209.24">terms</entity> of <entity id="L08-1209.25">conversion</entity> and usability for this particular <entity id="L08-1209.26">user</entity> group, using a <entity id="L08-1209.27">common</entity> <entity id="L08-1209.28">lexicon</entity> <entity id="L08-1209.29">entry</entity> and <entity id="L08-1209.30">providing</entity> <entity id="L08-1209.31">examples</entity> for each <entity id="L08-1209.32">schema</entity> under <entity id="L08-1209.33">consideration</entity> . The <entity id="L08-1209.34">formats</entity> are assessed and the final <entity id="L08-1209.35">recommendation</entity> is given for the potential <entity id="L08-1209.36">users</entity> , namely to request <entity id="L08-1209.37">standard</entity> compliance from the <entity id="L08-1209.38">developers</entity> of the <entity id="L08-1209.39">tools</entity> used. This <entity id="L08-1209.40">paper</entity> should foster a <entity id="L08-1209.41">discussion</entity> between authors of <entity id="L08-1209.42">standards</entity> , lexicographers and <entity id="L08-1209.43">field</entity> <entity id="L08-1209.44">linguists</entity> .
</abstract>


</text>

<text id="L08-1211">
<title>
Arabic WordNet: <entity id="L08-1211.1">Semi-automatic</entity> <entity id="L08-1211.2">Extensions</entity> using Bayesian <entity id="L08-1211.3">Inference</entity></title>
<abstract>This <entity id="L08-1211.4">presentation</entity> <entity id="L08-1211.5">focuses</entity> on the <entity id="L08-1211.6">semi-automatic</entity> <entity id="L08-1211.7">extension</entity> of Arabic WordNet (AWN) using <entity id="L08-1211.8">lexical</entity> and morphological <entity id="L08-1211.9">rules</entity> and <entity id="L08-1211.10">applying</entity> Bayesian <entity id="L08-1211.11">inference</entity> . We briefly <entity id="L08-1211.12">report</entity> on the <entity id="L08-1211.13">current</entity> <entity id="L08-1211.14">status</entity> of AWN and <entity id="L08-1211.15">propose</entity> a way of extending its <entity id="L08-1211.16">coverage</entity> by taking <entity id="L08-1211.17">advantage</entity> of a limited set of highly productive Arabic morphological <entity id="L08-1211.18">rules</entity> for deriving a range of semantically related <entity id="L08-1211.19">word</entity> <entity id="L08-1211.20">forms</entity> from <entity id="L08-1211.21">verb</entity> <entity id="L08-1211.22">entries</entity> . The <entity id="L08-1211.23">application</entity> of this set of <entity id="L08-1211.24">rules</entity> , combined with the use of bilingual Arabic- <entity id="L08-1211.25">English</entity> <entity id="L08-1211.26">resources</entity> and Princeton's WordNet, allows the <entity id="L08-1211.27">generation</entity> of a graph representing the <entity id="L08-1211.28">semantic</entity> neighbourhood of the original <entity id="L08-1211.29">word</entity> . In previous work, a set of <entity id="L08-1211.30">associations</entity> between the hypothesized Arabic <entity id="L08-1211.31">words</entity> and <entity id="L08-1211.32">English</entity> synsets was <entity id="L08-1211.33">proposed</entity> on the <entity id="L08-1211.34">basis</entity> of this graph. Here, a novel <entity id="L08-1211.35">approach</entity> to extending AWN is presented whereby a Bayesian <entity id="L08-1211.36">Network</entity> is automatically built from the graph and then the net is used as an <entity id="L08-1211.37">inferencing</entity> <entity id="L08-1211.38">mechanism</entity> for scoring the set of <entity id="L08-1211.39">candidate</entity> <entity id="L08-1211.40">associations</entity> . Both on its own and in <entity id="L08-1211.41">combination</entity> with the previous <entity id="L08-1211.42">technique</entity> , this new <entity id="L08-1211.43">approach</entity> has led to <entity id="L08-1211.44">improved</entity> <entity id="L08-1211.45">results</entity> .
</abstract>


</text>

<text id="L08-1212">
<title>
Subjective <entity id="L08-1212.1">Evaluation</entity> of an Emotional <entity id="L08-1212.2">Speech</entity> <entity id="L08-1212.3">Database</entity> for Basque
///q</title>
<abstract>
This <entity id="L08-1212.4">paper</entity> describes the <entity id="L08-1212.5">evaluation</entity> <entity id="L08-1212.6">process</entity> of an emotional <entity id="L08-1212.7">speech</entity> <entity id="L08-1212.8">database</entity> <entity id="L08-1212.9">recorded</entity> for <entity id="L08-1212.10">standard</entity> Basque, in <entity id="L08-1212.11">order</entity> to determine its <entity id="L08-1212.12">adequacy</entity> for the <entity id="L08-1212.13">analysis</entity> of emotional <entity id="L08-1212.14">models</entity> and its use in <entity id="L08-1212.15">speech synthesis</entity> . The <entity id="L08-1212.16">corpus</entity> consists of seven hundred semantically neutral <entity id="L08-1212.17">sentences</entity> that were <entity id="L08-1212.18">recorded</entity> for the Big Six <entity id="L08-1212.19">emotions</entity> and neutral style, by two professional actors. The <entity id="L08-1212.20">test</entity> <entity id="L08-1212.21">results</entity> show that every <entity id="L08-1212.22">emotion</entity> is readily recognized far above chance <entity id="L08-1212.23">level</entity> for both speakers. Therefore the <entity id="L08-1212.24">database</entity> is a valid <entity id="L08-1212.25">linguistic resource</entity> for the <entity id="L08-1212.26">research</entity> and <entity id="L08-1212.27">development</entity> <entity id="L08-1212.28">purposes</entity> it was <entity id="L08-1212.29">designed</entity> for.
</abstract>


</text>

<text id="L08-1213">
<title>
How to Compare Treebanks
</title>
<abstract>
Recent years have seen an <entity id="L08-1213.1">increasing</entity> interest in <entity id="L08-1213.2">developing</entity> <entity id="L08-1213.3">standards</entity> for linguistic annotation, with a <entity id="L08-1213.4">focus</entity> on the interoperability of the <entity id="L08-1213.5">resources</entity> . This <entity id="L08-1213.6">effort</entity> , however, <entity id="L08-1213.7">requires</entity> a profound <entity id="L08-1213.8">knowledge</entity> of the <entity id="L08-1213.9">advantages</entity> and disadvantages of linguistic annotation <entity id="L08-1213.10">schemes</entity> in <entity id="L08-1213.11">order</entity> to avoid importing the flaws and weaknesses of existing encoding <entity id="L08-1213.12">schemes</entity> into the new <entity id="L08-1213.13">standards</entity> . This <entity id="L08-1213.14">paper</entity> addresses the <entity id="L08-1213.15">question</entity> how to compare syntactically annotated <entity id="L08-1213.16">corpora</entity> and <entity id="L08-1213.17">gain</entity> <entity id="L08-1213.18">insights</entity> into the <entity id="L08-1213.19">usefulness</entity> of specific <entity id="L08-1213.20">design</entity> <entity id="L08-1213.21">decisions</entity> . We present an exhaustive <entity id="L08-1213.22">evaluation</entity> of two German treebanks with crucially different encoding <entity id="L08-1213.23">schemes</entity> . We <entity id="L08-1213.24">evaluate</entity> three different <entity id="L08-1213.25">parsers</entity> <entity id="L08-1213.26">trained</entity> on the two treebanks and compare <entity id="L08-1213.27">results</entity> using EvalB, the Leaf-Ancestor <entity id="L08-1213.28">metric</entity> , and a <entity id="L08-1213.29">dependency-based</entity> <entity id="L08-1213.30">evaluation</entity> . Furthermore, we present TePaCoC, a new testsuite for the <entity id="L08-1213.31">evaluation</entity> of <entity id="L08-1213.32">parsers</entity> on <entity id="L08-1213.33">complex</entity> German grammatical <entity id="L08-1213.34">constructions</entity> . The testsuite <entity id="L08-1213.35">provides</entity> a well thought-out <entity id="L08-1213.36">error</entity> <entity id="L08-1213.37">classification</entity> , which enables us to compare <entity id="L08-1213.38">parser</entity> <entity id="L08-1213.39">output</entity> for <entity id="L08-1213.40">parsers</entity> <entity id="L08-1213.41">trained</entity> on treebanks with different encoding <entity id="L08-1213.42">schemes</entity> and <entity id="L08-1213.43">provides</entity> interesting <entity id="L08-1213.44">insights</entity> into the <entity id="L08-1213.45">impact</entity> of treebank annotation <entity id="L08-1213.46">schemes</entity> on specific <entity id="L08-1213.47">constructions</entity> like PP <entity id="L08-1213.48">attachment</entity> or <entity id="L08-1213.49">non-constituent</entity> <entity id="L08-1213.50">coordination</entity> .
</abstract>


</text>

<text id="L08-1214">
<title>
The INFILE <entity id="L08-1214.1">Project</entity> : a Crosslingual Filtering <entity id="L08-1214.2">Systems</entity> <entity id="L08-1214.3">Evaluation</entity> Campaign
</title>
<abstract>
The InFile <entity id="L08-1214.4">project</entity> ( <entity id="L08-1214.5">INformation</entity> , FILtering, <entity id="L08-1214.6">Evaluation</entity> ) is a <entity id="L08-1214.7">cross-language</entity> adaptive filtering <entity id="L08-1214.8">evaluation</entity> campaign, sponsored by the French National <entity id="L08-1214.9">Research</entity> Agency. The campaign is organized by the CEA <entity id="L08-1214.10">LIST</entity> , ELDA and the <entity id="L08-1214.11">University</entity> of Lille3-GERiiCO. It has an international <entity id="L08-1214.12">scope</entity> as it is a <entity id="L08-1214.13">pilot</entity> track of the CLEF 2008 campaigns. The <entity id="L08-1214.14">corpus</entity> is built from a <entity id="L08-1214.15">collection</entity> of about 1,4 millions newswires (10 GB) in three <entity id="L08-1214.16">languages</entity> , Arabic, <entity id="L08-1214.17">English</entity> and French <entity id="L08-1214.18">provided</entity> by Agence France Press (AFP) and selected from a 3 years period. The profiles <entity id="L08-1214.19">corpus</entity> is made of 50 profiles from which 30 <entity id="L08-1214.20">concern</entity> general <entity id="L08-1214.21">news</entity> and <entity id="L08-1214.22">events</entity> (national and international affairs, politics, <entity id="L08-1214.23">sports</entity> ...) and 20 <entity id="L08-1214.24">concern</entity> scientific and technical subjects.
</abstract>


</text>

<text id="L08-1215">
<title>
DIAC+: a Professional Diacritics Recovering <entity id="L08-1215.1">System</entity></title>
<abstract>
In <entity id="L08-1215.2">languages</entity> that use diacritical characters, if these special signs are stripped-off from a <entity id="L08-1215.3">word</entity> , the <entity id="L08-1215.4">resulted</entity> <entity id="L08-1215.5">string</entity> of characters may not exist in the <entity id="L08-1215.6">language</entity> , and therefore its normative <entity id="L08-1215.7">form</entity> is, in general, easy to recover. However, this is not always the <entity id="L08-1215.8">case</entity> , as <entity id="L08-1215.9">presence</entity> or <entity id="L08-1215.10">absence</entity> of a diacritical sign attached to a <entity id="L08-1215.11">base</entity> letter of a <entity id="L08-1215.12">word</entity> which exists in both <entity id="L08-1215.13">variants</entity> , may change its grammatical <entity id="L08-1215.14">properties</entity> or even the meaning, making the <entity id="L08-1215.15">recovery</entity> of the missing diacritics a difficult <entity id="L08-1215.16">task</entity> , not only for a <entity id="L08-1215.17">program</entity> but sometimes even for a human reader. We describe and <entity id="L08-1215.18">evaluate</entity> an accurate <entity id="L08-1215.19">knowledge-based system</entity> for <entity id="L08-1215.20">automatic</entity> recovering the missing diacritics in MS-Office <entity id="L08-1215.21">documents</entity> written in Romanian. For the rare <entity id="L08-1215.22">cases</entity> when the <entity id="L08-1215.23">system</entity> is not able to reliably make a <entity id="L08-1215.24">decision</entity>,it either <entity id="L08-1215.25">provides</entity> the <entity id="L08-1215.26">user</entity> a <entity id="L08-1215.27">list</entity> of <entity id="L08-1215.28">words</entity> with their <entity id="L08-1215.29">recovery</entity> <entity id="L08-1215.30">suggestions</entity> , or probabilistically choose one of the possible changes, but leaves a trace (a highlighted comment) on each <entity id="L08-1215.31">word</entity> the <entity id="L08-1215.32">modification</entity> of which was uncertain.
</abstract>


</text>

<text id="L08-1216">
<title>
Annotating an Arabic Learner <entity id="L08-1216.1">Corpus</entity> for <entity id="L08-1216.2">Error</entity></title>
<abstract>
This <entity id="L08-1216.3">paper</entity> describes an ongoing <entity id="L08-1216.4">project</entity> in which we are collecting a learner <entity id="L08-1216.5">corpus</entity> of Arabic, <entity id="L08-1216.6">developing</entity> a tagset for <entity id="L08-1216.7">error</entity> annotation and <entity id="L08-1216.8">performing</entity> <entity id="L08-1216.9">Computer-aided</entity> <entity id="L08-1216.10">Error Analysis</entity> (CEA) on the <entity id="L08-1216.11">data</entity> . We <entity id="L08-1216.12">adapted</entity> the French Interlanguage <entity id="L08-1216.13">Database</entity> FRIDA tagset ( Granger, 2003a ) to the <entity id="L08-1216.14">data</entity> . We chose FRIDA in <entity id="L08-1216.15">order</entity> to follow a known <entity id="L08-1216.16">standard</entity> and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the <entity id="L08-1216.17">distance</entity> between the two <entity id="L08-1216.18">languages</entity> with <entity id="L08-1216.19">respect</entity> to learner <entity id="L08-1216.20">difficulty</entity> . The <entity id="L08-1216.21">current</entity> <entity id="L08-1216.22">collection</entity> of <entity id="L08-1216.23">texts</entity> , which is constantly growing, contains intermediate and <entity id="L08-1216.24">advanced-level</entity> student writings . We describe the need for such <entity id="L08-1216.25">corpora</entity> , the learner <entity id="L08-1216.26">data</entity> we have collected and the tagset we have <entity id="L08-1216.27">developed</entity> . We also describe the <entity id="L08-1216.28">error</entity> <entity id="L08-1216.29">frequency</entity> <entity id="L08-1216.30">distribution</entity> of both <entity id="L08-1216.31">proficiency</entity> <entity id="L08-1216.32">levels</entity> and the ongoing work.
</abstract>


</text>

<text id="L08-1217">
<title>
All, and only, the Errors: more Complete and Consistent Spelling and <entity id="L08-1217.1">OCR-Error</entity> <entity id="L08-1217.2">Correction</entity> <entity id="L08-1217.3">Evaluation</entity></title>
<abstract>
Some <entity id="L08-1217.4">time</entity> in the future, some <entity id="L08-1217.5">spelling</entity> <entity id="L08-1217.6">error</entity> <entity id="L08-1217.7">correction</entity> <entity id="L08-1217.8">system</entity> will correct all the <entity id="L08-1217.9">errors</entity> , and only the <entity id="L08-1217.10">errors</entity> . We need <entity id="L08-1217.11">evaluation metrics</entity> that will tell us when this has been achieved and that can <entity id="L08-1217.12">help</entity> guide us there. We <entity id="L08-1217.13">survey</entity> the <entity id="L08-1217.14">current</entity> <entity id="L08-1217.15">practice</entity> in the <entity id="L08-1217.16">form</entity> of the <entity id="L08-1217.17">evaluation</entity> <entity id="L08-1217.18">scheme</entity> of the latest major publication on <entity id="L08-1217.19">spelling</entity> <entity id="L08-1217.20">correction</entity> in a leading <entity id="L08-1217.21">journal</entity> . We are forced to conclude that while the <entity id="L08-1217.22">metric</entity> used there can tell us exactly when the ultimate <entity id="L08-1217.23">goal</entity> of <entity id="L08-1217.24">spelling</entity> <entity id="L08-1217.25">correction</entity> <entity id="L08-1217.26">research</entity> has been achieved, it offers little in the way of <entity id="L08-1217.27">directions</entity> to be followed to eventually get there. We <entity id="L08-1217.28">propose</entity> to consistently use the well-known <entity id="L08-1217.29">metrics</entity> <entity id="L08-1217.30">Recall</entity> and <entity id="L08-1217.31">Precision</entity> , as combined in the F score, on 5 possible <entity id="L08-1217.32">levels</entity> of measurement that should guide us more informedly along that <entity id="L08-1217.33">path</entity> . We describe briefly what is then measured or measurable at these <entity id="L08-1217.34">levels</entity> and <entity id="L08-1217.35">propose</entity> a <entity id="L08-1217.36">framework</entity> that should allow for concisely stating what it is one <entity id="L08-1217.37">performs</entity> in one's <entity id="L08-1217.38">evaluations</entity> . We finally <entity id="L08-1217.39">contrast</entity> our preferred <entity id="L08-1217.40">metrics</entity> to <entity id="L08-1217.41">Accuracy</entity> , which is widely used in this <entity id="L08-1217.42">field</entity> to this day and to the <entity id="L08-1217.43">Area-</entity> Under-the-Curve, which is increasingly finding acceptance in other <entity id="L08-1217.44">fields</entity> .
</abstract>


</text>

<text id="L08-1218">
<title>
Using Movie Subtitles for Creating a Large- <entity id="L08-1218.1">Scale</entity> Bilingual <entity id="L08-1218.2">Corpora</entity></title>
<abstract>
This <entity id="L08-1218.3">paper</entity> presents a <entity id="L08-1218.4">method</entity> for compiling a <entity id="L08-1218.5">large-scale</entity> bilingual <entity id="L08-1218.6">corpus</entity> from a <entity id="L08-1218.7">database</entity> of movie subtitles . To create the <entity id="L08-1218.8">corpus</entity> , we <entity id="L08-1218.9">propose</entity> an <entity id="L08-1218.10">algorithm</entity> <entity id="L08-1218.11">based</entity> on <entity id="L08-1218.12">Gale</entity> and Church's <entity id="L08-1218.13">sentence</entity> <entity id="L08-1218.14">alignment</entity> <entity id="L08-1218.15">algorithm</entity> (1993). However, our <entity id="L08-1218.16">algorithm</entity> not only relies on character <entity id="L08-1218.17">length</entity> <entity id="L08-1218.18">information</entity> , but also uses <entity id="L08-1218.19">subtitle-timing</entity> <entity id="L08-1218.20">information</entity> , which is encoded in the subtitle files. <entity id="L08-1218.21">Timing</entity> is highly correlated between subtitles in different <entity id="L08-1218.22">versions</entity> (for the same movie), since subtitles that <entity id="L08-1218.23">match</entity> should be <entity id="L08-1218.24">displayed</entity> at the same <entity id="L08-1218.25">time</entity> . However, the absolute <entity id="L08-1218.26">time</entity> values can't be used for <entity id="L08-1218.27">alignment</entity> , since the <entity id="L08-1218.28">timing</entity> is usually specified by <entity id="L08-1218.29">frame</entity> <entity id="L08-1218.30">numbers</entity> and not by <entity id="L08-1218.31">real time</entity> , and converting it to <entity id="L08-1218.32">real time</entity> values is not always possible, hence we  use normalized subtitle <entity id="L08-1218.33">duration</entity> instead . This <entity id="L08-1218.34">results</entity> in a significant <entity id="L08-1218.35">reduction</entity> in the <entity id="L08-1218.36">alignment</entity> <entity id="L08-1218.37">error rate</entity>.
</abstract>


</text>

<text id="L08-1219">
<title>
The IFADV <entity id="L08-1219.1">Corpus</entity> : a Free <entity id="L08-1219.2">Dialog</entity> <entity id="L08-1219.3">Video</entity> <entity id="L08-1219.4">Corpus</entity></title>
<abstract>
"<entity id="L08-1219.5">Research</entity> into spoken <entity id="L08-1219.6">language</entity> has become more visual over the years. Both fundamental and <entity id="L08-1219.7">applied</entity> <entity id="L08-1219.8">research</entity> have progressively <entity id="L08-1219.9">included</entity> <entity id="L08-1219.10">gestures</entity> , gaze, and facial <entity id="L08-1219.11">expression</entity> . <entity id="L08-1219.12">Corpora</entity> of multi-modal conversational <entity id="L08-1219.13">speech</entity> are rare and frequently difficult to use <entity id="L08-1219.14">due</entity> to privacy and copyright <entity id="L08-1219.15">restrictions</entity> . A freely available annotated <entity id="L08-1219.16">corpus</entity> is presented, gratis and libre, of <entity id="L08-1219.17">high quality</entity> <entity id="L08-1219.18">video</entity> <entity id="L08-1219.19">recordings</entity> of face-to-face conversational <entity id="L08-1219.20">speech</entity> . Within the <entity id="L08-1219.21">bounds</entity> of the law, everything has been done to remove copyright and use <entity id="L08-1219.22">restrictions</entity> . Annotations have been <entity id="L08-1219.23">processed</entity> to RDBMS <entity id="L08-1219.24">tables</entity> that allow SQL <entity id="L08-1219.25">queries</entity> and direct connections to <entity id="L08-1219.26">statistical</entity> <entity id="L08-1219.27">software</entity> . From our <entity id="L08-1219.28">experiences</entity> we would like to advocate the <entity id="L08-1219.29">formulation</entity> of ""best practises"" for both legal handling and <entity id="L08-1219.30">database</entity> <entity id="L08-1219.31">storage</entity> of <entity id="L08-1219.32">recordings</entity> and annotations. "
</abstract>


</text>

<text id="L08-1220">
<title>
WOZ Acoustic <entity id="L08-1220.1">Data</entity> <entity id="L08-1220.2">Collection</entity> for Interactive TV
</title>
<abstract>This <entity id="L08-1220.3">paper</entity> describes a multichannel acoustic <entity id="L08-1220.4">data</entity> <entity id="L08-1220.5">collection</entity> <entity id="L08-1220.6">recorded</entity> under the European DICIT <entity id="L08-1220.7">project</entity> , during the Wizard of Oz (WOZ) <entity id="L08-1220.8">experiments</entity> carried out at FAU and FBK-irst <entity id="L08-1220.9">laboratories</entity> . The <entity id="L08-1220.10">scenario</entity> is a distant-talking <entity id="L08-1220.11">interface</entity> for interactive <entity id="L08-1220.12">control</entity> of a TV. The <entity id="L08-1220.13">experiments</entity> involve the <entity id="L08-1220.14">acquisition</entity> of multichannel <entity id="L08-1220.15">data</entity> for <entity id="L08-1220.16">signal <entity id="L08-1220.17">processing</entity></entity> front-end and were carried out <entity id="L08-1220.18">due</entity> to the need to collect a <entity id="L08-1220.19">database</entity> for testing acoustic <entity id="L08-1220.20">pre-processing</entity> <entity id="L08-1220.21">algorithms</entity> . In this way, realistic <entity id="L08-1220.22">scenarios</entity> can be simulated at a preliminary stage, instead of <entity id="L08-1220.23">real-time</entity> <entity id="L08-1220.24">implementations</entity> , allowing for repeatable <entity id="L08-1220.25">experiments</entity> . To <entity id="L08-1220.26">match</entity> the <entity id="L08-1220.27">project</entity> <entity id="L08-1220.28">requirements</entity> , the WOZ <entity id="L08-1220.29">experiments</entity> were <entity id="L08-1220.30">recorded</entity> in three <entity id="L08-1220.31">languages</entity> : <entity id="L08-1220.32">English</entity> , German and Italian. Besides the <entity id="L08-1220.33">user</entity> <entity id="L08-1220.34">inputs</entity> , the <entity id="L08-1220.35">database</entity> also contains <entity id="L08-1220.36">non-speech</entity> related acoustic <entity id="L08-1220.37">events</entity> , room impulse response measurements and <entity id="L08-1220.38">video</entity> data, the latter used to <entity id="L08-1220.39">compute</entity> 3D labels . Sessions were manually transcribed and segmented at <entity id="L08-1220.40">word</entity> <entity id="L08-1220.41">level</entity> , introducing also specific labels for acoustic <entity id="L08-1220.42">events</entity> .
</abstract>


</text>

<text id="L08-1221">
<title><entity id="L08-1221.1">Process</entity> <entity id="L08-1221.2">Model</entity> for Composing <entity id="L08-1221.3">High-quality</entity> <entity id="L08-1221.4">Text</entity> <entity id="L08-1221.5">Corpora</entity></title>
<abstract>The Teko <entity id="L08-1221.6">corpus</entity> composing <entity id="L08-1221.7">model</entity> offers a decentralized, dynamic way of collecting <entity id="L08-1221.8">high-quality</entity> <entity id="L08-1221.9">text</entity> <entity id="L08-1221.10">corpora</entity> for linguistic <entity id="L08-1221.11">research</entity> . The  <entity id="L08-1221.12">resulting</entity> <entity id="L08-1221.13">corpus</entity> consists of independent <entity id="L08-1221.14">text</entity> sets. The sets are composed in <entity id="L08-1221.15">cooperation</entity> with linguistic <entity id="L08-1221.16">research projects</entity> , so each of them responds to a specific <entity id="L08-1221.17">research</entity> need. The <entity id="L08-1221.18">corpora</entity> are morphologically annotated and XML-based, with in-built compatibilty with the Kaino <entity id="L08-1221.19">user interface</entity> used in the <entity id="L08-1221.20">corpus</entity> <entity id="L08-1221.21">server</entity> of the <entity id="L08-1221.22">Research</entity> Institute for the <entity id="L08-1221.23">Languages</entity> of Finland. Furthermore, <entity id="L08-1221.24">software</entity> for <entity id="L08-1221.25">extracting</entity> <entity id="L08-1221.26">standard</entity> quantitative <entity id="L08-1221.27">reports</entity> from the <entity id="L08-1221.28">text</entity> sets has been created during the <entity id="L08-1221.29">project</entity> . The <entity id="L08-1221.30">paper</entity> describes the <entity id="L08-1221.31">project</entity> , and estimates its <entity id="L08-1221.32">benefits</entity> and <entity id="L08-1221.33">problems</entity> . It also gives an <entity id="L08-1221.34">overview</entity> of the technical <entity id="L08-1221.35">qualities</entity> of the <entity id="L08-1221.36">corpora</entity> and <entity id="L08-1221.37">corpus</entity> <entity id="L08-1221.38">interface</entity> connected to the Teko <entity id="L08-1221.39">project</entity> .
</abstract>


</text>

<text id="L08-1222">
<title>
AnCora: Multilevel Annotated <entity id="L08-1222.1">Corpora</entity> for Catalan and Spanish
</title>
<abstract>
This <entity id="L08-1222.2">paper</entity> presents AnCora, a multilingual <entity id="L08-1222.3">corpus</entity> annotated at different linguistic <entity id="L08-1222.4">levels</entity> consisting of 500,000 <entity id="L08-1222.5">words</entity> in Catalan (AnCora-Ca) and in Spanish (AnCora-Es). At present AnCora is the largest multilayer annotated <entity id="L08-1222.6">corpus</entity> of these <entity id="L08-1222.7">languages</entity> freely available from http://clic. ub. edu/ancora. The two <entity id="L08-1222.8">corpora</entity> consist mainly of <entity id="L08-1222.9">newspaper</entity> <entity id="L08-1222.10">texts</entity> annotated at different <entity id="L08-1222.11">levels</entity> of <entity id="L08-1222.12">linguistic description</entity> : morphological (PoS and <entity id="L08-1222.13">lemmas</entity> ), <entity id="L08-1222.14">syntactic</entity> ( <entity id="L08-1222.15">constituents</entity> and <entity id="L08-1222.16">functions</entity> ), and <entity id="L08-1222.17">semantic</entity> ( <entity id="L08-1222.18">argument structures</entity> , thematic <entity id="L08-1222.19">roles</entity> , <entity id="L08-1222.20">semantic</entity> <entity id="L08-1222.21">verb classes</entity> , <entity id="L08-1222.22">named</entity> <entity id="L08-1222.23">entities</entity> , and WordNet nominal senses). All <entity id="L08-1222.24">resulting</entity> <entity id="L08-1222.25">layers</entity> are independent of each other, thus making easier the <entity id="L08-1222.26">data</entity> <entity id="L08-1222.27">management</entity> . The annotation was <entity id="L08-1222.28">performed</entity> manually, semiautomatically, or fully automatically, depending on the encoded <entity id="L08-1222.29">linguistic information</entity> . The <entity id="L08-1222.30">development</entity> of these <entity id="L08-1222.31">basic</entity> <entity id="L08-1222.32">resources</entity> constituted a primary <entity id="L08-1222.33">objective</entity> , since there was a <entity id="L08-1222.34">lack</entity> of such <entity id="L08-1222.35">resources</entity> for these <entity id="L08-1222.36">languages</entity> . A second <entity id="L08-1222.37">goal</entity> was the <entity id="L08-1222.38">definition</entity> of a consistent <entity id="L08-1222.39">methodology</entity> that can be followed in further annotations. The <entity id="L08-1222.40">current</entity> <entity id="L08-1222.41">versions</entity> of AnCora have been used in several international <entity id="L08-1222.42">evaluation</entity> <entity id="L08-1222.43">competitions</entity></abstract>


</text>

<text id="W08-0213">
<title>
Studying <entity id="W08-0213.1">Discourse</entity> and <entity id="W08-0213.2">Dialogue</entity> with SIDGrid
</title>
<abstract>
Teaching <entity id="W08-0213.3">Computational Linguistics</entity> is inherently multi-disciplinary and frequently poses <entity id="W08-0213.4">challenges</entity> and <entity id="W08-0213.5">provides</entity> opportunities in teaching to a student body with diverse educational <entity id="W08-0213.6">backgrounds</entity> and <entity id="W08-0213.7">goals</entity> . This <entity id="W08-0213.8">paper</entity> describes the use of a <entity id="W08-0213.9">computational</entity> <entity id="W08-0213.10">environment</entity> (SIDGrid) that facilitates interdisciplinary <entity id="W08-0213.11">instruction</entity> by <entity id="W08-0213.12">providing</entity> <entity id="W08-0213.13">support</entity> for students with little <entity id="W08-0213.14">computational</entity> <entity id="W08-0213.15">background</entity> as well as extending the <entity id="W08-0213.16">scale</entity> of <entity id="W08-0213.17">projects</entity> accessible to students with more <entity id="W08-0213.18">advanced</entity> <entity id="W08-0213.19">computational</entity> skills. The <entity id="W08-0213.20">environment</entity> facilitates the use of hands-on exercises and is being <entity id="W08-0213.21">applied</entity> to interdisciplinary <entity id="W08-0213.22">instruction</entity> in <entity id="W08-0213.23">Discourse</entity> and <entity id="W08-0213.24">Dialogue</entity> .
</abstract>


</text>

<text id="W08-2128"><title>
A Combined <entity id="W08-2128.1">Memory-</entity><entity id="W08-2128.2">Based</entity> <entity id="W08-2128.3">Semantic Role</entity> Labeler of <entity id="W08-2128.4">English</entity></title><abstract>
We describe the <entity id="W08-2128.5">system</entity> submitted to the closed <entity id="W08-2128.6">challenge</entity> of the CoNLL-2008 shared <entity id="W08-2128.7">task</entity> on joint <entity id="W08-2128.8">parsing</entity> of <entity id="W08-2128.9">syntactic</entity> and <entity id="W08-2128.10">semantic</entity> <entity id="W08-2128.11">dependencies</entity>. <entity id="W08-2128.12">Syntactic</entity> <entity id="W08-2128.13">dependencies</entity> are <entity id="W08-2128.14">processed</entity> with the Malt-<entity id="W08-2128.15">Parser</entity> 0.4. <entity id="W08-2128.16">Semantic</entity> <entity id="W08-2128.17">dependencies</entity> are <entity id="W08-2128.18">processed</entity> with a <entity id="W08-2128.19">combination</entity> of <entity id="W08-2128.20">memory-based</entity> <entity id="W08-2128.21">classifiers</entity>. The <entity id="W08-2128.22">system</entity> achieves 78.43 labeled macro Fl for the complete <entity id="W08-2128.23">problem</entity>, 86.07 labeled <entity id="W08-2128.24">attachment</entity> score for <entity id="W08-2128.25">syntactic</entity> <entity id="W08-2128.26">dependencies</entity>, and 70.51 labeled Fl for <entity id="W08-2128.27">semantic</entity> <entity id="W08-2128.28">dependencies</entity>.
</abstract>


</text>

<text id="N07-1047"><title><entity id="N07-1047.1">Applying</entity> Many-to-Many Alignments and Hidden Markov <entity id="N07-1047.2">Models</entity> to Letter-to-<entity id="N07-1047.3">Phoneme</entity> <entity id="N07-1047.4">Conversion</entity></title><abstract><entity id="N07-1047.5">Letter-to-phoneme</entity> <entity id="N07-1047.6">conversion</entity> generally <entity id="N07-1047.7">requires</entity> aligned <entity id="N07-1047.8">training</entity><entity id="N07-1047.9">data</entity> of letters and <entity id="N07-1047.10">phonemes</entity>. Typically, the <entity id="N07-1047.11">alignments</entity> are <entity id="N07-1047.12">limited</entity> to one-to-one <entity id="N07-1047.13">alignments</entity>. We present a novel <entity id="N07-1047.14">technique</entity> of <entity id="N07-1047.15">training</entity> with many-to-many <entity id="N07-1047.16">alignments</entity>. A letter <entity id="N07-1047.17">chunking</entity> bigram <entity id="N07-1047.18">prediction</entity> manages double letters and double <entity id="N07-1047.19">phonemes</entity> automatically as opposed to preprocessing with fixed <entity id="N07-1047.20">lists</entity>. We also <entity id="N07-1047.21">apply</entity> an HMM <entity id="N07-1047.22">method</entity> in <entity id="N07-1047.23">conjunction</entity> with a local <entity id="N07-1047.24">classification model</entity> to predict a global <entity id="N07-1047.25">phoneme</entity> <entity id="N07-1047.26">sequence</entity> given a <entity id="N07-1047.27">word</entity>. The many-to-many <entity id="N07-1047.28">alignments</entity> <entity id="N07-1047.29">result</entity> in significant <entity id="N07-1047.30">improvements</entity> over the traditional one-to-one <entity id="N07-1047.31">approach</entity>. Our <entity id="N07-1047.32">system</entity> achieves state-of-the-art <entity id="N07-1047.33">performance</entity> on several <entity id="N07-1047.34">languages</entity> and<entity id="N07-1047.35">data</entity> sets.
</abstract>


</text>

<text id="N07-2003"><title>
Conquest---An Open-<entity id="N07-2003.1">Source</entity> <entity id="N07-2003.2">Dialog System</entity> for Conferences
</title><abstract>
We describe ConQuest, an <entity id="N07-2003.3">open-source</entity>, reusable spoken <entity id="N07-2003.4">dialog system</entity> that <entity id="N07-2003.5">provides</entity> technical <entity id="N07-2003.6">program</entity> <entity id="N07-2003.7">information</entity> during conferences. The <entity id="N07-2003.8">system</entity> uses a transparent, modular and open <entity id="N07-2003.9">infrastructure</entity>, and aims to enable <entity id="N07-2003.10">applied</entity> <entity id="N07-2003.11">research</entity> in spoken <entity id="N07-2003.12">language</entity> <entity id="N07-2003.13">interfaces</entity>. The conference <entity id="N07-2003.14">domain</entity> is a good <entity id="N07-2003.15">platform</entity> for <entity id="N07-2003.16">applied</entity> <entity id="N07-2003.17">research</entity> since it permits periodical redeployments and <entity id="N07-2003.18">evaluations</entity> with a real <entity id="N07-2003.19">user-base</entity>. In this <entity id="N07-2003.20">paper</entity>, we describe the <entity id="N07-2003.21">system</entity>'s <entity id="N07-2003.22">functionality</entity>, overall <entity id="N07-2003.23">architecture</entity>, and we discuss two initial <entity id="N07-2003.24">deployments</entity>.
</abstract>


</text>

<text id="P04-1006"><title>
Attention Shifting For <entity id="P04-1006.1">Parsing</entity> <entity id="P04-1006.2">Speech</entity></title><abstract>
We present a <entity id="P04-1006.3">technique</entity> that <entity id="P04-1006.4">improves</entity> the <entity id="P04-1006.5">efficiency</entity> of <entity id="P04-1006.6">word-lattice</entity> <entity id="P04-1006.7">parsing</entity> as used in <entity id="P04-1006.8">speech recognition</entity> <entity id="P04-1006.9">language modeling</entity>. Our <entity id="P04-1006.10">technique</entity> <entity id="P04-1006.11">applies</entity> a probabilistic <entity id="P04-1006.12">parser</entity> iteratively where on each <entity id="P04-1006.13">iteration</entity> it <entity id="P04-1006.14">focuses</entity> on a different subset of the <entity id="P04-1006.15">word-lattice</entity>. The <entity id="P04-1006.16">parser</entity>'s attention is <entity id="P04-1006.17">shifted</entity> towards <entity id="P04-1006.18">word-lattice</entity> subsets for which there are few or no <entity id="P04-1006.19">syntactic analyses</entity> posited. This attention-shifting <entity id="P04-1006.20">technique</entity> <entity id="P04-1006.21">provides</entity> a six-times <entity id="P04-1006.22">increase</entity> in <entity id="P04-1006.23">speed</entity> (measured as the <entity id="P04-1006.24">number</entity> of <entity id="P04-1006.25">parser</entity> <entity id="P04-1006.26">analyses</entity> <entity id="P04-1006.27">evaluated</entity>) while <entity id="P04-1006.28">performing</entity> equivalently when used as the first-stage of a multi-stage <entity id="P04-1006.29">parsing-based</entity> <entity id="P04-1006.30">language model</entity>.
</abstract>


</text>

<text id="P06-3010"><title>
A Hybrid <entity id="P06-3010.1">Relational</entity> <entity id="P06-3010.2">Approach</entity> For WSD - First <entity id="P06-3010.3">Results</entity></title><abstract>
We present a novel hybrid <entity id="P06-3010.4">approach</entity> for <entity id="P06-3010.5">Word Sense Disambiguation</entity> (WSD) which makes use of a <entity id="P06-3010.6">relational</entity> <entity id="P06-3010.7">formalism</entity> to represent <entity id="P06-3010.8">instances</entity> and <entity id="P06-3010.9">background knowledge</entity>. It is built using Inductive <entity id="P06-3010.10">Logic Programming</entity> <entity id="P06-3010.11">techniques</entity> to combine <entity id="P06-3010.12">evidence</entity> coming from both <entity id="P06-3010.13">sources</entity> during the learning <entity id="P06-3010.14">process</entity>, producing a <entity id="P06-3010.15">rule-based</entity> WSD <entity id="P06-3010.16">model</entity>. We <entity id="P06-3010.17">experimented</entity> with this <entity id="P06-3010.18">approach</entity> to <entity id="P06-3010.19">disambiguate</entity> 7 highly ambiguous <entity id="P06-3010.20">verbs</entity> in <entity id="P06-3010.21">English-</entity>Portuguese <entity id="P06-3010.22">translation</entity>. <entity id="P06-3010.23">Results</entity> showed that the <entity id="P06-3010.24">approach</entity> is promising, achieving an average <entity id="P06-3010.25">accuracy</entity> of 75%, which outperforms the other <entity id="P06-3010.26">machine</entity> learning <entity id="P06-3010.27">techniques</entity> investigated (66%).
</abstract>


</text>

<text id="C82-1028"><title><entity id="C82-1028.1">Machine Translation</entity> <entity id="C82-1028.2">Based</entity> On Logically Isomorphic Montague Grammars
</title><abstract>
"Usually two <entity id="C82-1028.3">approaches</entity> to <entity id="C82-1028.4">machine translation</entity> are distinguished: the interlingual <entity id="C82-1028.5">approach</entity> and the <entity id="C82-1028.6">transfer</entity> <entity id="C82-1028.7">approach</entity> ( cf. Hutchins [1]). In the interlingual <entity id="C82-1028.8">approach</entity> <entity id="C82-1028.9">translation</entity> is a two-stage <entity id="C82-1028.10">process</entity>: from <entity id="C82-1028.11">source language</entity> to interlingua and from interlingua to <entity id="C82-1028.12">target language</entity>. In the <entity id="C82-1028.13">transfer</entity> <entity id="C82-1028.14">approach</entity> there are three stages: <entity id="C82-1028.15">source language</entity> <entity id="C82-1028.16">analysis</entity>, <entity id="C82-1028.17">transfer</entity> and <entity id="C82-1028.18">target language</entity> <entity id="C82-1028.19">generation</entity>. The <entity id="C82-1028.20">approach</entity> <entity id="C82-1028.21">advanced</entity> in this <entity id="C82-1028.22">paper</entity> is a <entity id="C82-1028.23">variant</entity> of the interlingual one. It <entity id="C82-1028.24">requires</entity> that 'logically isomorphic grammars' are written for the <entity id="C82-1028.25">languages</entity>,under <entity id="C82-1028.26">consideration</entity>. The <entity id="C82-1028.27">syntactic</entity> <entity id="C82-1028.28">rules</entity> of these grammars must correspond with logical <entity id="C82-1028.29">operations</entity>, in accordance with the compositionality <entity id="C82-1028.30">principle</entity> of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a <entity id="C82-1028.31">rule</entity> corresponding with a particular logical <entity id="C82-1028.32">operation</entity>, the other grammars must contain <entity id="C82-1028.33">rules</entity> corresponding with the same <entity id="C82-1028.34">operation</entity>. Syntactically, these <entity id="C82-1028.35">rules</entity> may differ considerably. If the grammars are attuned to each other in this way,  'logical <entity id="C82-1028.36">derivation</entity> <entity id="C82-1028.37">trees</entity>', <entity id="C82-1028.38">representations</entity> of both the syntactical and the logical <entity id="C82-1028.39">structure</entity> of <entity id="C82-1028.40">sentences</entity>, can be used as intermediate <entity id="C82-1028.41">expressions</entity>. The <entity id="C82-1028.42">paper</entity> is organized as follows. In <entity id="C82-1028.43">section</entity> 2 the relevant <entity id="C82-1028.44">concepts</entity> of Montague grammar and the <entity id="C82-1028.45">notion</entity> 'logically isomorphic grammars' are introduced. In <entity id="C82-1028.46">section</entity> 3 a <entity id="C82-1028.47">version</entity> of Montague grammar is described, <entity id="C82-1028.48">called</entity> M-grammar, which is more suitable for <entity id="C82-1028.49">computational</entity> use than Montague 's original <entity id="C82-1028.50">proposals</entity>. The <entity id="C82-1028.51">property</entity> of logical isomorphy is then defined for M-grammars. In <entity id="C82-1028.52">section</entity> 4 the <entity id="C82-1028.53">design</entity> of the Rosetta <entity id="C82-1028.54">translation system</entity>, <entity id="C82-1028.55">based</entity> on this <entity id="C82-1028.56">approach</entity>, is <entity id="C82-1028.57">outlined</entity>, followed by a brief <entity id="C82-1028.58">discussion</entity> in <entity id="C82-1028.59">section</entity> 5. "
</abstract>


</text>

<text id="C86-1082"><title>
A Compositional <entity id="C86-1082.1">Semantics</entity> For Directional Modifiers - Locative <entity id="C86-1082.2">Case</entity> Reopened
</title><abstract>
This <entity id="C86-1082.3">paper</entity> presents a <entity id="C86-1082.4">model-theoretic</entity> <entity id="C86-1082.5">semantics</entity> for directional <entity id="C86-1082.6">modifiers</entity> in <entity id="C86-1082.7">English</entity>. The <entity id="C86-1082.8">semantic</entity> <entity id="C86-1082.9">theory</entity> presupposed for the <entity id="C86-1082.10">analysis</entity> is that of Montague Grammar (cf. Montague 1970 , 1973) which makes it possible to <entity id="C86-1082.11">develop</entity> a strongly compositional <entity id="C86-1082.12">treatment</entity> of directional <entity id="C86-1082.13">modifiers</entity>. Such a <entity id="C86-1082.14">treatment</entity> has significant <entity id="C86-1082.15">computational</entity> <entity id="C86-1082.16">advantages</entity> over <entity id="C86-1082.17">case-based</entity> <entity id="C86-1082.18">treatments</entity> of directional <entity id="C86-1082.19">modifiers</entity> that are advocated in the Al <entity id="C86-1082.20">literature</entity>.
</abstract>


</text>

<text id="C88-1031"><title>
Stylistic Grammars In <entity id="C88-1031.1">Language</entity> <entity id="C88-1031.2">Translation</entity></title><abstract>
We are <entity id="C88-1031.3">developing</entity> stylistic grammars to <entity id="C88-1031.4">provide</entity> the <entity id="C88-1031.5">basis</entity> for a French and <entity id="C88-1031.6">English</entity> stylistic <entity id="C88-1031.7">parser</entity>. Our stylistic grammar is a branching stratificational <entity id="C88-1031.8">model</entity>, built upon a foundation <entity id="C88-1031.9">dealing</entity> with <entity id="C88-1031.10">lexical</entity>, <entity id="C88-1031.11">syntactic</entity>, and <entity id="C88-1031.12">semantic</entity> stylistic <entity id="C88-1031.13">realizations</entity>. Its central <entity id="C88-1031.14">level</entity> uses a <entity id="C88-1031.15">vocabulary</entity> of <entity id="C88-1031.16">constituent</entity> stylistic elements <entity id="C88-1031.17">common</entity> to both <entity id="C88-1031.18">English</entity> and French, while the top <entity id="C88-1031.19">level</entity> correlates stylistic <entity id="C88-1031.20">goals</entity>, such as clarity and concreteness, with <entity id="C88-1031.21">patterns</entity> of these elements. Overall, we are <entity id="C88-1031.22">implementing</entity> a <entity id="C88-1031.23">computational</entity> <entity id="C88-1031.24">schema</entity> of stylistics in French-to-<entity id="C88-1031.25">English</entity> <entity id="C88-1031.26">translation</entity>. We believe that the incorporation of stylistic <entity id="C88-1031.27">analysis</entity> into <entity id="C88-1031.28">machine translation systems</entity> will significantly reduce the <entity id="C88-1031.29">current</entity> reliance on human post-editing and <entity id="C88-1031.30">improve</entity> the <entity id="C88-1031.31">quality</entity> of the <entity id="C88-1031.32">systems'</entity> <entity id="C88-1031.33">output</entity>.
</abstract>


</text>

<text id="C94-1036"><title>
Segmenting A <entity id="C94-1036.1">Sentence</entity> Into Morphemes Using Statistic <entity id="C94-1036.2">Information</entity> Between <entity id="C94-1036.3">Words</entity></title><abstract>
This <entity id="C94-1036.4">paper</entity> is on dividing non-separated <entity id="C94-1036.5">language</entity> <entity id="C94-1036.6">sentences</entity> (whose <entity id="C94-1036.7">words</entity> are not separated from each other with a <entity id="C94-1036.8">space</entity> or other separater
) into morphemes using <entity id="C94-1036.9">statistical</entity> <entity id="C94-1036.10">information</entity>, not grammatical <entity id="C94-1036.11">information</entity> which is often used in NLP. In this <entity id="C94-1036.12">paper</entity> we describe our <entity id="C94-1036.13">method</entity> and <entity id="C94-1036.14">experimental</entity> <entity id="C94-1036.15">result</entity> on <entity id="C94-1036.16">Japanese</entity> and <entity id="C94-1036.17">Chinese</entity> <entity id="C94-1036.18">sentences</entity>. As will be seen in the body of this <entity id="C94-1036.19">paper</entity>, the <entity id="C94-1036.20">result</entity> shows that this <entity id="C94-1036.21">system</entity> is efficient for most of the <entity id="C94-1036.22">sentences</entity>.
</abstract>


</text>

<text id="C94-2106"><title>
A <entity id="C94-2106.1">System</entity> Of <entity id="C94-2106.2">Verbal</entity> <entity id="C94-2106.3">Semantic</entity> Attributes Focused On The <entity id="C94-2106.4">Syntactic</entity> <entity id="C94-2106.5">Correspondence</entity> Between <entity id="C94-2106.6">Japanese</entity> And <entity id="C94-2106.7">English</entity></title><abstract>
This <entity id="C94-2106.8">paper</entity> <entity id="C94-2106.9">proposes</entity> a <entity id="C94-2106.10">system</entity> of 97 <entity id="C94-2106.11">verbal</entity> <entity id="C94-2106.12">semantic</entity> attributes for <entity id="C94-2106.13">Japanese</entity> <entity id="C94-2106.14">verbs</entity> which considers both dynamic <entity id="C94-2106.15">characteristics</entity> and the <entity id="C94-2106.16">relationship</entity> of <entity id="C94-2106.17">verbs</entity> to <entity id="C94-2106.18">cases</entity>. These attribute values are used to <entity id="C94-2106.19">disambiguate</entity> the meanings of all <entity id="C94-2106.20">Japanese</entity> and <entity id="C94-2106.21">English</entity> <entity id="C94-2106.22">pattern</entity> <entity id="C94-2106.23">pairs</entity> in a <entity id="C94-2106.24">Japanese</entity> to <entity id="C94-2106.25">English</entity> <entity id="C94-2106.26">transfer</entity> <entity id="C94-2106.27">pattern</entity> <entity id="C94-2106.28">dictionary</entity> consisting of 15,000 <entity id="C94-2106.29">pairs</entity> of <entity id="C94-2106.30">Japanese</entity> <entity id="C94-2106.31">valence</entity> <entity id="C94-2106.32">patterns</entity> and equivalent <entity id="C94-2106.33">English</entity> <entity id="C94-2106.34">syntactic structures</entity>.
</abstract>


</text>

<text id="C86-1046"><title><entity id="C86-1046.1">Dependency</entity> <entity id="C86-1046.2">Unification</entity> Grammar
</title><abstract>
This <entity id="C86-1046.3">paper</entity> describes the <entity id="C86-1046.4">analysis</entity> <entity id="C86-1046.5">component</entity> of the <entity id="C86-1046.6">language processing system</entity> PLAIN from the viewpoint of <entity id="C86-1046.7">unification</entity> grammars. The <entity id="C86-1046.8">principles</entity> of <entity id="C86-1046.9">Dependency</entity> <entity id="C86-1046.10">Unification</entity> Grammar (DUG) are discussed. The <entity id="C86-1046.11">computer</entity> <entity id="C86-1046.12">language</entity> DRL (<entity id="C86-1046.13">Dependency</entity> <entity id="C86-1046.14">Representation Language</entity>) is introduced in which DUGs can be formulated. A <entity id="C86-1046.15">unification-based</entity> <entity id="C86-1046.16">parsing</entity> <entity id="C86-1046.17">procedure</entity> is <entity id="C86-1046.18">part</entity> of the <entity id="C86-1046.19">formalism</entity>. PLAIN is <entity id="C86-1046.20">implemented</entity> at. the <entity id="C86-1046.21">universities</entity> of Heidelberg, Bonn, Flensburg, Kiel, Zurich and Cambridge U.K.
</abstract>


</text>

<text id="D08-1100"><title>
Acquiring <entity id="D08-1100.1">Domain-</entity>Specific <entity id="D08-1100.2">Dialog</entity> <entity id="D08-1100.3">Information</entity> from <entity id="D08-1100.4">Task-</entity>Oriented Human-Human <entity id="D08-1100.5">Interaction</entity> through an Unsupervised <entity id="D08-1100.6">Learning</entity></title><abstract>
We describe an <entity id="D08-1100.7">approach</entity> for acquiring the <entity id="D08-1100.8">domain-specific</entity> <entity id="D08-1100.9">dialog</entity> <entity id="D08-1100.10">knowledge</entity> <entity id="D08-1100.11">required</entity> to configure a <entity id="D08-1100.12">task-oriented</entity> <entity id="D08-1100.13">dialog system</entity> that uses human-human <entity id="D08-1100.14">interaction</entity><entity id="D08-1100.15">data</entity>. The key <entity id="D08-1100.16">aspects</entity> of this <entity id="D08-1100.17">problem</entity> are the <entity id="D08-1100.18">design</entity> of a <entity id="D08-1100.19">dialog</entity> <entity id="D08-1100.20">information</entity> <entity id="D08-1100.21">representation</entity> and a learning <entity id="D08-1100.22">approach</entity> that <entity id="D08-1100.23">supports</entity> capture of <entity id="D08-1100.24">domain</entity> <entity id="D08-1100.25">information</entity> from <entity id="D08-1100.26">in-domain</entity> <entity id="D08-1100.27">dialogs</entity>. 
</abstract>


</text>

<text id="L08-1189"><title>
A <entity id="L08-1189.1">Development</entity> <entity id="L08-1189.2">Environment</entity> for Configurable Meta-Annotators in a Pipelined NLP <entity id="L08-1189.3">Architecture</entity></title><abstract>
<entity id="L08-1189.4">Information extraction</entity> from large<entity id="L08-1189.5">data</entity> <entity id="L08-1189.6">repositories</entity> is critical to <entity id="L08-1189.7">Information</entity> <entity id="L08-1189.8">Management</entity> <entity id="L08-1189.9">solutions</entity>. In <entity id="L08-1189.10">addition</entity> to prerequisite <entity id="L08-1189.11">corpus</entity> <entity id="L08-1189.12">analysis</entity>, to determine <entity id="L08-1189.13">domain-specific</entity> <entity id="L08-1189.14">characteristics</entity> of <entity id="L08-1189.15">text</entity> <entity id="L08-1189.16">resources</entity>, <entity id="L08-1189.17">developing</entity>, refining and <entity id="L08-1189.18">evaluating</entity> analytics entails a <entity id="L08-1189.19">complex</entity> and lengthy <entity id="L08-1189.20">process</entity>, typically <entity id="L08-1189.21">requiring</entity> more than just <entity id="L08-1189.22">domain</entity> expertise. Modern <entity id="L08-1189.23">architectures</entity> for <entity id="L08-1189.24">text processing</entity>, while facilitating reuse and (re-)composition of analytical <entity id="L08-1189.25">pipelines</entity>, place additional <entity id="L08-1189.26">constraints</entity> upon the analytics <entity id="L08-1189.27">development</entity>, as <entity id="L08-1189.28">domain</entity> <entity id="L08-1189.29">experts</entity> need not only configure <entity id="L08-1189.30">individual</entity> annotator <entity id="L08-1189.31">components</entity>, but situate these within a fully functional annotator <entity id="L08-1189.32">pipeline</entity>. We present the <entity id="L08-1189.33">design</entity>, and <entity id="L08-1189.34">current</entity> <entity id="L08-1189.35">status</entity>, of a <entity id="L08-1189.36">tool</entity> for configuring <entity id="L08-1189.37">model-driven</entity> annotators, which <entity id="L08-1189.38">abstracts</entity> away from annotator <entity id="L08-1189.39">implementation</entity> <entity id="L08-1189.40">details</entity>, <entity id="L08-1189.41">pipeline</entity> composition <entity id="L08-1189.42">constraints</entity>, and<entity id="L08-1189.43">data</entity> <entity id="L08-1189.44">management</entity>. Instead, the <entity id="L08-1189.45">tool</entity> embodies <entity id="L08-1189.46">support</entity> for all stages of <entity id="L08-1189.47">ontology-centric</entity> <entity id="L08-1189.48">model</entity> <entity id="L08-1189.49">development</entity> <entity id="L08-1189.50">cycle</entity> - from <entity id="L08-1189.51">corpus</entity> <entity id="L08-1189.52">analysis</entity> and <entity id="L08-1189.53">concept</entity> <entity id="L08-1189.54">definition</entity>, to <entity id="L08-1189.55">model</entity> <entity id="L08-1189.56">development</entity> and testing, to large <entity id="L08-1189.57">scale</entity> <entity id="L08-1189.58">evaluation</entity>, to easy and rapid composition of <entity id="L08-1189.59">text</entity> <entity id="L08-1189.60">applications</entity> deploying these <entity id="L08-1189.61">concept</entity> <entity id="L08-1189.62">models</entity>. With our <entity id="L08-1189.63">design</entity>, we aim to meet the needs of <entity id="L08-1189.64">domain</entity> <entity id="L08-1189.65">experts</entity>, who are not necessarily <entity id="L08-1189.66">expert</entity> NLP practitioners.
</abstract>


</text>

<text id="L08-1002"><title><entity id="L08-1002.1">Combining</entity> Multiple <entity id="L08-1002.2">Models</entity> for <entity id="L08-1002.3">Speech</entity> <entity id="L08-1002.4">Information Retrieval</entity></title><abstract>
In this article we present a <entity id="L08-1002.5">method</entity> for combining different <entity id="L08-1002.6">information retrieval</entity> <entity id="L08-1002.7">models</entity> in <entity id="L08-1002.8">order</entity> to <entity id="L08-1002.9">increase</entity> the <entity id="L08-1002.10">retrieval</entity> <entity id="L08-1002.11">performance</entity> in a <entity id="L08-1002.12">Speech</entity> <entity id="L08-1002.13">Information Retrieval</entity> <entity id="L08-1002.14">task</entity>. The <entity id="L08-1002.15">formulas</entity> for combining the <entity id="L08-1002.16">models</entity> are tuned on <entity id="L08-1002.17">training</entity><entity id="L08-1002.18">data</entity>. Then the <entity id="L08-1002.19">system</entity> is <entity id="L08-1002.20">evaluated</entity> on <entity id="L08-1002.21">test</entity><entity id="L08-1002.22">data</entity>. The <entity id="L08-1002.23">task</entity> is particularly difficult because the <entity id="L08-1002.24">text</entity> <entity id="L08-1002.25">collection</entity> is automatically transcribed <entity id="L08-1002.26">spontaneous speech</entity>, with many <entity id="L08-1002.27">recognition</entity> <entity id="L08-1002.28">errors</entity>. Also, the <entity id="L08-1002.29">topics</entity> are real <entity id="L08-1002.30">information</entity> needs, difficult to satisfy. <entity id="L08-1002.31">Information Retrieval systems</entity> are not able to obtain good <entity id="L08-1002.32">results</entity> on this <entity id="L08-1002.33">data</entity> set, except for the <entity id="L08-1002.34">case</entity> when <entity id="L08-1002.35">manual</entity> <entity id="L08-1002.36">summaries</entity> are <entity id="L08-1002.37">included</entity>.
</abstract>


</text>

<text id="L08-1022"><title>
Subdomain Sensitive <entity id="L08-1022.1">Statistical</entity> <entity id="L08-1022.2">Parsing</entity> using Raw <entity id="L08-1022.3">Corpora</entity></title><abstract>
Modern <entity id="L08-1022.4">statistical</entity> <entity id="L08-1022.5">parsers</entity> are <entity id="L08-1022.6">trained</entity> on large annotated <entity id="L08-1022.7">corpora</entity> (treebanks). These treebanks usually consist of <entity id="L08-1022.8">sentences</entity> addressing different subdomains (e.g. <entity id="L08-1022.9">sports</entity>, politics, music), which implies that the <entity id="L08-1022.10">statistics</entity> gathered by <entity id="L08-1022.11">current</entity> <entity id="L08-1022.12">statistical</entity> <entity id="L08-1022.13">parsers</entity> are <entity id="L08-1022.14">mixtures</entity> of subdomains of <entity id="L08-1022.15">language</entity> use. In this <entity id="L08-1022.16">paper</entity> we present a <entity id="L08-1022.17">method</entity> that exploits raw subdomain <entity id="L08-1022.18">corpora</entity> gathered from the web to introduce subdomain <entity id="L08-1022.19">sensitivity</entity> into a given <entity id="L08-1022.20">parser</entity>. We employ <entity id="L08-1022.21">statistical</entity> <entity id="L08-1022.22">techniques</entity> for creating an ensemble of <entity id="L08-1022.23">domain</entity> sensitive <entity id="L08-1022.24">parsers</entity>, and explore <entity id="L08-1022.25">methods</entity> for amalgamating their <entity id="L08-1022.26">predictions</entity>. Our <entity id="L08-1022.27">experiments</entity> show that introducing <entity id="L08-1022.28">domain</entity> <entity id="L08-1022.29">sensitivity</entity> by exploiting raw <entity id="L08-1022.30">corpora</entity> can <entity id="L08-1022.31">improve</entity> over a tough, state-of-the-art baseline.
</abstract>


</text>

<text id="E99-1027"><title>
An <entity id="E99-1027.1">Experiment</entity> On The Upper <entity id="E99-1027.2">Bound</entity> Of Interjudge <entity id="E99-1027.3">Agreement</entity>: The <entity id="E99-1027.4">Case</entity> Of <entity id="E99-1027.5">Tagging</entity></title><abstract>
We investigate the controversial <entity id="E99-1027.6">issue</entity> about the upper bound of interjudge <entity id="E99-1027.7">agreement</entity> in the use of a <entity id="E99-1027.8">low-level</entity> grammatical <entity id="E99-1027.9">representation</entity>. Pessimistic views suggest that several percent of <entity id="E99-1027.10">words</entity> in running <entity id="E99-1027.11">text</entity> are undecidable in <entity id="E99-1027.12">terms</entity> of <entity id="E99-1027.13">part-of-speech</entity> <entity id="E99-1027.14">categories</entity>. Our <entity id="E99-1027.15">experiments</entity> with 55kW<entity id="E99-1027.16">data</entity> give <entity id="E99-1027.17">reason</entity> for optimism: <entity id="E99-1027.18">linguists</entity> with only 30 hours' <entity id="E99-1027.19">training</entity> <entity id="E99-1027.20">apply</entity> the EngCG-2 morphological <entity id="E99-1027.21">tags</entity> with almost 100% interjudge <entity id="E99-1027.22">agreement</entity>.
</abstract>


</text>

<text id="E99-1049"><title>
Pointing To <entity id="E99-1049.1">Events</entity></title><abstract>
Although there is an extensive body of <entity id="E99-1049.2">research</entity> <entity id="E99-1049.3">concerned</entity> with <entity id="E99-1049.4">anaphora resolution</entity> (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), <entity id="E99-1049.5">event</entity> anaphora has been widely neglected. This <entity id="E99-1049.6">paper</entity> describes the <entity id="E99-1049.7">results</entity> of an empirical <entity id="E99-1049.8">study</entity> regarding <entity id="E99-1049.9">event</entity> <entity id="E99-1049.10">reference</entity>. The <entity id="E99-1049.11">experiment</entity> investigated <entity id="E99-1049.12">event</entity> anaphora in narrative <entity id="E99-1049.13">discourse</entity> via a <entity id="E99-1049.14">sentence</entity> <entity id="E99-1049.15">completion</entity> <entity id="E99-1049.16">task</entity>.
</abstract>


</text>

<text id="E03-1051"><title><entity id="E03-1051.1">Learning</entity> PP <entity id="E03-1051.2">Attachment</entity> For Filtering Prosodic Phrasing
</title><abstract>
We explore learning <entity id="E03-1051.3">prepositional-phrase</entity> <entity id="E03-1051.4">attachment</entity> in Dutch, to use it as a filter in prosodic phrasing. From a <entity id="E03-1051.5">syntactic</entity> treebank of spoken Dutch we <entity id="E03-1051.6">extract</entity> <entity id="E03-1051.7">instances</entity> of the <entity id="E03-1051.8">attachment</entity> of <entity id="E03-1051.9">prepositional phrases</entity> to either a governing <entity id="E03-1051.10">verb</entity> or <entity id="E03-1051.11">noun</entity>. Using <entity id="E03-1051.12">cross-validated</entity> <entity id="E03-1051.13">parameter</entity> and <entity id="E03-1051.14">feature selection</entity>, we <entity id="E03-1051.15">train</entity> two learning <entity id="E03-1051.16">algorithms</entity>, iBl and RIPPER, on making this <entity id="E03-1051.17">distinction</entity>, <entity id="E03-1051.18">based</entity> on unigram and bigram <entity id="E03-1051.19">lexical features</entity> and a cooccurrence <entity id="E03-1051.20">feature</entity> derived from WWW counts. We optimize the learning on <entity id="E03-1051.21">noun</entity> <entity id="E03-1051.22">attachment</entity>, since in a second stage we use the <entity id="E03-1051.23">attachment</entity> <entity id="E03-1051.24">decision</entity> for blocking the incorrect placement of <entity id="E03-1051.25">phrase</entity> <entity id="E03-1051.26">boundaries</entity> before <entity id="E03-1051.27">prepositional phrases</entity> attached to the preceding <entity id="E03-1051.28">noun</entity>. On <entity id="E03-1051.29">noun</entity> <entity id="E03-1051.30">attachment</entity>, IBl attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using <entity id="E03-1051.31">attachment</entity> <entity id="E03-1051.32">decisions</entity> from IBl <entity id="E03-1051.33">yields</entity> the best <entity id="E03-1051.34">improvement</entity> on <entity id="E03-1051.35">precision</entity> (by six points to 71) on <entity id="E03-1051.36">phrase</entity> <entity id="E03-1051.37">boundary</entity> placement.
</abstract>


</text>

<text id="C96-2210"><title>
A <entity id="C96-2210.1">Distributed</entity> <entity id="C96-2210.2">Architecture</entity> For <entity id="C96-2210.3">Text Analysis</entity> In French: An <entity id="C96-2210.4">Application</entity> To <entity id="C96-2210.5">Complex</entity> Linguistic Phenomena <entity id="C96-2210.6">Processing</entity></title><abstract>
Most <entity id="C96-2210.7">Natural Language Processing systems</entity> use a sequential <entity id="C96-2210.8">architecture</entity> embodying classical linguistic <entity id="C96-2210.9">layers</entity>. When one works with a general <entity id="C96-2210.10">language</entity>; and not a sublanguage, there are different <entity id="C96-2210.11">cases</entity> of <entity id="C96-2210.12">ambiguities</entity> at different classical <entity id="C96-2210.13">levels</entity>; and more particularly when one works on <entity id="C96-2210.14">complex</entity> <entity id="C96-2210.15">language</entity> <entity id="C96-2210.16">phenomena</entity> <entity id="C96-2210.17">analysis</entity> (<entity id="C96-2210.18">coordination</entity>, <entity id="C96-2210.19">ellipsis</entity>, <entity id="C96-2210.20">negation</entity>...) it is difficult to take into account all the different <entity id="C96-2210.21">types</entity> of these <entity id="C96-2210.22">constructions</entity> with a general grammar. Indeed, the inconvenience of this <entity id="C96-2210.23">approach</entity> is the possible risk of a combinatory <entity id="C96-2210.24">explosion</entity>. So, we have defined the TALISMAN <entity id="C96-2210.25">architecture</entity> that <entity id="C96-2210.26">includes</entity> linguistic <entity id="C96-2210.27">agents</entity> that correspond either to classical <entity id="C96-2210.28">levels</entity> in <entity id="C96-2210.29">linguistics</entity> (<entity id="C96-2210.30">morphology</entity>, <entity id="C96-2210.31">syntax</entity>, <entity id="C96-2210.32">semantic</entity>) or to <entity id="C96-2210.33">complex</entity> <entity id="C96-2210.34">language</entity> <entity id="C96-2210.35">phenomena</entity> <entity id="C96-2210.36">analysis</entity>.
</abstract>


</text>

<text id="C00-1058"><title><entity id="C00-1058.1">Automatic</entity> <entity id="C00-1058.2">Thesaurus</entity> <entity id="C00-1058.3">Generation</entity> Through Multiple Filtering
</title><abstract>
In this <entity id="C00-1058.4">paper</entity>, we <entity id="C00-1058.5">propose</entity> a <entity id="C00-1058.6">method</entity> of <entity id="C00-1058.7">generating</entity> bilingual <entity id="C00-1058.8">keyword</entity> <entity id="C00-1058.9">clusters</entity> or <entity id="C00-1058.10">thesauri</entity> from parallel or comparable bilingual <entity id="C00-1058.11">corpora</entity>. The <entity id="C00-1058.12">method</entity> combines morphological and <entity id="C00-1058.13">lexical</entity> <entity id="C00-1058.14">processing</entity>, bilingual <entity id="C00-1058.15">word alignment</entity>, and graph-theoretic <entity id="C00-1058.16">cluster</entity> <entity id="C00-1058.17">generation</entity>. An <entity id="C00-1058.18">experiment</entity> shows that the <entity id="C00-1058.19">method</entity> is promising.
</abstract>


</text>

<text id="C02-1086"><title>
Implicit <entity id="C02-1086.1">Ambiguity</entity> <entity id="C02-1086.2">Resolution</entity> Using Incremental Clustering In Korean-To-<entity id="C02-1086.3">English</entity> <entity id="C02-1086.4">Cross-</entity><entity id="C02-1086.5">Language</entity> <entity id="C02-1086.6">Information Retrieval</entity></title><abstract>
This <entity id="C02-1086.7">paper</entity> presents a <entity id="C02-1086.8">method</entity> to implicitly resolve <entity id="C02-1086.9">ambiguities</entity> using dynamic incremental <entity id="C02-1086.10">clustering</entity> in Korean-to-<entity id="C02-1086.11">English</entity> <entity id="C02-1086.12">cross-language</entity> <entity id="C02-1086.13">information retrieval</entity>. In the <entity id="C02-1086.14">framework</entity> we <entity id="C02-1086.15">propose</entity>, a <entity id="C02-1086.16">query</entity> in Korean is first <entity id="C02-1086.17">translated</entity> into <entity id="C02-1086.18">English</entity> by looking up Korean-<entity id="C02-1086.19">English</entity> <entity id="C02-1086.20">dictionary</entity>, then <entity id="C02-1086.21">documents</entity> are retrieved <entity id="C02-1086.22">based</entity> on the <entity id="C02-1086.23">vector</entity> <entity id="C02-1086.24">space</entity> <entity id="C02-1086.25">retrieval</entity> for the <entity id="C02-1086.26">translated</entity> <entity id="C02-1086.27">query</entity> <entity id="C02-1086.28">terms</entity>. For the top-ranked retrieved <entity id="C02-1086.29">documents</entity>, <entity id="C02-1086.30">query-oriented</entity> <entity id="C02-1086.31">document</entity> <entity id="C02-1086.32">clusters</entity> are incrementally created and the <entity id="C02-1086.33">weight</entity> of each retrieved <entity id="C02-1086.34">document</entity> is re-calculated by using <entity id="C02-1086.35">clusters</entity>. In <entity id="C02-1086.36">experiment</entity> on TREC-6 CLIR <entity id="C02-1086.37">test</entity> <entity id="C02-1086.38">collection</entity>, our <entity id="C02-1086.39">method</entity> achieved 28.29% <entity id="C02-1086.40">performance improvement</entity> for <entity id="C02-1086.41">translated</entity> <entity id="C02-1086.42">queries</entity> without <entity id="C02-1086.43">ambiguity</entity> <entity id="C02-1086.44">resolution</entity> for <entity id="C02-1086.45">queries</entity>. This corresponds to 97.27% of the monolingual <entity id="C02-1086.46">performance</entity> for original <entity id="C02-1086.47">queries</entity>. When we combine our <entity id="C02-1086.48">method</entity> with <entity id="C02-1086.49">query</entity> <entity id="C02-1086.50">ambiguity</entity> <entity id="C02-1086.51">resolution</entity>, our <entity id="C02-1086.52">method</entity> even outperforms the monolingual <entity id="C02-1086.53">retrieval</entity>.
</abstract>


</text>

<text id="C04-1091"><title>
An Algorithmic <entity id="C04-1091.1">Framework</entity> For <entity id="C04-1091.2">Solving</entity> The Decoding <entity id="C04-1091.3">Problem</entity> In <entity id="C04-1091.4">Statistical Machine Translation</entity></title><abstract>
The decoding <entity id="C04-1091.5">problem</entity> in <entity id="C04-1091.6">Statistical Machine Translation</entity> (SMT) is a computationally hard combinatorial <entity id="C04-1091.7">optimization problem</entity>. In this <entity id="C04-1091.8">paper</entity>, we <entity id="C04-1091.9">propose</entity> a new algorithmic <entity id="C04-1091.10">framework</entity> for <entity id="C04-1091.11">solving</entity> the decoding <entity id="C04-1091.12">problem</entity> and demonstrate its <entity id="C04-1091.13">utility</entity>. In the new algorithmic <entity id="C04-1091.14">framework</entity>, the decoding <entity id="C04-1091.15">problem</entity> can be <entity id="C04-1091.16">solved</entity> both exactly and approximately. The key idea behind the <entity id="C04-1091.17">framework</entity> is the <entity id="C04-1091.18">modeling</entity> of the decoding <entity id="C04-1091.19">problem</entity> as one that involves alternating <entity id="C04-1091.20">maximization</entity> of two relatively simpler subproblems. We show how the subproblems can be <entity id="C04-1091.21">solved</entity> efficiently and how their <entity id="C04-1091.22">solutions</entity> can be combined to arrive at a <entity id="C04-1091.23">solution</entity> for the decoding <entity id="C04-1091.24">problem</entity>. A family of provably fast decoding <entity id="C04-1091.25">algorithms</entity> can be derived from the <entity id="C04-1091.26">basic</entity> <entity id="C04-1091.27">techniques</entity> underlying the <entity id="C04-1091.28">framework</entity> and we present a few illustrations. Our first <entity id="C04-1091.29">algorithm</entity> is a prov-ably linear <entity id="C04-1091.30">time</entity> <entity id="C04-1091.31">search algorithm</entity>. We use this <entity id="C04-1091.32">algorithm</entity> as a subroutine in the other <entity id="C04-1091.33">algorithms</entity>. We believe that decoding <entity id="C04-1091.34">algorithms</entity> derived from our <entity id="C04-1091.35">framework</entity> can be of practical <entity id="C04-1091.36">significance</entity>.
</abstract>


</text>

<text id="C04-1155"><title>
A Flexible <entity id="C04-1155.1">Example</entity> Annotation <entity id="C04-1155.2">Schema</entity>: <entity id="C04-1155.3">Translation</entity> Corresponding <entity id="C04-1155.4">Tree</entity> <entity id="C04-1155.5">Representation</entity></title><abstract>
This <entity id="C04-1155.6">paper</entity> presents work on the <entity id="C04-1155.7">task</entity> of <entity id="C04-1155.8">constructing</entity> an <entity id="C04-1155.9">example</entity> <entity id="C04-1155.10">base</entity> from a given bilingual <entity id="C04-1155.11">corpus</entity> <entity id="C04-1155.12">based</entity> on the annotation <entity id="C04-1155.13">schema</entity> of <entity id="C04-1155.14">Translation</entity> Corresponding <entity id="C04-1155.15">Tree</entity> (TCT). Each TCT describes a <entity id="C04-1155.16">translation</entity> <entity id="C04-1155.17">example</entity> (a <entity id="C04-1155.18">pair</entity> of bilingual <entity id="C04-1155.19">sentences</entity>). It represents the <entity id="C04-1155.20">syntactic structure</entity> of <entity id="C04-1155.21">source language</entity> <entity id="C04-1155.22">sentence</entity>, and more importantly is the facility to specify the <entity id="C04-1155.23">correspondences</entity> between <entity id="C04-1155.24">string</entity> (both the <entity id="C04-1155.25">source</entity> and <entity id="C04-1155.26">target</entity> <entity id="C04-1155.27">sentences</entity>) and the <entity id="C04-1155.28">representation</entity> <entity id="C04-1155.29">tree</entity>. Furthermore, <entity id="C04-1155.30">syntax</entity> <entity id="C04-1155.31">transformation</entity> clues are also encapsulated at each <entity id="C04-1155.32">node</entity> in the TCT <entity id="C04-1155.33">representation</entity> to capture the differentiation of grammatical <entity id="C04-1155.34">structure</entity> between the <entity id="C04-1155.35">source</entity> and <entity id="C04-1155.36">target languages</entity>. With this annotation <entity id="C04-1155.37">schema</entity>, <entity id="C04-1155.38">translation</entity> <entity id="C04-1155.39">examples</entity> are effectively represented and organized in the bilingual <entity id="C04-1155.40">knowledge</entity> <entity id="C04-1155.41">database</entity> that we need for the Portuguese to <entity id="C04-1155.42">Chinese</entity> <entity id="C04-1155.43">machine translation system</entity>.
</abstract>


</text>

<text id="C04-1204"><title>
Deep <entity id="C04-1204.1">Linguistic Analysis</entity> For The Accurate <entity id="C04-1204.2">Identification</entity> Of Predicate-<entity id="C04-1204.3">Argument</entity> <entity id="C04-1204.4">Relations</entity></title><abstract>
This <entity id="C04-1204.5">paper</entity> <entity id="C04-1204.6">evaluates</entity> the <entity id="C04-1204.7">accuracy</entity> of HPSG <entity id="C04-1204.8">parsing</entity> in <entity id="C04-1204.9">terms</entity> of the <entity id="C04-1204.10">identification</entity> of <entity id="C04-1204.11">predicate-argument</entity> <entity id="C04-1204.12">relations</entity>. We could directly compare the <entity id="C04-1204.13">output</entity> of HPSG <entity id="C04-1204.14">parsing</entity> with <entity id="C04-1204.15">Prop-</entity><entity id="C04-1204.16">Bank</entity> annotations, by assuming a unique <entity id="C04-1204.17">mapping</entity> from HPSG <entity id="C04-1204.18">semantic representation</entity> into PropBank annotation. Even though PropBank was not used for the <entity id="C04-1204.19">training</entity> of a <entity id="C04-1204.20">disambiguation</entity> <entity id="C04-1204.21">model</entity>, an HPSG <entity id="C04-1204.22">parser</entity> achieved the <entity id="C04-1204.23">accuracy</entity> competitive with existing <entity id="C04-1204.24">studies</entity> on the <entity id="C04-1204.25">task</entity> of identifying PropBank annotations.
</abstract>


</text>

<text id="E91-1004"><title>
Pearl: A Probabilistic Chart <entity id="E91-1004.1">Parser</entity></title><abstract>
This <entity id="E91-1004.2">paper</entity> describes a <entity id="E91-1004.3">natural language</entity> <entity id="E91-1004.4">parsing</entity> <entity id="E91-1004.5">algorithm</entity> for unrestricted <entity id="E91-1004.6">text</entity> which uses a <entity id="E91-1004.7">probability-based</entity> scoring <entity id="E91-1004.8">function</entity> to select the ""best"" <entity id="E91-1004.9">parse</entity> of a <entity id="E91-1004.10">sentence</entity>. The <entity id="E91-1004.11">parser</entity>, Pearl, is a <entity id="E91-1004.12">time-asynchronous</entity> bottom-up chart <entity id="E91-1004.13">parser</entity> with <entity id="E91-1004.14">Earley-type</entity> top-down <entity id="E91-1004.15">prediction</entity> which pursues the highest-scoring <entity id="E91-1004.16">theory</entity> in the chart, where the score of a <entity id="E91-1004.17">theory</entity> represents the <entity id="E91-1004.18">extent</entity> to which the <entity id="E91-1004.19">context</entity>, of the <entity id="E91-1004.20">sentence</entity> predicts that <entity id="E91-1004.21">interpretation</entity>. This <entity id="E91-1004.22">parser</entity> differs from previous attempts at stochastic <entity id="E91-1004.23">parsers</entity> in that it uses a richer <entity id="E91-1004.24">form</entity> of <entity id="E91-1004.25">conditional probabilities</entity> <entity id="E91-1004.26">based</entity> on <entity id="E91-1004.27">context</entity>, to predict, likelihood. 'Pearl also <entity id="E91-1004.28">provides</entity> a <entity id="E91-1004.29">framework</entity> for incorporating the <entity id="E91-1004.30">results</entity> of previous work in pait-ol'-<entity id="E91-1004.31">speech</entity> <entity id="E91-1004.32">assignment</entity>, unknown <entity id="E91-1004.33">word models</entity>, and other <entity id="E91-1004.34">probabilistic models</entity> of <entity id="E91-1004.35">linguistic features</entity> into one <entity id="E91-1004.36">parsing</entity> <entity id="E91-1004.37">tool</entity>, interleaving these <entity id="E91-1004.38">techniques</entity> instead of using the traditional <entity id="E91-1004.39">pipeline</entity> <entity id="E91-1004.40">architecture</entity>. In preliminary <entity id="E91-1004.41">tests</entity>, 'Pearl has been successful at resolving <entity id="E91-1004.42">part-of-speech</entity> and <entity id="E91-1004.43">word</entity> (in <entity id="E91-1004.44">speech processing</entity>) <entity id="E91-1004.45">ambiguity</entity>, determining <entity id="E91-1004.46">categories</entity> for <entity id="E91-1004.47">unknown words</entity>, and selecting correct <entity id="E91-1004.48">parses</entity> first using a very loosely fitting covering grammar."
</abstract>


</text>

<text id="L08-1227">
<title>
Relationships between Nursing Converstaions and Activities
</title>
<abstract>
In this <entity id="L08-1227.1">paper</entity> , we determine the <entity id="L08-1227.2">relationships</entity> between nursing activities and nurseing <entity id="L08-1227.3">conversations</entity> <entity id="L08-1227.4">based</entity> on the <entity id="L08-1227.5">principle</entity> of <entity id="L08-1227.6">maximum entropy</entity> . For <entity id="L08-1227.7">analysis</entity> of the <entity id="L08-1227.8">features</entity> of nursing activities, we built nursing <entity id="L08-1227.9">corpora</entity> from actual nursing <entity id="L08-1227.10">conversation</entity> sets collected in hospitals that involve various <entity id="L08-1227.11">information</entity> about nursing activities. Ex-nurses manually assigned nursing activity <entity id="L08-1227.12">information</entity> to the nursing <entity id="L08-1227.13">conversations</entity> in the <entity id="L08-1227.14">corpora</entity> . Since it is inefficient and too expensive to attach all <entity id="L08-1227.15">information</entity> manually, we introduced an <entity id="L08-1227.16">automatic</entity> nursing activity <entity id="L08-1227.17">determination</entity> <entity id="L08-1227.18">method</entity> for which we built <entity id="L08-1227.19">models</entity> of <entity id="L08-1227.20">relationships</entity> between nursing <entity id="L08-1227.21">conversations</entity> and activities. In this <entity id="L08-1227.22">paper</entity> , we adopted a <entity id="L08-1227.23">maximum entropy</entity> <entity id="L08-1227.24">approach</entity> for learning. Even though the <entity id="L08-1227.25">conversation</entity> <entity id="L08-1227.26">data</entity> set is not large enough for learning, acceptable <entity id="L08-1227.27">results</entity> were obtained.
</abstract>


</text>

<text id="D08-1022">
<title><entity id="D08-1022.1">Forest-based</entity> <entity id="D08-1022.2">Translation</entity> <entity id="D08-1022.3">Rule</entity> <entity id="D08-1022.4">Extraction</entity></title> 
<abstract><entity id="D08-1022.5">Translation</entity> <entity id="D08-1022.6">rule</entity> <entity id="D08-1022.7">extraction</entity> is a fundamental <entity id="D08-1022.8">problem</entity> in <entity id="D08-1022.9">machine <entity id="D08-1022.10">translation</entity></entity> , especially for linguistically <entity id="D08-1022.11">syntax-based</entity> packed <entity id="D08-1022.12">forest</entity></abstract>


</text>

<text id="D08-1048">
<title><entity id="D08-1048.1">Automatic</entity> <entity id="D08-1048.2">induction</entity> of FrameNet <entity id="D08-1048.3">lexical</entity> <entity id="D08-1048.4">units</entity></title>
<abstract>
Most attempts to integrate FrameNet in <entity id="D08-1048.5">NLP systems</entity> have so far failed because of its limited <entity id="D08-1048.6">coverage</entity> . In this <entity id="D08-1048.7">paper</entity> , we investigate the <entity id="D08-1048.8">applicability</entity> of distributional and <entity id="D08-1048.9">WordNet-based <entity id="D08-1048.10">models</entity></entity> on the <entity id="D08-1048.11">task</entity> of <entity id="D08-1048.12">lexical</entity> <entity id="D08-1048.13">unit</entity> <entity id="D08-1048.14">induction</entity> ,
</abstract>


</text>

<text id="I05-1038">
<title><entity id="I05-1038.1">Classification</entity> of Multiple- <entity id="I05-1038.2">Sentence</entity> <entity id="I05-1038.3">Questions</entity></title> 
<abstract><entity id="I05-1038.4">Abstract</entity> .
</abstract>


</text>

<text id="I05-1067">
<title>Using the <entity id="I05-1067.1">Structure</entity> of a Conceptual <entity id="I05-1067.2">Network</entity> in <entity id="I05-1067.3">Computing</entity> <entity id="I05-1067.4">Semantic</entity> <entity id="I05-1067.5">Relatedness</entity></title> 
<abstract><entity id="I05-1067.6">Abstract</entity> . proper
</abstract>


</text>

<text id="E99-1033">
<title>
Investigating NLG Architectures: Taking Style Into <entity id="E99-1033.1">Consideration</entity></title>
<abstract>
In this <entity id="E99-1033.2">paper</entity> we <entity id="E99-1033.3">propose</entity> a <entity id="E99-1033.4">methodology</entity> for investigating the <entity id="E99-1033.5">relationship</entity> between <entity id="E99-1033.6">architectures</entity> of <entity id="E99-1033.7">natural language generation</entity> (NLG)
</abstract>


</text>

<text id="E03-1010">
<title><entity id="E03-1010.1">Automatic <entity id="E03-1010.2">Evaluation</entity></entity> For A Palpable Measure Of A Speech <entity id="E03-1010.3">Translation <entity id="E03-1010.4">System</entity></entity> 's <entity id="E03-1010.5">Capability</entity></title>
<abstract>
The <entity id="E03-1010.6">main</entity> <entity id="E03-1010.7">goal</entity> of this <entity id="E03-1010.8">paper</entity> is to <entity id="E03-1010.9">propose</entity> <entity id="E03-1010.10">automatic</entity> <entity id="E03-1010.11">schemes</entity> for the <entity id="E03-1010.12">translation</entity> <entity id="E03-1010.13">paired</entity> <entity id="E03-1010.14">comparison</entity> <entity id="E03-1010.15">method</entity> . This <entity id="E03-1010.16">method</entity> was <entity id="E03-1010.17">proposed</entity> to precisely <entity id="E03-1010.18">evaluate</entity> a <entity id="E03-1010.19">speech translation</entity> <entity id="E03-1010.20">system</entity> 's <entity id="E03-1010.21">capability</entity> . Furthermore, the <entity id="E03-1010.22">method</entity> gives an <entity id="E03-1010.23">objective</entity> <entity id="E03-1010.24">evaluation <entity id="E03-1010.25">result</entity></entity> , i.e., a score of the <entity id="E03-1010.26">Test</entity> of <entity id="E03-1010.27">English</entity> for International <entity id="E03-1010.28">Communication</entity> (TOEIC). The TOEIC score is used as a measure of one's <entity id="E03-1010.29">speech translation</entity> <entity id="E03-1010.30">capability</entity> . However, this <entity id="E03-1010.31">method</entity> <entity id="E03-1010.32">requires</entity> tremendous <entity id="E03-1010.33">evaluation</entity> <entity id="E03-1010.34">costs</entity> . Accordingly, automatization of this <entity id="E03-1010.35">method</entity> is an important subject for <entity id="E03-1010.36">study</entity> . In the <entity id="E03-1010.37">proposed</entity> <entity id="E03-1010.38">method</entity> , currently available automatic <entity id="E03-1010.39">evaluation <entity id="E03-1010.40">methods</entity></entity> are <entity id="E03-1010.41">applied</entity> to automate the <entity id="E03-1010.42">translation</entity> <entity id="E03-1010.43">paired</entity> <entity id="E03-1010.44">comparison</entity> <entity id="E03-1010.45">method</entity> . In the <entity id="E03-1010.46">experiments</entity> , several <entity id="E03-1010.47">automatic evaluation</entity> <entity id="E03-1010.48">methods</entity> (BLEU, NIST, <entity id="E03-1010.49">DP-based method</entity> ) are <entity id="E03-1010.50">applied</entity> . The <entity id="E03-1010.51">experimental</entity> <entity id="E03-1010.52">results</entity> of these <entity id="E03-1010.53">automatic</entity> measures show a good <entity id="E03-1010.54">correlation</entity> with <entity id="E03-1010.55">evaluation <entity id="E03-1010.56">results</entity></entity> of the <entity id="E03-1010.57">translation</entity> <entity id="E03-1010.58">paired</entity> <entity id="E03-1010.59">comparison</entity> <entity id="E03-1010.60">method</entity> .
</abstract>


</text>

<text id="E03-1014">
<title>
Arabic <entity id="E03-1014.1">Syntactic</entity> Trees: From <entity id="E03-1014.2">Constituency</entity> To <entity id="E03-1014.3">Dependency</entity></title>
<abstract>
This <entity id="E03-1014.4">research</entity> <entity id="E03-1014.5">note</entity> <entity id="E03-1014.6">reports</entity> on the work in <entity id="E03-1014.7">progress</entity> which regards <entity id="E03-1014.8">automatic</entity> <entity id="E03-1014.9">transformation</entity> of <entity id="E03-1014.10">phrase-structure</entity> <entity id="E03-1014.11">syntactic</entity> <entity id="E03-1014.12">trees</entity> of Arabic into <entity id="E03-1014.13">dependency-driven</entity> analytical ones. <entity id="E03-1014.14">Guidelines</entity> for these <entity id="E03-1014.15">descriptions</entity> have been <entity id="E03-1014.16">developed</entity> at the Linguistic <entity id="E03-1014.17">Data</entity> Consortium, <entity id="E03-1014.18">University</entity> of Pennsylvania, and at the <entity id="E03-1014.19">Faculty</entity> of Mathematics and Physics and the <entity id="E03-1014.20">Faculty</entity> of Arts, Charles <entity id="E03-1014.21">University</entity> in Prague, respectively. The <entity id="E03-1014.22">transformation</entity> consists of (i) a recursive <entity id="E03-1014.23">function</entity> <entity id="E03-1014.24">translating</entity> the <entity id="E03-1014.25">topology</entity> of a <entity id="E03-1014.26">phrase</entity> <entity id="E03-1014.27">tree</entity> into a corresponding <entity id="E03-1014.28">dependency tree</entity> , and (ii) a <entity id="E03-1014.29">procedure</entity> assigning analytical <entity id="E03-1014.30">functions</entity> to the <entity id="E03-1014.31">nodes</entity> of the <entity id="E03-1014.32">dependency <entity id="E03-1014.33">tree</entity></entity> . Apart from an <entity id="E03-1014.34">outline</entity> of the annotation <entity id="E03-1014.35">schemes</entity> and a deeper <entity id="E03-1014.36">insight</entity> into these <entity id="E03-1014.37">procedures</entity> , <entity id="E03-1014.38">model</entity> <entity id="E03-1014.39">application</entity> of the <entity id="E03-1014.40">transformation</entity> is given herein.
</abstract>


</text>

<text id="N03-1008">
<title>
Latent <entity id="N03-1008.1">Semantic Information</entity> In <entity id="N03-1008.2">Maximum Entropy</entity> <entity id="N03-1008.3">Language</entity> <entity id="N03-1008.4">Models</entity> For Conversational <entity id="N03-1008.5">Speech <entity id="N03-1008.6">Recognition</entity></entity></title> 
<abstract><entity id="N03-1008.7">Latent <entity id="N03-1008.8">semantic analysis</entity></entity> (LSA), first exploited in <entity id="N03-1008.9">indexing</entity> <entity id="N03-1008.10">documents</entity> for <entity id="N03-1008.11">information <entity id="N03-1008.12">retrieval</entity></entity> , has since been used by several <entity id="N03-1008.13">researchers</entity> to demonstrate impressive <entity id="N03-1008.14">reductions</entity> in the <entity id="N03-1008.15">perplexity</entity> of <entity id="N03-1008.16">statistical language models</entity> on <entity id="N03-1008.17">text</entity> <entity id="N03-1008.18">corpora</entity> such as the <entity id="N03-1008.19">Wall Street Journal</entity> . In this <entity id="N03-1008.20">paper</entity> we present an <entity id="N03-1008.21">investigation</entity> into the use of LSA in <entity id="N03-1008.22">language <entity id="N03-1008.23">modeling</entity></entity> for conversational <entity id="N03-1008.24">speech <entity id="N03-1008.25">recognition</entity></entity> . We find that previously <entity id="N03-1008.26">proposed</entity> <entity id="N03-1008.27">methods</entity> of combining an LSA-based unigram <entity id="N03-1008.28">model</entity> with an N- <entity id="N03-1008.29">feature</entity></abstract>


</text>

<text id="N03-1013">
<title>
A Categorial <entity id="N03-1013.1">Variation</entity> <entity id="N03-1013.2">Database</entity> For <entity id="N03-1013.3">English</entity></title>
<abstract>
"We describe our <entity id="N03-1013.4">approach</entity> to the <entity id="N03-1013.5">construction</entity> and <entity id="N03-1013.6">evaluation</entity> of a <entity id="N03-1013.7">large-scale</entity> <entity id="N03-1013.8">database</entity> <entity id="N03-1013.9">called</entity> ""CatVar"" which contains categorial <entity id="N03-1013.10">variations</entity> of <entity id="N03-1013.11">English</entity> lexemes. <entity id="N03-1013.12">Due</entity> to the prevalence of <entity id="N03-1013.13">cross-language</entity> categorial <entity id="N03-1013.14">variation</entity> in multilingual <entity id="N03-1013.15">applications</entity> , our <entity id="N03-1013.16">categorial-variation</entity> <entity id="N03-1013.17">resource</entity> may serve as an integral <entity id="N03-1013.18">part</entity> of a diverse range of <entity id="N03-1013.19">natural language</entity> <entity id="N03-1013.20">applications</entity> . Thus, the <entity id="N03-1013.21">research</entity> <entity id="N03-1013.22">reported</entity> herein overlaps heavily with that of the <entity id="N03-1013.23">machine-translation</entity> , <entity id="N03-1013.24">lexicon-construction</entity> , and <entity id="N03-1013.25">information-retrieval</entity> <entity id="N03-1013.26">communities</entity> . We <entity id="N03-1013.27">apply</entity> the <entity id="N03-1013.28">information-retrieval</entity> <entity id="N03-1013.29">metrics</entity> of <entity id="N03-1013.30">precision</entity> and <entity id="N03-1013.31">recall</entity> to <entity id="N03-1013.32">evaluate</entity> the <entity id="N03-1013.33">accuracy</entity> and <entity id="N03-1013.34">coverage</entity> of our <entity id="N03-1013.35">database</entity> with <entity id="N03-1013.36">respect</entity> to a human-produced <entity id="N03-1013.37">gold <entity id="N03-1013.38">standard</entity></entity> . This <entity id="N03-1013.39">evaluation</entity> reveals that the categorial <entity id="N03-1013.40">database</entity> achieves a high <entity id="N03-1013.41">degree</entity> of <entity id="N03-1013.42">precision</entity> and <entity id="N03-1013.43">recall</entity> . Additionally, we demonstrate that the <entity id="N03-1013.44">database</entity> <entity id="N03-1013.45">improves</entity> on the linkability of Porter stemmer by over 30%. "
</abstract>


</text>

<text id="M91-1007">
<title>
GE NLTOOLSET: MUC-3 <entity id="M91-1007.1">Test</entity> <entity id="M91-1007.2">Results</entity> And <entity id="M91-1007.3">Analysis</entity></title>
<abstract>
This <entity id="M91-1007.4">paper</entity> <entity id="M91-1007.5">reports</entity> on the GE NLTOOLSET customization <entity id="M91-1007.6">effort</entity> for MUC-3, and analyzes the <entity id="M91-1007.7">results</entity> of the TST2 run. Although our own <entity id="M91-1007.8">tests</entity> had shown steady <entity id="M91-1007.9">improvement</entity> between TST1 and TST2, our official scores on TST2 were lo wer than on TST1. The <entity id="M91-1007.10">analysis</entity> of this unexpected <entity id="M91-1007.11">result</entity> explains some of the <entity id="M91-1007.12">details</entity> of th e MUC-3 <entity id="M91-1007.13">test</entity> , and we <entity id="M91-1007.14">propose</entity> ways of looking at the scores to distinguish different <entity id="M91-1007.15">aspects</entity> of <entity id="M91-1007.16">system</entity> <entity id="M91-1007.17">performance</entity> .
</abstract>


</text>

<text id="M92-1004">
<title><entity id="M92-1004.1">Text</entity> Filtering In MUC-3 And MUC-4
</title>
<abstract>
"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) <entity id="M92-1004.2">Message</entity> <entity id="M92-1004.3">Understanding</entity> Conference was the emergence of <entity id="M92-1004.4">text</entity> filtering as an explicit <entity id="M92-1004.5">topic</entity> of <entity id="M92-1004.6">discussion</entity> . In this <entity id="M92-1004.7">paper</entity> we examine <entity id="M92-1004.8">text</entity> filtering in MUC <entity id="M92-1004.9">systems</entity> with three <entity id="M92-1004.10">goals</entity> in mind. First, we clarify the <entity id="M92-1004.11">difference</entity> between two uses of the <entity id="M92-1004.12">term</entity> ""<entity id="M92-1004.13">text</entity> filtering"" in the <entity id="M92-1004.14">context</entity> of <entity id="M92-1004.15">data</entity> <entity id="M92-1004.16">extraction systems</entity> , and put these <entity id="M92-1004.17">phenomena</entity> in the <entity id="M92-1004.18">context</entity> of prior <entity id="M92-1004.19">research</entity> on <entity id="M92-1004.20">information retrieval</entity> (IR). Secondly, we discuss the use of <entity id="M92-1004.21">text</entity> filtering <entity id="M92-1004.22">components</entity> in MUC-3 and MUC-4 <entity id="M92-1004.23">systems</entity> , and present a preliminary <entity id="M92-1004.24">scheme</entity> for classifying <entity id="M92-1004.25">data</entity> <entity id="M92-1004.26">extraction <entity id="M92-1004.27">systems</entity></entity> in <entity id="M92-1004.28">terms</entity> of the <entity id="M92-1004.29">features</entity> over which they do <entity id="M92-1004.30">text</entity> filtering. Finally, we examine the <entity id="M92-1004.31">text</entity> filtering <entity id="M92-1004.32">effectiveness</entity> of MUC-3 and MUC-4 <entity id="M92-1004.33">systems</entity> , and introduce some <entity id="M92-1004.34">approaches</entity> to the <entity id="M92-1004.35">evaluation</entity> of <entity id="M92-1004.36">text</entity> filtering <entity id="M92-1004.37">systems</entity> which may be of interest themselves. Two <entity id="M92-1004.38">questions</entity> of crucial interest are whether <entity id="M92-1004.39">sites</entity> <entity id="M92-1004.40">improved</entity> their <entity id="M92-1004.41">system</entity> <entity id="M92-1004.42">level</entity> <entity id="M92-1004.43">text</entity> filtering <entity id="M92-1004.44">effectiveness</entity> from MUC-3 to MUC-4, and what the <entity id="M92-1004.45">effectiveness</entity> of MUC <entity id="M92-1004.46">systems</entity> would be on <entity id="M92-1004.47">real world</entity> <entity id="M92-1004.48">data</entity> <entity id="M92-1004.49">streams</entity> . Because of changes in both <entity id="M92-1004.50">test set</entity> and <entity id="M92-1004.51">system</entity> <entity id="M92-1004.52">design</entity> since MUC-3 we were not able to address the first <entity id="M92-1004.53">question</entity> . However, with <entity id="M92-1004.54">respect</entity> to the second <entity id="M92-1004.55">question</entity> , we present preliminary <entity id="M92-1004.56">evidence</entity> suggesting that the <entity id="M92-1004.57">text</entity> filtering <entity id="M92-1004.58">precision</entity> of MUC <entity id="M92-1004.59">systems</entity> declines with the <entity id="M92-1004.60">generality</entity> of the <entity id="M92-1004.61">data</entity> <entity id="M92-1004.62">stream</entity> they <entity id="M92-1004.63">process</entity> , i.e. the proportion of relevant <entity id="M92-1004.64">documents</entity> . The ramifications of this for future <entity id="M92-1004.65">research</entity> and for operational <entity id="M92-1004.66">systems</entity> are discussed. "
</abstract>


</text>

<abstract></abstract>

<text id="X93-1015">
<title><entity id="X93-1015.1">Template</entity> <entity id="X93-1015.2">Design</entity> For <entity id="X93-1015.3">Information <entity id="X93-1015.4">Extraction</entity></entity></title>
<abstract>
The <entity id="X93-1015.5">design</entity> of the <entity id="X93-1015.6">template</entity> for an <entity id="X93-1015.7">information extraction</entity> <entity id="X93-1015.8">application</entity> (or exercise) reflects the <entity id="X93-1015.9">nature</entity> of the <entity id="X93-1015.10">task</entity> and therefore crucially <entity id="X93-1015.11">affects</entity> the <entity id="X93-1015.12">success</entity> of the attempt to capture <entity id="X93-1015.13">information</entity> from <entity id="X93-1015.14">text</entity> . This <entity id="X93-1015.15">paper</entity> addresses the <entity id="X93-1015.16">template</entity> <entity id="X93-1015.17">design</entity> <entity id="X93-1015.18">requirement</entity> by discussing the general <entity id="X93-1015.19">principles</entity> or <entity id="X93-1015.20">template</entity> <entity id="X93-1015.21">definition</entity> <entity id="X93-1015.22">effort</entity> which is explicitly discussed in a <entity id="X93-1015.23">Case <entity id="X93-1015.24">Study</entity></entity> in the last <entity id="X93-1015.25">section</entity> of this <entity id="X93-1015.26">paper</entity> .
</abstract>


</text>

<text id="W90-0111">
<title><entity id="W90-0111.1">Selection</entity> : Salience, <entity id="W90-0111.2">Relevance</entity> And The Coupling Between <entity id="W90-0111.3">Domain-</entity> <entity id="W90-0111.4">Level</entity> Tasks And <entity id="W90-0111.5">Text</entity> Planning
</title>
<abstract>
In this <entity id="W90-0111.6">paper</entity> we examine some <entity id="W90-0111.7">issues</entity> pertaining to the <entity id="W90-0111.8">task</entity> of <entity id="W90-0111.9">selection</entity> in <entity id="W90-0111.10">text</entity> planning. We attempt to distinguish salience and <entity id="W90-0111.11">relevance</entity> , and characterize their <entity id="W90-0111.12">role</entity> as important fundamental <entity id="W90-0111.13">notions</entity> governing <entity id="W90-0111.14">selection</entity> . We also formulate the <entity id="W90-0111.15">problem</entity> of <entity id="W90-0111.16">selection</entity> of <entity id="W90-0111.17">text</entity> <entity id="W90-0111.18">content</entity> in <entity id="W90-0111.19">terms</entity> of the coupling between <entity id="W90-0111.20">domain-level</entity> <entity id="W90-0111.21">tasks</entity> and <entity id="W90-0111.22">text</entity> planning <entity id="W90-0111.23">tasks</entity> . We describe our <entity id="W90-0111.24">research</entity> on <entity id="W90-0111.25">generating</entity> bus <entity id="W90-0111.26">route</entity> <entity id="W90-0111.27">descriptions</entity> .Keywords: <entity id="W90-0111.28">Natural Language Generation</entity> , <entity id="W90-0111.29">Text</entity> Planning, <entity id="W90-0111.30">Selection</entity> , Salience, <entity id="W90-0111.31">Relevance</entity> , Coupling, <entity id="W90-0111.32">Route</entity> Descriptions
</abstract>


</text>

<text id="W93-0213">
<title>
Using <entity id="W93-0213.1">Cue</entity> Phrases To Determine Rhetorical <entity id="W93-0213.2">Relations</entity></title>
<abstract>
'<entity id="W93-0213.3">Relation</entity> <entity id="W93-0213.4">based</entity> ' <entity id="W93-0213.5">approaches</entity> to <entity id="W93-0213.6">discourse <entity id="W93-0213.7">analysis</entity></entity> and <entity id="W93-0213.8">text generation</entity> suffer from a <entity id="W93-0213.9">common</entity> <entity id="W93-0213.10">problem</entity> : there is considerable <entity id="W93-0213.11">disagreement</entity> between <entity id="W93-0213.12">researchers</entity> over the set of <entity id="W93-0213.13">relations</entity> which is <entity id="W93-0213.14">proposed</entity> . Few <entity id="W93-0213.15">researchers</entity> use identical sets of <entity id="W93-0213.16">relations</entity> , and many use <entity id="W93-0213.17">relations</entity> not found in any other sets. This proliferation of <entity id="W93-0213.18">relations</entity> has been pointed out before (eg Hovy [1]), and several <entity id="W93-0213.19">methods</entity> for justifying a <entity id="W93-0213.20">standard</entity> set of <entity id="W93-0213.21">relations</entity> have been <entity id="W93-0213.22">proposed</entity> : this <entity id="W93-0213.23">paper</entity> <entity id="W93-0213.24">reviews</entity> some of these, and presents a new <entity id="W93-0213.25">method</entity> of <entity id="W93-0213.26">justification</entity> which overcomes some awkward <entity id="W93-0213.27">problems</entity> .
</abstract>


</text>

<text id="W93-0231">
<title><entity id="W93-0231.1">Domain</entity> <entity id="W93-0231.2">Structure</entity> , Rhetorical <entity id="W93-0231.3">Structure</entity> , And <entity id="W93-0231.4">Text Structure</entity></title>
<abstract>
It is generally agreed that <entity id="W93-0231.5">text</entity> has <entity id="W93-0231.6">structure</entity> (at least, coherent <entity id="W93-0231.7">text</entity> does). Therefore, an <entity id="W93-0231.8">understanding</entity> and appreciation of <entity id="W93-0231.9">text <entity id="W93-0231.10">structure</entity></entity> must play some <entity id="W93-0231.11">role</entity> in building <entity id="W93-0231.12">computational</entity> <entity id="W93-0231.13">systems</entity> that are capable of using <entity id="W93-0231.14">text</entity> as people do. What is less clear is what are necessary and sufficient <entity id="W93-0231.15">sources</entity> of structure for a text-using system , and further, what such a <entity id="W93-0231.16">system</entity> needs to know about and do with these <entity id="W93-0231.17">structures</entity> in the <entity id="W93-0231.18">process</entity> of using <entity id="W93-0231.19">text</entity> . By using <entity id="W93-0231.20">text</entity> , I mean <entity id="W93-0231.21">understanding</entity> it or producing it; speaking it, writing it, or thinking about it. Li this <entity id="W93-0231.22">paper</entity> , I present a <entity id="W93-0231.23">case</entity> for the <entity id="W93-0231.24">importance</entity> of <entity id="W93-0231.25">domain</entity> <entity id="W93-0231.26">structure</entity> in <entity id="W93-0231.27">structuring</entity> <entity id="W93-0231.28">text</entity> , and discuss the <entity id="W93-0231.29">role</entity> of rhetorical <entity id="W93-0231.30">structure</entity> and intentionality.
</abstract>


</text>

<text id="W93-0311">
<title><entity id="W93-0311.1">Corpus-</entity> <entity id="W93-0311.2">Based</entity> <entity id="W93-0311.3">Adaptation</entity> Mechanisms For <entity id="W93-0311.4">Chinese</entity> Homophone <entity id="W93-0311.5">Disambiguation</entity></title> 
<abstract><entity id="W93-0311.6">Based</entity> on the <entity id="W93-0311.7">concepts</entity> of bidirectional <entity id="W93-0311.8">conversion</entity> and <entity id="W93-0311.9">automatic evaluation</entity> , we <entity id="W93-0311.10">propose</entity> two nser-adapiation <entity id="W93-0311.11">mechanisms</entity> , <entity id="W93-0311.12">character-preference</entity> <entity id="W93-0311.13">learning</entity> and <entity id="W93-0311.14">pstlido-word</entity> <entity id="W93-0311.15">learning</entity> , for resolving <entity id="W93-0311.16">Chinese</entity> homophone <entity id="W93-0311.17">ambiguities</entity> in syllable-to-character <entity id="W93-0311.18">conversion</entity> . The 1991  United Daily <entity id="W93-0311.19">corpus</entity> oj approximately 10 million <entity id="W93-0311.20">Chinese</entity> characters is used for <entity id="W93-0311.21">extraction</entity> of 10 reporter-specific article <entity id="W93-0311.22">databases</entity> and for <entity id="W93-0311.23">computation</entity> of <entity id="W93-0311.24">word</entity> <entity id="W93-0311.25">frequencies</entity> and character bi-grams. <entity id="W93-0311.26">Experiments</entity> show that 20.5 percent (testing sets) to 71.8 percent ( <entity id="W93-0311.27">training sets</entity> ) of <entity id="W93-0311.28">conversion</entity> <entity id="W93-0311.29">errors</entity> can be eliminated through the <entity id="W93-0311.30">proposed</entity> <entity id="W93-0311.31">mechanisms</entity> . These <entity id="W93-0311.32">concepts</entity> are thus very useful m <entity id="W93-0311.33">applications</entity> such as <entity id="W93-0311.34">Chinese</entity> <entity id="W93-0311.35">input</entity> <entity id="W93-0311.36">methods</entity> and <entity id="W93-0311.37">speech recognition systems</entity></abstract>


</text>

<text id="W95-0109">
<title><entity id="W95-0109.1">Automatic</entity> <entity id="W95-0109.2">Construction</entity> Of A <entity id="W95-0109.3">Chinese</entity> <entity id="W95-0109.4">Electronic Dictionary</entity></title>
<abstract>
    In this <entity id="W95-0109.5">paper</entity> , an unsupervised <entity id="W95-0109.6">approach</entity> for <entity id="W95-0109.7">constructing</entity> a <entity id="W95-0109.8">large-scale</entity> <entity id="W95-0109.9">Chinese</entity> <entity id="W95-0109.10">electronic dictionary</entity> is <entity id="W95-0109.11">surveyed</entity> . The <entity id="W95-0109.12">main</entity> <entity id="W95-0109.13">purpose</entity> is to enable cheap and quick <entity id="W95-0109.14">acquisition</entity> of a <entity id="W95-0109.15">large-scale</entity> <entity id="W95-0109.16">dictionary</entity> from a large untagged <entity id="W95-0109.17">text</entity> <entity id="W95-0109.18">corpus</entity> with the aid of the <entity id="W95-0109.19">information</entity> in a small <entity id="W95-0109.20">tagged</entity> <entity id="W95-0109.21">seed</entity> <entity id="W95-0109.22">corpus</entity> . The <entity id="W95-0109.23">basic</entity> <entity id="W95-0109.24">model</entity> is <entity id="W95-0109.25">based</entity> on a Viterbi reestimation <entity id="W95-0109.26">technique</entity> . During the <entity id="W95-0109.27">dictionary</entity> <entity id="W95-0109.28">construction</entity> <entity id="W95-0109.29">process</entity> , it tries to optimize the <entity id="W95-0109.30">automatic</entity> segmentation and <entity id="W95-0109.31">tagging</entity> <entity id="W95-0109.32">process</entity> by repeatedly refining the set of <entity id="W95-0109.33">parameters</entity> of the underlying <entity id="W95-0109.34">language <entity id="W95-0109.35">model</entity></entity> . The refined <entity id="W95-0109.36">parameters</entity> are then used to further get a better <entity id="W95-0109.37">tagging</entity> <entity id="W95-0109.38">result</entity> . In <entity id="W95-0109.39">addition</entity> , a <entity id="W95-0109.40">two-class</entity> <entity id="W95-0109.41">classifier</entity> , which is capable of classifying an <entity id="W95-0109.42">n-gram</entity> either as a <entity id="W95-0109.43">word</entity> or a <entity id="W95-0109.44">non-word</entity> , is used in <entity id="W95-0109.45">combination</entity> with the Viterbi <entity id="W95-0109.46">training</entity> <entity id="W95-0109.47">module</entity> to <entity id="W95-0109.48">improve</entity> the <entity id="W95-0109.49">system</entity> <entity id="W95-0109.50">performance</entity> . Two different <entity id="W95-0109.51">system</entity> <entity id="W95-0109.52">configurations</entity> had been <entity id="W95-0109.53">developed</entity> to <entity id="W95-0109.54">construct</entity> the <entity id="W95-0109.55">dictionary</entity> . The <entity id="W95-0109.56">configurations</entity> <entity id="W95-0109.57">include</entity> (1) a Viterbi <entity id="W95-0109.58">word</entity> <entity id="W95-0109.59">identification</entity> <entity id="W95-0109.60">module</entity> followed by a Viterbi <entity id="W95-0109.61">POS tagging</entity> <entity id="W95-0109.62">module</entity> and (2) a <entity id="W95-0109.63">two-class</entity> <entity id="W95-0109.64">classification</entity> <entity id="W95-0109.65">module</entity> as the postfilter for the above Viterbi <entity id="W95-0109.66">word</entity> <entity id="W95-0109.67">identification</entity> <entity id="W95-0109.68">module</entity> . With a <entity id="W95-0109.69">seed</entity> of 1,000 <entity id="W95-0109.70">sentences</entity> and an untagged <entity id="W95-0109.71">corpus</entity> of 311,591 <entity id="W95-0109.72">sentences</entity> , the <entity id="W95-0109.73">performance</entity> for bigram <entity id="W95-0109.74">word</entity> <entity id="W95-0109.75">identification</entity> is 56.88% in <entity id="W95-0109.76">precision</entity> and 77.37% in <entity id="W95-0109.77">recall</entity> when the <entity id="W95-0109.78">two-class </entity><entity id="W95-0109.79">classifier</entity> is <entity id="W95-0109.80">applied</entity> to the <entity id="W95-0109.81">word</entity> <entity id="W95-0109.82">list</entity> suggested by the Viterbi <entity id="W95-0109.83">word</entity> <entity id="W95-0109.84">identification</entity> <entity id="W95-0109.85">module</entity> . The Viterbi <entity id="W95-0109.86">part of <entity id="W95-0109.87">speech</entity> <entity id="W95-0109.88">tag</entity></entity> reestimation stage gives the <entity id="W95-0109.89">figures</entity> of 71.16% and 71.81% weighted <entity id="W95-0109.90">precision</entity> <entity id="W95-0109.91">rates</entity> and 73.42% and 73.83% weighted <entity id="W95-0109.92">recall</entity> <entity id="W95-0109.93">rates</entity> for the 2 different <entity id="W95-0109.94">configurations</entity> when using a <entity id="W95-0109.95">seed</entity> <entity id="W95-0109.96">corpus</entity> of 9676 <entity id="W95-0109.97">sentences</entity> .
</abstract>


</text>

<text id="W96-0201">
<title>
A Geometric <entity id="W96-0201.1">Approach</entity> To <entity id="W96-0201.2">Mapping</entity> Bitext <entity id="W96-0201.3">Correspondence</entity></title>
<abstract>
The <entity id="W96-0201.4">first step</entity> in most <entity id="W96-0201.5">corpus-based</entity> multilingual NLP work is to <entity id="W96-0201.6">construct</entity> a <entity id="W96-0201.7">detailed</entity> <entity id="W96-0201.8">map</entity> of the <entity id="W96-0201.9">correspondence</entity> between a <entity id="W96-0201.10">text</entity> and its <entity id="W96-0201.11">translation</entity> . Several <entity id="W96-0201.12">automatic</entity> <entity id="W96-0201.13">methods</entity> for this <entity id="W96-0201.14">task</entity> have been <entity id="W96-0201.15">proposed</entity> in recent years. Yet even the best of these <entity id="W96-0201.16">methods</entity> can err by several typeset <entity id="W96-0201.17">pages</entity> . The <entity id="W96-0201.18">Smooth</entity> Injective <entity id="W96-0201.19">Map</entity> Recognizer (SIMR) is a new bitext <entity id="W96-0201.20">mapping</entity> <entity id="W96-0201.21">algorithm</entity> . SIMR's <entity id="W96-0201.22">errors</entity> are smaller than those of the previous front-runner by more than a <entity id="W96-0201.23">factor</entity> of 4. Its <entity id="W96-0201.24">robustness</entity> has enabled new <entity id="W96-0201.25">commercial-quality</entity> <entity id="W96-0201.26">applications</entity> . The greedy <entity id="W96-0201.27">nature</entity> of the <entity id="W96-0201.28">algorithm</entity> makes it independent of <entity id="W96-0201.29">memory</entity> <entity id="W96-0201.30">resources</entity> . Unlike other bitext <entity id="W96-0201.31">mapping</entity> <entity id="W96-0201.32">algorithms</entity> , SIMR allows <entity id="W96-0201.33">crossing</entity> <entity id="W96-0201.34">correspondences</entity> to account for <entity id="W96-0201.35">word</entity> <entity id="W96-0201.36">order</entity> <entity id="W96-0201.37">differences</entity> . Its <entity id="W96-0201.38">output</entity> can be converted quickly and easily into a <entity id="W96-0201.39">sentence</entity> <entity id="W96-0201.40">alignment</entity> . SIMR's <entity id="W96-0201.41">output</entity> has been used to align more than 200 megabytes of the Canadian Hansards for publication by the Linguistic <entity id="W96-0201.42">Data</entity> Consortium.
</abstract>


</text>

<text id="W96-0210">
<title>
The Measure Of A <entity id="W96-0210.1">Model</entity></title>
<abstract>
This <entity id="W96-0210.2">paper</entity> describes measures for <entity id="W96-0210.3">evaluating</entity> the three determinants of how well a probabilistic <entity id="W96-0210.4">classifier</entity> <entity id="W96-0210.5">performs</entity> on a given <entity id="W96-0210.6">test set</entity> . These determinants are the <entity id="W96-0210.7">appropriateness</entity> , for the <entity id="W96-0210.8">test set</entity> , of the <entity id="W96-0210.9">results</entity> of (1) <entity id="W96-0210.10">feature selection</entity> , (2) <entity id="W96-0210.11">formulation</entity> of the parametric <entity id="W96-0210.12">form</entity> of the <entity id="W96-0210.13">model</entity> , and (3) <entity id="W96-0210.14">parameter estimation</entity> . These are <entity id="W96-0210.15">part</entity> of any <entity id="W96-0210.16">model</entity> <entity id="W96-0210.17">formulation</entity> <entity id="W96-0210.18">procedure</entity> , even if not broken out as separate <entity id="W96-0210.19">steps</entity> , so the tradeoffs explored in this <entity id="W96-0210.20">paper</entity> are relevant to a wide <entity id="W96-0210.21">variety</entity> of <entity id="W96-0210.22">methods</entity> . The measures are demonstrated in a large <entity id="W96-0210.23">experiment</entity> , in which they are used to analyze the <entity id="W96-0210.24">results</entity> of roughly 300 <entity id="W96-0210.25">classifiers</entity> that <entity id="W96-0210.26">perform</entity> <entity id="W96-0210.27"><entity id="W96-0210.28">word-sense</entity> <entity id="W96-0210.29">disambiguation</entity></entity> .
</abstract>


</text>

<text id="W96-0404">
<title>
Approximate <entity id="W96-0404.1">Generation</entity> From Non-Hierarchical Representations
</title>
<abstract>
This <entity id="W96-0404.2">paper</entity> presents a <entity id="W96-0404.3">technique</entity> for <entity id="W96-0404.4">sentence generation</entity> . We argue that the <entity id="W96-0404.5">input</entity> to <entity id="W96-0404.6">generators</entity> should have a non-hierarchical <entity id="W96-0404.7">nature</entity> . This allows us to investigate a more general <entity id="W96-0404.8">version</entity> of the <entity id="W96-0404.9">sentence generation</entity> <entity id="W96-0404.10">problem</entity> where one is not pre-committed to a <entity id="W96-0404.11">choice</entity> of the syntactically prominent elements in the initial <entity id="W96-0404.12">semantics</entity> . We also consider that a <entity id="W96-0404.13">generator</entity> can happen to convey more (or less) <entity id="W96-0404.14">information</entity> than is originally specified in its <entity id="W96-0404.15">semantic</entity> <entity id="W96-0404.16">input</entity> . In <entity id="W96-0404.17">order</entity> to constrain this approximate matching of the <entity id="W96-0404.18">input</entity> we impose additional <entity id="W96-0404.19">restrictions</entity> on the <entity id="W96-0404.20">semantics</entity> of the <entity id="W96-0404.21">generated</entity> <entity id="W96-0404.22">sentence</entity> . Our <entity id="W96-0404.23">technique</entity> <entity id="W96-0404.24">provides</entity> <entity id="W96-0404.25">flexibility</entity> to address <entity id="W96-0404.26">cases</entity> where the entire <entity id="W96-0404.27">input</entity> cannot be precisely expressed in a single <entity id="W96-0404.28">sentence</entity> . Thus the <entity id="W96-0404.29">generator</entity> does not rely on the strategic <entity id="W96-0404.30">component</entity> having <entity id="W96-0404.31">linguistic knowledge</entity> . We show clearly how the <entity id="W96-0404.32">semantic <entity id="W96-0404.33">structure</entity></entity> is declaratively related to linguistically motivated <entity id="W96-0404.34">syntactic <entity id="W96-0404.35">representation</entity></entity> .
</abstract>


</text>

<text id="W96-0405">
<title><entity id="W96-0405.1">Input</entity> <entity id="W96-0405.2">Specification</entity> In The WAG <entity id="W96-0405.3">Sentence Generation</entity> <entity id="W96-0405.4">System</entity></title>
<abstract>
This <entity id="W96-0405.5">paper</entity> describes the <entity id="W96-0405.6">input</entity> <entity id="W96-0405.7">specification</entity> <entity id="W96-0405.8">language</entity> of the WAG <entity id="W96-0405.9">Sentence Generation</entity> <entity id="W96-0405.10">system</entity> . The <entity id="W96-0405.11">input</entity> is described in <entity id="W96-0405.12">terms</entity> of Halliday 's (1978) three meaning <entity id="W96-0405.13">components</entity> , ideational meaning (the propositional <entity id="W96-0405.14">content</entity> to be expressed), interactional meaning (what the speaker intends the <entity id="W96-0405.15">listener</entity> to do in making the <entity id="W96-0405.16">utterance</entity> ), and textual meaning (how the <entity id="W96-0405.17">content</entity> is <entity id="W96-0405.18">structured</entity> as a <entity id="W96-0405.19">message</entity> , in <entity id="W96-0405.20">terms</entity> of theme, <entity id="W96-0405.21">reference</entity> , etc.).
</abstract>


</text>

<text id="W97-0406">
<title><entity id="W97-0406.1">Dealing</entity> With Multilinguality In A <entity id="W97-0406.2">Spoken Language</entity> <entity id="W97-0406.3">Query</entity> <entity id="W97-0406.4">Translator</entity></title> 
<abstract><entity id="W97-0406.5">Robustness</entity> is an important <entity id="W97-0406.6">issue</entity> for multilingual <entity id="W97-0406.7">speech </entity><entity id="W97-0406.8">interfaces</entity> for spoken <entity id="W97-0406.9">language</entity> <entity id="W97-0406.10">translation <entity id="W97-0406.11">systems</entity></entity> . We have <entity id="W97-0406.12">studied</entity> three <entity id="W97-0406.13">aspects</entity> of <entity id="W97-0406.14">robustness</entity> in such a <entity id="W97-0406.15">system</entity> : accent <entity id="W97-0406.16">differences</entity> , mixed <entity id="W97-0406.17">language</entity> <entity id="W97-0406.18">input</entity> , and the use of <entity id="W97-0406.19">common</entity> <entity id="W97-0406.20">feature sets</entity> for HMM-based <entity id="W97-0406.21">speech</entity> recognizers for <entity id="W97-0406.22">English</entity> and Cantonese. The <entity id="W97-0406.23">results</entity> of our preliminary <entity id="W97-0406.24">experiments</entity> show that accent <entity id="W97-0406.25">differences</entity> cause recognizer <entity id="W97-0406.26">performance</entity> to degrade . A rather surprising <entity id="W97-0406.27">finding</entity> is that for mixed <entity id="W97-0406.28">language</entity> <entity id="W97-0406.29">input</entity> , a straight forward <entity id="W97-0406.30">implementation</entity> of a mixed <entity id="W97-0406.31">language <entity id="W97-0406.32">model-based</entity></entity> <entity id="W97-0406.33">speech</entity> recognizer <entity id="W97-0406.34">performs</entity> less well than the concatenation of pure <entity id="W97-0406.35">language</entity> recognizers. Our <entity id="W97-0406.36">experimental</entity> <entity id="W97-0406.37">results</entity> also show that a <entity id="W97-0406.38">common</entity> <entity id="W97-0406.39">feature set</entity> , <entity id="W97-0406.40">parameter</entity> set, and <entity id="W97-0406.41">common</entity> <entity id="W97-0406.42">algorithm</entity> lead to different <entity id="W97-0406.43">performance</entity> <entity id="W97-0406.44">output</entity> for Cantonese and <entity id="W97-0406.45">English</entity> <entity id="W97-0406.46">speech recognition</entity> <entity id="W97-0406.47">modules</entity> .
</abstract>


</text>

<text id="W97-0408">
<title><entity id="W97-0408.1">English-</entity> To-Mandarin <entity id="W97-0408.2">Speech Translation</entity> With Head Transducers
</title>
<abstract>
We describe the head transducer <entity id="W97-0408.3">model</entity> used in an <entity id="W97-0408.4">experimental</entity> <entity id="W97-0408.5">English-to-</entity> Mandarin speech <entity id="W97-0408.6">translation <entity id="W97-0408.7">system</entity></entity> . Head <entity id="W97-0408.8">transduction</entity> is a <entity id="W97-0408.9">translation</entity> <entity id="W97-0408.10">method</entity> in which weighted <entity id="W97-0408.11">finite state transducers</entity> are associated with <entity id="W97-0408.12">source-target word</entity> <entity id="W97-0408.13">pairs</entity> . The <entity id="W97-0408.14">method</entity> is suitable for <entity id="W97-0408.15">speech <entity id="W97-0408.16">translation</entity></entity> because it allows efficient bottom up <entity id="W97-0408.17">processing</entity> . The head transducers in the <entity id="W97-0408.18">experimental</entity> <entity id="W97-0408.19">system</entity> have a wider range of <entity id="W97-0408.20">output</entity> positions than <entity id="W97-0408.21">input</entity> positions. This asymmetry is motivated by a tradeoff between <entity id="W97-0408.22">model</entity> <entity id="W97-0408.23">complexity</entity> and <entity id="W97-0408.24">search</entity> <entity id="W97-0408.25">efficiency</entity> .
</abstract>


</text>

<text id="W97-0615">
<title>
Filtering Errors And Repairing Linguistic Anomalies For <entity id="W97-0615.1">Spoken Dialogue</entity> <entity id="W97-0615.2">Systems</entity></title>
<abstract>
Our work addresses the <entity id="W97-0615.3">integration</entity> of <entity id="W97-0615.4">speech recognition</entity> and <entity id="W97-0615.5">language <entity id="W97-0615.6">processing</entity></entity> for whole spoken <entity id="W97-0615.7">dialogue <entity id="W97-0615.8">systems</entity></entity> .To filter ill-recognized <entity id="W97-0615.9">words</entity> , we <entity id="W97-0615.10">design</entity> an on-line <entity id="W97-0615.11">computing</entity> of <entity id="W97-0615.12">word</entity> <entity id="W97-0615.13">confidence</entity> scores <entity id="W97-0615.14">based</entity> on the recognizer <entity id="W97-0615.15">output</entity> <entity id="W97-0615.16">hypothesis</entity> . To infer as much <entity id="W97-0615.17">information</entity> as possible from the retained <entity id="W97-0615.18">sequence</entity> of <entity id="W97-0615.19">words</entity> , we <entity id="W97-0615.20">propose</entity> a bottom-up <entity id="W97-0615.21">syntactico-semantic</entity> <entity id="W97-0615.22">robust</entity> <entity id="W97-0615.23">parsing</entity> relying on a lexi-calized <entity id="W97-0615.24">tree</entity> grammar and on integrated <entity id="W97-0615.25">repairing</entity> <entity id="W97-0615.26">strategies</entity> .
</abstract>


</text>

<text id="W97-0618">
<title>
A Programmable Multi-Blackboard <entity id="W97-0618.1">Architecture</entity> For <entity id="W97-0618.2">Dialogue</entity> <entity id="W97-0618.3">Processing</entity> <entity id="W97-0618.4">Systems</entity></title>
<abstract>
In <entity id="W97-0618.5">current</entity> <entity id="W97-0618.6">Natural Language Processing</entity> <entity id="W97-0618.7">Systems</entity> , different <entity id="W97-0618.8">components</entity> for different <entity id="W97-0618.9">processing</entity> <entity id="W97-0618.10">tasks</entity> and <entity id="W97-0618.11">input</entity> / <entity id="W97-0618.12">output</entity> <entity id="W97-0618.13">modalities</entity> have to be integrated. Once integrated, the <entity id="W97-0618.14">interactions</entity> between the <entity id="W97-0618.15">components</entity> have to be specified. Interactions in <entity id="W97-0618.16">dialogue systems</entity> can be <entity id="W97-0618.17">complex</entity> <entity id="W97-0618.18">due</entity> in <entity id="W97-0618.19">part</entity> to the many states the <entity id="W97-0618.20">system</entity> can be in. When <entity id="W97-0618.21">porting</entity> the <entity id="W97-0618.22">system</entity> to another <entity id="W97-0618.23">domain</entity> , <entity id="W97-0618.24">parts</entity> of the <entity id="W97-0618.25">integration</entity> <entity id="W97-0618.26">process</entity> have to be repeated. To overcome these <entity id="W97-0618.27">difficulties</entity> , we <entity id="W97-0618.28">propose</entity> a multi-blackboard <entity id="W97-0618.29">architecture</entity> that is controlled by a set of <entity id="W97-0618.30">expert-system</entity> like <entity id="W97-0618.31">rules</entity> . These <entity id="W97-0618.32">rules</entity> may contain <entity id="W97-0618.33">typed</entity> <entity id="W97-0618.34">variables</entity> . Variables can be substituted by <entity id="W97-0618.35">representations</entity> with an appropriate <entity id="W97-0618.36">type</entity> stored in the blackboards. Furthermore, the <entity id="W97-0618.37">representations</entity> in the blackboards allow to represent <entity id="W97-0618.38">partial</entity> <entity id="W97-0618.39">information</entity> and to leave <entity id="W97-0618.40">disjunctions</entity> unresolved. Moreover, the conditions of the <entity id="W97-0618.41">rule</entity> may depend on the <entity id="W97-0618.42">specificity</entity> of the <entity id="W97-0618.43">representations</entity> with which the <entity id="W97-0618.44">variables</entity> are instantiated. For this <entity id="W97-0618.45">reason</entity> , the <entity id="W97-0618.46">interaction</entity> is <entity id="W97-0618.47">information-driven</entity> . The described <entity id="W97-0618.48">system</entity> has been <entity id="W97-0618.49">implemented</entity> and has been integrated with the <entity id="W97-0618.50">speech</entity> recognizer JANUS.
</abstract>


</text>

<text id="J82-1001">
<title><entity id="J82-1001.1">Phrase Structure</entity> Trees Bear More Fruit Than You Would Have Thought
</title>
<abstract>
In this <entity id="J82-1001.2">paper</entity> we will present several <entity id="J82-1001.3">results</entity> <entity id="J82-1001.4">concerning</entity> <entity id="J82-1001.5">phrase structure trees</entity> . These <entity id="J82-1001.6">results</entity> show that <entity id="J82-1001.7">phrase structure trees</entity> , when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local <entity id="J82-1001.8">constraints</entity> on <entity id="J82-1001.9">structural</entity> <entity id="J82-1001.10">descriptions</entity> and an intuitive proof of a <entity id="J82-1001.11">theorem</entity> about local <entity id="J82-1001.12">constraints</entity> . We have compared the local <entity id="J82-1001.13">constraints</entity> <entity id="J82-1001.14">approach</entity> to some <entity id="J82-1001.15">aspects</entity> of Gazdar 's <entity id="J82-1001.16">framework</entity> and that of Peters and Ritchie and of Karttunen . We have also presented some <entity id="J82-1001.17">results</entity> on skeletons ( <entity id="J82-1001.18">phrase structure trees</entity> without labels) which show that <entity id="J82-1001.19">phrase <entity id="J82-1001.20">structure</entity> <entity id="J82-1001.21">trees</entity></entity> , even when deprived of the labels, retain in a certain <entity id="J82-1001.22">sense</entity> all the <entity id="J82-1001.23">structural</entity> <entity id="J82-1001.24">information</entity> . This <entity id="J82-1001.25">result</entity> has <entity id="J82-1001.26">implications</entity> for grammatical <entity id="J82-1001.27">inference</entity> <entity id="J82-1001.28">procedures</entity> .
</abstract>


</text>

<text id="J82-2002">
<title><entity id="J82-2002.1">Natural-</entity> <entity id="J82-2002.2">Language</entity> <entity id="J82-2002.3">Interface</entity></title>
<abstract>
A major <entity id="J82-2002.4">problem</entity> faced by would-be <entity id="J82-2002.5">users</entity> of <entity id="J82-2002.6">computer</entity> <entity id="J82-2002.7">systems</entity> is that <entity id="J82-2002.8">computers</entity> generally make use of <entity id="J82-2002.9">special-purpose</entity> <entity id="J82-2002.10">languages</entity> familiar only to those <entity id="J82-2002.11">trained</entity> in <entity id="J82-2002.12">computer science</entity> . For a large <entity id="J82-2002.13">number</entity> of <entity id="J82-2002.14">applications</entity> <entity id="J82-2002.15">requiring</entity> <entity id="J82-2002.16">interaction</entity> between humans and <entity id="J82-2002.17">computer</entity> <entity id="J82-2002.18">systems</entity> , it would be highly desirable for <entity id="J82-2002.19">machines</entity> to converse in <entity id="J82-2002.20">English</entity> or other <entity id="J82-2002.21">natural languages</entity> familiar to their human <entity id="J82-2002.22">users</entity> . Over the last decade, in <entity id="J82-2002.23">laboratories</entity> around the world, several <entity id="J82-2002.24">computer</entity> <entity id="J82-2002.25">systems</entity> have been <entity id="J82-2002.26">developed</entity> that <entity id="J82-2002.27">support</entity> at least elementary <entity id="J82-2002.28">levels</entity> of <entity id="J82-2002.29">natural-language</entity> <entity id="J82-2002.30">interaction</entity> . Among these are such <entity id="J82-2002.31">systems</entity> as those described in the several <entity id="J82-2002.32">references</entity> at the end of this <entity id="J82-2002.33">paper</entity> .
</abstract>


</text>

<text id="P98-1099">
<title><entity id="P98-1099.1">Combining</entity> Multiple, Large- <entity id="P98-1099.2">Scale</entity> Resources in a Reusable <entity id="P98-1099.3">Lexicon</entity> for <entity id="P98-1099.4">Natural <entity id="P98-1099.5">Language Generation</entity></entity></title>
<abstract>
A <entity id="P98-1099.6">lexicon</entity> is an essential <entity id="P98-1099.7">component</entity> in a <entity id="P98-1099.8">generation <entity id="P98-1099.9">system</entity></entity> but few <entity id="P98-1099.10">efforts</entity> have been made to build a rich, <entity id="P98-1099.11">large-scale</entity> <entity id="P98-1099.12">lexicon</entity> and make it reusable for different <entity id="P98-1099.13">generation</entity> <entity id="P98-1099.14">applications</entity> . In this <entity id="P98-1099.15">paper</entity> , we describe our work to build such a <entity id="P98-1099.16">lexicon</entity> by combining multiple, heterogeneous <entity id="P98-1099.17">linguistic resources</entity> which have been <entity id="P98-1099.18">developed</entity> for other <entity id="P98-1099.19">purposes</entity> . Novel <entity id="P98-1099.20">transformation</entity> and <entity id="P98-1099.21">integration</entity> of <entity id="P98-1099.22">resources</entity> is <entity id="P98-1099.23">required</entity> to reuse them for <entity id="P98-1099.24">generation</entity> . We also <entity id="P98-1099.25">applied</entity> the <entity id="P98-1099.26">lexicon</entity> to the <entity id="P98-1099.27">lexical <entity id="P98-1099.28">choice</entity></entity> and <entity id="P98-1099.29">realization</entity> <entity id="P98-1099.30">component</entity> of a practical <entity id="P98-1099.31">generation</entity> <entity id="P98-1099.32">application</entity> by using a <entity id="P98-1099.33">multi-level</entity> <entity id="P98-1099.34">feedback</entity> <entity id="P98-1099.35">architecture</entity> . The <entity id="P98-1099.36">integration</entity> of the <entity id="P98-1099.37">lexicon</entity> and the <entity id="P98-1099.38">architecture</entity> is able to effectively <entity id="P98-1099.39">improve</entity> the <entity id="P98-1099.40">system</entity> paraphrasing power, minimize the chance of grammatical <entity id="P98-1099.41">errors</entity> , and simplify the <entity id="P98-1099.42">development</entity> <entity id="P98-1099.43">process</entity> substantially.
</abstract>


</text>

<text id="P98-1107">
<title>
A <entity id="P98-1107.1">Method</entity> for Correcting Errors in <entity id="P98-1107.2">Speech <entity id="P98-1107.3">Recognition</entity></entity> using the <entity id="P98-1107.4">Statistical</entity> <entity id="P98-1107.5">Features</entity> of Character <entity id="P98-1107.6">Co-occurrence</entity></title>
<abstract>
It is important to correct the <entity id="P98-1107.7">errors</entity> in the <entity id="P98-1107.8">results</entity> of <entity id="P98-1107.9">speech recognition</entity> to <entity id="P98-1107.10">increase</entity> the <entity id="P98-1107.11">performance</entity> of a speech <entity id="P98-1107.12">translation system</entity> . This <entity id="P98-1107.13">paper</entity> <entity id="P98-1107.14">proposes</entity> a <entity id="P98-1107.15">method</entity> for correcting <entity id="P98-1107.16">errors</entity> using the <entity id="P98-1107.17">statistical</entity> <entity id="P98-1107.18">features</entity> of character <entity id="P98-1107.19">co-occurrence</entity> , and <entity id="P98-1107.20">evaluates</entity> the <entity id="P98-1107.21">method</entity> . The <entity id="P98-1107.22">proposed</entity> <entity id="P98-1107.23">method</entity> comprises two successive correcting <entity id="P98-1107.24">processes</entity> . The first <entity id="P98-1107.25">process</entity> uses <entity id="P98-1107.26">pairs</entity> of <entity id="P98-1107.27">strings</entity> : the first <entity id="P98-1107.28">string</entity> is an erroneous substring of the <entity id="P98-1107.29">utterance</entity> predicted by <entity id="P98-1107.30">speech <entity id="P98-1107.31">recognition</entity></entity> , the second <entity id="P98-1107.32">string</entity> is the corresponding <entity id="P98-1107.33">section</entity> of the actual <entity id="P98-1107.34">utterance</entity> . Errors are detected and corrected according to the <entity id="P98-1107.35">database</entity> learned from erroneous-correct <entity id="P98-1107.36">utterance</entity> <entity id="P98-1107.37">pairs</entity> . The remaining <entity id="P98-1107.38">errors</entity> are passed to the posterior <entity id="P98-1107.39">process</entity> which uses a <entity id="P98-1107.40">string</entity> in the <entity id="P98-1107.41">corpus</entity> that is similar to the <entity id="P98-1107.42">string</entity> <entity id="P98-1107.43">including</entity> <entity id="P98-1107.44">recognition</entity> <entity id="P98-1107.45">errors</entity> . The <entity id="P98-1107.46">results</entity> of our <entity id="P98-1107.47">evaluation</entity> show that the use of our <entity id="P98-1107.48">proposed</entity> <entity id="P98-1107.49">method</entity> as a <entity id="P98-1107.50">post-processor</entity> for <entity id="P98-1107.51">speech <entity id="P98-1107.52">recognition</entity></entity> is likely to make a significant <entity id="P98-1107.53">contribution</entity> to the <entity id="P98-1107.54">performance</entity> of speech <entity id="P98-1107.55">translation systems</entity> . <entity id="P98-1107.56">method</entity> also obtains reliably recognized <entity id="P98-1107.57">partial</entity> <entity id="P98-1107.58">segments</entity> of an <entity id="P98-1107.59">utterance</entity> by cooperatively using both grammatical and <entity id="P98-1107.60">n-gram</entity> <entity id="P98-1107.61">based</entity> <entity id="P98-1107.62">statistical</entity> <entity id="P98-1107.63">language</entity> <entity id="P98-1107.64">constraints</entity> , and uses a <entity id="P98-1107.65">robust</entity> <entity id="P98-1107.66">parsing</entity> <entity id="P98-1107.67">technique</entity> to <entity id="P98-1107.68">apply</entity> the grammatical <entity id="P98-1107.69">constraints</entity> described by <entity id="P98-1107.70">context-free</entity> grammar ( Tsukada</abstract>


</text>

<text id="P98-2134">
<title>
Bitext Correspondences through Rich Mark-up</title>
<abstract>
Rich mark-up can considerably <entity id="P98-2134.1">benefit</entity> the <entity id="P98-2134.2">process</entity> of establishing bitext <entity id="P98-2134.3">correspondences</entity> , that is, the <entity id="P98-2134.4">task</entity> of <entity id="P98-2134.5">providing</entity> correct <entity id="P98-2134.6">identification</entity> and <entity id="P98-2134.7">alignment</entity> <entity id="P98-2134.8">methods</entity> for <entity id="P98-2134.9">text</entity> <entity id="P98-2134.10">segments</entity> that are <entity id="P98-2134.11">translation</entity> <entity id="P98-2134.12">equivalences</entity> of each other in a <entity id="P98-2134.13">parallel corpus</entity> . We present a <entity id="P98-2134.14">sentence</entity> <entity id="P98-2134.15">alignment</entity> <entity id="P98-2134.16">algorithm</entity> that, by taking <entity id="P98-2134.17">advantage</entity> of previously annotated <entity id="P98-2134.18">texts</entity> , obtains <entity id="P98-2134.19">accuracy</entity> <entity id="P98-2134.20">rates</entity> close to 100%. The <entity id="P98-2134.21">algorithm</entity> <entity id="P98-2134.22">evaluates</entity> the <entity id="P98-2134.23">similarity</entity> of the linguistic and extra-linguistic mark-up in both <entity id="P98-2134.24">sides</entity> of a bitext. Given that annotations are neutral with <entity id="P98-2134.25">respect</entity> to typological, grammatical and orthographical <entity id="P98-2134.26">differences</entity> between <entity id="P98-2134.27">languages</entity> , rich mark-up becomes an <entity id="P98-2134.28">optimal</entity> foundation to <entity id="P98-2134.29">support</entity> bitext <entity id="P98-2134.30">correspondences</entity> . The <entity id="P98-2134.31">main</entity> originality of this <entity id="P98-2134.32">approach</entity> is that it makes maximal use of annotations, which is a very sensible and efficient <entity id="P98-2134.33">method</entity> for the <entity id="P98-2134.34">exploitation</entity> of <entity id="P98-2134.35">parallel corpora</entity> when annotations exist.
</abstract>


</text>

<text id="P98-2154">
<title>
Translating a <entity id="P98-2154.1">Unification</entity> Grammar with Disjunctions into Logical Constraints
</title>
<abstract>
This <entity id="P98-2154.2">paper</entity> <entity id="P98-2154.3">proposes</entity> a <entity id="P98-2154.4">method</entity> for <entity id="P98-2154.5">generating</entity> a <entity id="P98-2154.6">logical-constraint-based</entity> internal <entity id="P98-2154.7">representation</entity> from a <entity id="P98-2154.8">unification</entity> grammar <entity id="P98-2154.9">formalism</entity> with disjunctive <entity id="P98-2154.10">information</entity> . <entity id="P98-2154.11">Unification</entity> grammar <entity id="P98-2154.12">formalisms</entity> <entity id="P98-2154.13">based</entity> on <entity id="P98-2154.14">path</entity> <entity id="P98-2154.15">equations</entity> and <entity id="P98-2154.16">lists</entity> of <entity id="P98-2154.17">pairs</entity> of labels and values are better than those <entity id="P98-2154.18">based</entity> on <entity id="P98-2154.19">first-order</entity> <entity id="P98-2154.20">terms</entity> in that the former is easier to describe and to understand. <entity id="P98-2154.21">Parsing</entity> with <entity id="P98-2154.22">term-based</entity> internal <entity id="P98-2154.23">representations</entity> is more efficient than <entity id="P98-2154.24">parsing</entity> with graph-based <entity id="P98-2154.25">representations</entity> . Therefore, it is effective to <entity id="P98-2154.26">translate</entity> <entity id="P98-2154.27">unification</entity> grammar <entity id="P98-2154.28">formalism</entity> <entity id="P98-2154.29">based</entity> on <entity id="P98-2154.30">path</entity> <entity id="P98-2154.31">equations</entity> and <entity id="P98-2154.32">lists</entity> of <entity id="P98-2154.33">pairs</entity> of labels and values into a <entity id="P98-2154.34">term-based</entity> internal <entity id="P98-2154.35">representation</entity> . Previous <entity id="P98-2154.36">translation</entity> <entity id="P98-2154.37">methods</entity> cannot <entity id="P98-2154.38">deal</entity> with disjunctive <entity id="P98-2154.39">feature</entity> <entity id="P98-2154.40">descriptions</entity>, which reduce <entity id="P98-2154.41">redundancies</entity> in the grammar and make <entity id="P98-2154.42">parsing</entity> efficient. Since the <entity id="P98-2154.43">proposed</entity> <entity id="P98-2154.44">method</entity> <entity id="P98-2154.45">translates</entity> a <entity id="P98-2154.46">formalism</entity> without expanding <entity id="P98-2154.47">disjunctions</entity> , <entity id="P98-2154.48">parsing</entity> with the <entity id="P98-2154.49">resulting</entity> <entity id="P98-2154.50">representation</entity> is efficient.
</abstract>


</text>

<text id="C08-1013">
<title>
ParaMetric: An <entity id="C08-1013.1">Automatic Evaluation</entity> <entity id="C08-1013.2">Metric</entity> for Paraphrasing
</title>
<abstract>
We present ParaMetric, an <entity id="C08-1013.3">automatic evaluation</entity> <entity id="C08-1013.4">metric</entity> for <entity id="C08-1013.5">data-driven</entity> <entity id="C08-1013.6">approaches</entity> to paraphrasing. ParaMetric <entity id="C08-1013.7">provides</entity> an <entity id="C08-1013.8">objective</entity> measure of <entity id="C08-1013.9">quality</entity> using a <entity id="C08-1013.10">collection</entity> of multiple <entity id="C08-1013.11">translations</entity> whose paraphrases have been manually annotated. ParaMetric calculates <entity id="C08-1013.12">precision</entity> and <entity id="C08-1013.13">recall</entity> scores by comparing the paraphrases discovered by <entity id="C08-1013.14">automatic</entity> paraphrasing <entity id="C08-1013.15">techniques</entity> against <entity id="C08-1013.16">gold standard</entity> <entity id="C08-1013.17">alignments</entity> of <entity id="C08-1013.18">words</entity> and <entity id="C08-1013.19">phrases</entity> within equivalent <entity id="C08-1013.20">sentences</entity> . We <entity id="C08-1013.21">report</entity> scores for several established paraphrasing <entity id="C08-1013.22">techniques</entity> .
</abstract>


</text>

<text id="C08-1025">
<title><entity id="C08-1025.1">Re-estimation</entity> of <entity id="C08-1025.2">Lexical</entity> Parameters for Treebank PCFGs
</title>
<abstract>
We present <entity id="C08-1025.3">procedures</entity> which pool <entity id="C08-1025.4">lexical <entity id="C08-1025.5">information</entity></entity> estimated from unlabeled <entity id="C08-1025.6">data</entity> via the Inside-Outside <entity id="C08-1025.7">algorithm</entity> , with <entity id="C08-1025.8">lexical information</entity> from a treebank PCFG. The <entity id="C08-1025.9">procedures</entity> produce substantial <entity id="C08-1025.10">improvements</entity> (up to 31.6% <entity id="C08-1025.11">error</entity> <entity id="C08-1025.12">reduction</entity> ) on the <entity id="C08-1025.13">task</entity> of determining subcategorization <entity id="C08-1025.14">frames</entity> of novel <entity id="C08-1025.15">verbs</entity> , <entity id="C08-1025.16">relative</entity> to a <entity id="C08-1025.17">smoothed</entity> <entity id="C08-1025.18">Penn Treebank-trained</entity> PCFG. Even with relatively small <entity id="C08-1025.19">quantities</entity> of unlabeled <entity id="C08-1025.20">training</entity> data, the re-estimated <entity id="C08-1025.21">models</entity> show promising <entity id="C08-1025.22">improvements</entity> in labeled <entity id="C08-1025.23">bracketing</entity> /scores on <entity id="C08-1025.24">Wall Street Journal</entity> <entity id="C08-1025.25">parsing</entity> , and substantial <entity id="C08-1025.26">benefit</entity> in acquiring the subcategorization <entity id="C08-1025.27">preferences</entity> of <entity id="C08-1025.28">low-frequency</entity> <entity id="C08-1025.29">verbs</entity> .
</abstract>


</text>

<text id="C08-1082">
<title><entity id="C08-1082.1">Semantic</entity> <entity id="C08-1082.2">Classification</entity> with Distributional Kernels
</title>
<abstract>
Distributional measures of <entity id="C08-1082.3">lexical</entity> <entity id="C08-1082.4">similarity</entity> and <entity id="C08-1082.5">kernel <entity id="C08-1082.6">methods</entity></entity> for <entity id="C08-1082.7">classification</entity> are well-known <entity id="C08-1082.8">tools</entity> in <entity id="C08-1082.9">Natural Language Processing</entity> . We bring these two <entity id="C08-1082.10">methods</entity> together by introducing distributional kernels
</abstract>


</text>

<text id="C08-1125">
<title><entity id="C08-1125.1">Domain Adaptation</entity> for <entity id="C08-1125.2">Statistical Machine Translation</entity> with <entity id="C08-1125.3">Domain</entity> <entity id="C08-1125.4">Dictionary</entity> and Monolingual <entity id="C08-1125.5">Corpora</entity></title> 
<abstract><entity id="C08-1125.6">Statistical <entity id="C08-1125.7">machine translation systems</entity></entity> are usually <entity id="C08-1125.8">trained</entity> on large <entity id="C08-1125.9">amounts</entity> of bilingual <entity id="C08-1125.10">text</entity> and monolingual <entity id="C08-1125.11">text</entity> . In this <entity id="C08-1125.12">paper</entity> , we <entity id="C08-1125.13">propose</entity> a <entity id="C08-1125.14">method</entity> to <entity id="C08-1125.15">perform</entity> <entity id="C08-1125.16">domain adaptation</entity> for <entity id="C08-1125.17">statistical machine translation</entity> , where <entity id="C08-1125.18">in-domain</entity> bilingual <entity id="C08-1125.19">corpora</entity> do not exist. This <entity id="C08-1125.20">method</entity> first uses <entity id="C08-1125.21">out-of-domain</entity> <entity id="C08-1125.22">corpora</entity> to <entity id="C08-1125.23">train</entity> a <entity id="C08-1125.24">baseline system</entity> and then uses <entity id="C08-1125.25">in-domain</entity> <entity id="C08-1125.26">translation</entity> <entity id="C08-1125.27">dictionaries</entity> and <entity id="C08-1125.28">in-domain</entity> monolingual <entity id="C08-1125.29">corpora</entity> to <entity id="C08-1125.30">improve</entity> the indomain <entity id="C08-1125.31">performance</entity> . We <entity id="C08-1125.32">propose</entity> an <entity id="C08-1125.33">algorithm</entity> to combine these different <entity id="C08-1125.34">resources</entity> in a unified <entity id="C08-1125.35">framework</entity> . <entity id="C08-1125.36">Experimental</entity> <entity id="C08-1125.37">results</entity> indicate that our <entity id="C08-1125.38">method</entity> achieves absolute <entity id="C08-1125.39">improvements</entity> of 8.16 and 3.36 BLEU scores on <entity id="C08-1125.40">Chinese</entity> to <entity id="C08-1125.41">English</entity> <entity id="C08-1125.42">translation</entity> and <entity id="C08-1125.43">English</entity> to French <entity id="C08-1125.44">translation</entity> respectively, as compared with the baselines using only <entity id="C08-1125.45">out-of-domain</entity> <entity id="C08-1125.46">corpora</entity> .
</abstract>


</text>

<text id="C08-1137">
<title><entity id="C08-1137.1">Sentence</entity> <entity id="C08-1137.2">Type</entity> <entity id="C08-1137.3">Based</entity> <entity id="C08-1137.4">Reordering <entity id="C08-1137.5">Model</entity></entity> for <entity id="C08-1137.6">Statistical <entity id="C08-1137.7">Machine Translation</entity></entity></title>
<abstract>
Many reordering <entity id="C08-1137.8">approaches</entity> have been <entity id="C08-1137.9">proposed</entity> for the <entity id="C08-1137.10">statistical machine translation</entity> (SMT) <entity id="C08-1137.11">system</entity> . However, the <entity id="C08-1137.12">information</entity> about the <entity id="C08-1137.13">type</entity> of <entity id="C08-1137.14">source <entity id="C08-1137.15">sentence</entity></entity> is ignored in the previous works. In this <entity id="C08-1137.16">paper</entity> , we <entity id="C08-1137.17">propose</entity> a group of novel <entity id="C08-1137.18">reordering <entity id="C08-1137.19">models</entity></entity> <entity id="C08-1137.20">based</entity> on the <entity id="C08-1137.21">source sentence</entity> <entity id="C08-1137.22">type</entity> for <entity id="C08-1137.23">Chinese-to-</entity> <entity id="C08-1137.24">English</entity> <entity id="C08-1137.25">translation</entity> . In our <entity id="C08-1137.26">approach</entity> , an SVM-based <entity id="C08-1137.27">classifier</entity> is employed to classify the given <entity id="C08-1137.28">Chinese</entity> <entity id="C08-1137.29">sentences</entity> into three <entity id="C08-1137.30">types</entity> : special interrogative <entity id="C08-1137.31">sentences</entity> , other interrogative <entity id="C08-1137.32">sentences</entity> , and <entity id="C08-1137.33">non-question</entity> <entity id="C08-1137.34">sentences</entity> . The different <entity id="C08-1137.35">reordering <entity id="C08-1137.36">models</entity></entity> are <entity id="C08-1137.37">developed</entity> oriented to the different <entity id="C08-1137.38">sentence</entity> <entity id="C08-1137.39">types</entity> . Our <entity id="C08-1137.40">experiments</entity> show that the novel <entity id="C08-1137.41">reordering <entity id="C08-1137.42">models</entity></entity> have obtained an <entity id="C08-1137.43">improvement</entity> of more than 2.65% in BLEU for a <entity id="C08-1137.44">phrase-based</entity> spoken <entity id="C08-1137.45">language</entity> <entity id="C08-1137.46">translation system</entity> .
</abstract>


</text>

<text id="C80-1002">
<title><entity id="C80-1002.1">Automatic</entity> <entity id="C80-1002.2">Processing</entity> Of Written French <entity id="C80-1002.3">Language</entity></title>
<abstract>
An <entity id="C80-1002.4">automatic</entity> <entity id="C80-1002.5">processor</entity> of written French <entity id="C80-1002.6">language</entity> is described. This <entity id="C80-1002.7">processor</entity> uses <entity id="C80-1002.8">syntactic</entity> and <entity id="C80-1002.9">semantic <entity id="C80-1002.10">informations</entity></entity> about <entity id="C80-1002.11">words</entity> in <entity id="C80-1002.12">order</entity> to <entity id="C80-1002.13">construct</entity> a <entity id="C80-1002.14">semantic</entity> net representing the meaning of the <entity id="C80-1002.15">sentences</entity> . The <entity id="C80-1002.16">structure</entity> of the <entity id="C80-1002.17">network</entity> and the <entity id="C80-1002.18">principles</entity> of the <entity id="C80-1002.19">parser</entity> are explained. An <entity id="C80-1002.20">application</entity> to the <entity id="C80-1002.21">processing</entity> of the medical <entity id="C80-1002.22">records</entity> is then discussed.
</abstract>


</text>

<text id="C80-1005">
<title><entity id="C80-1005.1">Computer-</entity> Aided Grammatical <entity id="C80-1005.2">Tagging</entity> Of Spoken <entity id="C80-1005.3">English</entity></title>
<abstract></abstract>



</text>

<text id="C80-1008">
<title>
A <entity id="C80-1008.1">Rule-</entity> <entity id="C80-1008.2">Based</entity> <entity id="C80-1008.3">Approach</entity> To Ill- <entity id="C80-1008.4">Formed</entity> <entity id="C80-1008.5">Input</entity></title>
<abstract>
Though <entity id="C80-1008.6">natural language understanding systems</entity> have <entity id="C80-1008.7">improved</entity> markedly in recent years, they have only begun to consider a major <entity id="C80-1008.8">problem</entity> of truly <entity id="C80-1008.9">natural</entity> <entity id="C80-1008.10">input</entity> : ill-formedness. Quite often <entity id="C80-1008.11">natural language</entity> <entity id="C80-1008.12">input</entity> is ill-formed in the <entity id="C80-1008.13">sense</entity> of being misspelled, ungrammatical, or not entirely meaningful. A <entity id="C80-1008.14">requirement</entity> for any successful <entity id="C80-1008.15">natural language interface</entity> must be that the <entity id="C80-1008.16">system</entity> either intelligently guesses at a <entity id="C80-1008.17">user</entity> 's intent, requests direct <entity id="C80-1008.18">clarification</entity> , or at the very least, accurately identifies the ill-formedness. This <entity id="C80-1008.19">paper</entity> presents a <entity id="C80-1008.20">proposal</entity> for the proper <entity id="C80-1008.21">treatment</entity> of ill-formed <entity id="C80-1008.22">input</entity> . Our conjecture is that ill-formedness should be treated as <entity id="C80-1008.23">rule-based</entity> . Violation of the <entity id="C80-1008.24">rules</entity> of normal <entity id="C80-1008.25">processing</entity> should be used to <entity id="C80-1008.26">signal</entity> ill-formedness. Meta-rules modifying the <entity id="C80-1008.27">rules</entity> of normal <entity id="C80-1008.28">processing</entity> should be used for <entity id="C80-1008.29">error</entity> <entity id="C80-1008.30">identification</entity> and <entity id="C80-1008.31">recovery</entity> . These meta-rules correspond to <entity id="C80-1008.32">types</entity> of <entity id="C80-1008.33">errors</entity> . <entity id="C80-1008.34">Evidence</entity> for this conjecture is presented as well as some open <entity id="C80-1008.35">questions</entity> .
</abstract>


</text>

<text id="C80-1022">
<title>
The <entity id="C80-1022.1">Knowledge <entity id="C80-1022.2">Representation</entity></entity> For A Story <entity id="C80-1022.3">Understanding</entity> And <entity id="C80-1022.4">Simulation</entity> <entity id="C80-1022.5">System</entity></title>
<abstract>
TOYONAKA,  OSAKA 560, JAPAN !!!! MATSUSHITA ELECTRIC  INDUSTRIAL CO.,LTD. KADOMA,  OSAKA 571, JAPAN Abstruet</abstract>


</text>

<text id="L08-1232">
<title><entity id="L08-1232.1">Language</entity> Resources and <entity id="L08-1232.2">Chemical</entity> Informatics
</title>
<abstract>
Chemistry <entity id="L08-1232.3">research</entity> <entity id="L08-1232.4">papers</entity> are a primary <entity id="L08-1232.5">source</entity> of <entity id="L08-1232.6">information</entity> about chemistry, as in any scientific <entity id="L08-1232.7">field</entity> . The <entity id="L08-1232.8">presentation</entity> of the <entity id="L08-1232.9">data</entity> is, predominantly, unstructured <entity id="L08-1232.10">information</entity> , and so not immediately susceptible to <entity id="L08-1232.11">processes</entity> <entity id="L08-1232.12">developed</entity> within <entity id="L08-1232.13">chemical</entity> informatics for carrying out chemistry <entity id="L08-1232.14">research</entity> by <entity id="L08-1232.15">information processing</entity> <entity id="L08-1232.16">techniques</entity> . At one <entity id="L08-1232.17">level</entity> , <entity id="L08-1232.18">extracting</entity> the relevant <entity id="L08-1232.19">information</entity> from <entity id="L08-1232.20">research</entity> <entity id="L08-1232.21">papers</entity> is a <entity id="L08-1232.22">text</entity> mining <entity id="L08-1232.23">task</entity> , <entity id="L08-1232.24">requiring</entity> both extensive <entity id="L08-1232.25">language resources</entity> and specialised <entity id="L08-1232.26">knowledge</entity> of the subject <entity id="L08-1232.27">domain</entity> . However, the <entity id="L08-1232.28">papers</entity> also encode <entity id="L08-1232.29">information</entity> about the way the <entity id="L08-1232.30">research</entity> is conducted and the <entity id="L08-1232.31">structure</entity> of the <entity id="L08-1232.32">field</entity> itself. <entity id="L08-1232.33">Applying</entity> <entity id="L08-1232.34">language technology</entity> to <entity id="L08-1232.35">research</entity> <entity id="L08-1232.36">papers</entity> in chemistry can facilitate eScience on several different <entity id="L08-1232.37">levels</entity> . The SciBorg <entity id="L08-1232.38">project</entity> sets out to <entity id="L08-1232.39">provide</entity> an extensive, analysed <entity id="L08-1232.40">corpus</entity> of published chemistry <entity id="L08-1232.41">research</entity> . This relies on the <entity id="L08-1232.42">cooperation</entity> of several <entity id="L08-1232.43">journal</entity> publishers to <entity id="L08-1232.44">provide</entity> <entity id="L08-1232.45">papers</entity> in an appropriate <entity id="L08-1232.46">form</entity> . The work is carried out as a <entity id="L08-1232.47">collaboration</entity> involving the <entity id="L08-1232.48">Computer</entity> <entity id="L08-1232.49">Laboratory</entity> , Chemistry <entity id="L08-1232.50">Department</entity> and eScience Centre at Cambridge <entity id="L08-1232.51">University</entity> , and is funded under the UK eScience programme.
</abstract>


</text>

<text id="L08-1236">
<title>
MeSH: from a Controlled <entity id="L08-1236.1">Vocabulary</entity> to a Processable <entity id="L08-1236.2">Resource</entity></title>
<abstract>
Large <entity id="L08-1236.3">repositories</entity> of life <entity id="L08-1236.4">science</entity> <entity id="L08-1236.5">data</entity> in the <entity id="L08-1236.6">form</entity> of <entity id="L08-1236.7">domain-specific</entity> <entity id="L08-1236.8">literature</entity> , textual <entity id="L08-1236.9">databases</entity> and other large specialised textual <entity id="L08-1236.10">collections</entity> ( <entity id="L08-1236.11">corpora</entity> ) in electronic <entity id="L08-1236.12">form</entity> <entity id="L08-1236.13">increase</entity> on a daily <entity id="L08-1236.14">basis</entity> to a <entity id="L08-1236.15">level</entity> beyond the human mind can grasp and interpret. As the <entity id="L08-1236.16">volume</entity> of <entity id="L08-1236.17">data</entity> continues to <entity id="L08-1236.18">increase</entity> , substantial <entity id="L08-1236.19">support</entity> from new <entity id="L08-1236.20">information technologies</entity> and <entity id="L08-1236.21">computational</entity> <entity id="L08-1236.22">techniques</entity> grounded in the <entity id="L08-1236.23">form</entity> of the ever <entity id="L08-1236.24">increasing</entity> <entity id="L08-1236.25">applications</entity> of the mining <entity id="L08-1236.26">paradigm</entity></abstract>


</text>

<text id="L08-1378">
<title>
An <entity id="L08-1378.1">Evaluation</entity> of Spoken and Textual <entity id="L08-1378.2">Interaction</entity> in the RITEL Interactive <entity id="L08-1378.3">Question <entity id="L08-1378.4">Answering System</entity></entity></title>
<abstract>
The RITEL <entity id="L08-1378.5">project</entity> aims to integrate a spoken <entity id="L08-1378.6">language</entity> <entity id="L08-1378.7">dialogue system</entity> and an <entity id="L08-1378.8">open-domain</entity> <entity id="L08-1378.9">information retrieval system</entity> in <entity id="L08-1378.10">order</entity> to enable human <entity id="L08-1378.11">users</entity> to ask a general <entity id="L08-1378.12">question</entity> and to refine their <entity id="L08-1378.13">search</entity> for <entity id="L08-1378.14">information</entity> interactively. This <entity id="L08-1378.15">type</entity> of <entity id="L08-1378.16">system</entity> is often referred to as an Interactive <entity id="L08-1378.17">Question Answering</entity> (IQA) <entity id="L08-1378.18">system</entity> . In this <entity id="L08-1378.19">paper</entity> , we present an <entity id="L08-1378.20">evaluation</entity> of how the <entity id="L08-1378.21">performance</entity> of the RITEL <entity id="L08-1378.22">system</entity> differs when <entity id="L08-1378.23">users</entity> interact with it using spoken versus textual <entity id="L08-1378.24">input</entity> and <entity id="L08-1378.25">output</entity> . Our <entity id="L08-1378.26">results</entity> indicate that while <entity id="L08-1378.27">users</entity> do not perceive the two <entity id="L08-1378.28">versions</entity> to <entity id="L08-1378.29">perform</entity> significantly differently, many more <entity id="L08-1378.30">questions</entity> are asked in a typical <entity id="L08-1378.31">text-based</entity> <entity id="L08-1378.32">dialogue</entity> .
</abstract>


</text>

<text id="D08-1093">
<title><entity id="D08-1093.1">Automatic</entity> <entity id="D08-1093.2">Prediction</entity> of <entity id="D08-1093.3">Parser</entity> <entity id="D08-1093.4">Accuracy</entity></title> 
<abstract><entity id="D08-1093.5">Statistical</entity> <entity id="D08-1093.6">parsers</entity> have become increasingly accurate, to the point where they are useful in many <entity id="D08-1093.7">natural language</entity> <entity id="D08-1093.8">applications</entity> . However, estimating <entity id="D08-1093.9">parsing</entity> <entity id="D08-1093.10">accuracy</entity> on a wide <entity id="D08-1093.11">variety</entity> of <entity id="D08-1093.12">domains</entity> and <entity id="D08-1093.13">genres</entity> is still a <entity id="D08-1093.14">challenge</entity> in the <entity id="D08-1093.15">absence</entity> of <entity id="D08-1093.16">gold-standard</entity> <entity id="D08-1093.17">parse trees</entity> . In this <entity id="D08-1093.18">paper</entity> , we <entity id="D08-1093.19">propose</entity> a <entity id="D08-1093.20">technique</entity> that automatically takes into account certain <entity id="D08-1093.21">characteristics</entity> of the <entity id="D08-1093.22">domains</entity> of interest, and accurately predicts <entity id="D08-1093.23">parser</entity> <entity id="D08-1093.24">performance</entity> on <entity id="D08-1093.25">data</entity> from these new <entity id="D08-1093.26">domains</entity> . As a <entity id="D08-1093.27">result</entity> , we have a cheap (no annotation involved) and effective recipe for measuring the <entity id="D08-1093.28">performance</entity> of a <entity id="D08-1093.29">statistical</entity> <entity id="D08-1093.30">parser</entity> on any given <entity id="D08-1093.31">domain</entity> .
</abstract>


</text>

<text id="I05-1062">
<title>
French- <entity id="I05-1062.1">English</entity> <entity id="I05-1062.2">Terminology</entity> <entity id="I05-1062.3">Extraction</entity> from Comparable <entity id="I05-1062.4">Corpora</entity></title> 
<abstract><entity id="I05-1062.5">Abstract</entity> .
</abstract>


</text>

<text id="I05-3010">
<title>
Turn-taking in Mandarin <entity id="I05-3010.1">Dialogue</entity> : Interactions of Tone and <entity id="I05-3010.2">Intonation</entity></title>
<abstract>
Fluent <entity id="I05-3010.3">dialogue</entity> <entity id="I05-3010.4">requires</entity> that speakers successfully negotiate and <entity id="I05-3010.5">signal</entity> turn-taking. While many <entity id="I05-3010.6">cues</entity> to turn change have been <entity id="I05-3010.7">proposed</entity> , especially in multi-modal <entity id="I05-3010.8">frameworks</entity> , here we <entity id="I05-3010.9">focus</entity> on the use ofprosodic <entity id="I05-3010.10">cues</entity> to these <entity id="I05-3010.11">functions</entity> . In particular, we consider the use of prosodic <entity id="I05-3010.12">cues</entity> in a tone <entity id="I05-3010.13">language</entity> , Mandarin <entity id="I05-3010.14">Chinese</entity> , where <entity id="I05-3010.15">variations</entity> in <entity id="I05-3010.16">pitch</entity> height and slope additionally serve to determine <entity id="I05-3010.17">word meaning</entity> . Within a <entity id="I05-3010.18">corpus</entity> of spontaneous <entity id="I05-3010.19">Chinese</entity> <entity id="I05-3010.20">dialogues</entity> , we find that <entity id="I05-3010.21">turn-unit</entity> final syllables are significantly lower in average <entity id="I05-3010.22">pitch</entity> and intensity than <entity id="I05-3010.23">turn-unit</entity> initial syllables in both <entity id="I05-3010.24">smooth</entity> turn changes and <entity id="I05-3010.25">segments</entity> ended by speaker overlap. Interruptions are characterized by significant prosodic <entity id="I05-3010.26">differences</entity> from <entity id="I05-3010.27">smooth</entity> turn initiations. Furthermore, we demonstrate that these <entity id="I05-3010.28">contrasts</entity> correspond to an overall lowering across all tones in final position, which largely preserves the <entity id="I05-3010.29">relative</entity> heights of the <entity id="I05-3010.30">lexical</entity> tones. In <entity id="I05-3010.31">classification tasks</entity> , we <entity id="I05-3010.32">contrast</entity> the use of <entity id="I05-3010.33">text</entity> and prosodic <entity id="I05-3010.34">features</entity> . Finally, we demonstrate that, on balanced <entity id="I05-3010.35">training</entity> and <entity id="I05-3010.36">test sets</entity> , we can distinguish <entity id="I05-3010.37">turn-unit</entity> final <entity id="I05-3010.38">words</entity> from other <entity id="I05-3010.39">words</entity> at 
 93% <entity id="I05-3010.40">accuracy</entity> and interruptions from <entity id="I05-3010.41">smooth</entity> turn <entity id="I05-3010.42">unit</entity> initiations at 62% <entity id="I05-3010.43">accuracy</entity> .
</abstract>


</text>

<text id="E03-1036">
<title>
Multi-Modal Combinatory <entity id="E03-1036.1">Categorial Grammar</entity></title>
<abstract>
The <entity id="E03-1036.2">paper</entity> shows how Combinatory <entity id="E03-1036.3">Categorial Grammar</entity> (CCG) can be <entity id="E03-1036.4">adapted</entity> to take <entity id="E03-1036.5">advantage</entity> of the extra <entity id="E03-1036.6">resource-sensitivity</entity> <entity id="E03-1036.7">provided</entity> by the Categorial <entity id="E03-1036.8">Type</entity> <entity id="E03-1036.9">Logic</entity> <entity id="E03-1036.10">framework</entity> . The <entity id="E03-1036.11">resulting</entity> reformulation, Multi-Modal CCG, <entity id="E03-1036.12">supports</entity> lexically specified <entity id="E03-1036.13">control</entity> over the <entity id="E03-1036.14">applicability</entity> of combinatory <entity id="E03-1036.15">rules</entity> , permitting a universal rale <entity id="E03-1036.16">component</entity> and shedding the need for <entity id="E03-1036.17">language-specific</entity> <entity id="E03-1036.18">restrictions</entity> on <entity id="E03-1036.19">rules</entity> . We discuss some of the linguistic <entity id="E03-1036.20">motivation</entity> for these changes, define the Multi-Modal CCG <entity id="E03-1036.21">system</entity> and demonstrate how it works on some <entity id="E03-1036.22">basic</entity> <entity id="E03-1036.23">examples</entity> . We furthermore <entity id="E03-1036.24">outline</entity> some possible <entity id="E03-1036.25">extensions</entity> and address <entity id="E03-1036.26">computational</entity> <entity id="E03-1036.27">aspects</entity> of Multi-Modal CCG.
</abstract>


</text>

<text id="E03-1061">
<title><entity id="E03-1061.1">Automatic</entity> <entity id="E03-1061.2">Acquisition</entity> Of <entity id="E03-1061.3">Script</entity> <entity id="E03-1061.4">Knowledge</entity> From A <entity id="E03-1061.5">Text</entity> <entity id="E03-1061.6">Collection</entity></title>
<abstract>
In this <entity id="E03-1061.7">paper</entity> , we describe a <entity id="E03-1061.8">method</entity> for <entity id="E03-1061.9">automatic</entity> <entity id="E03-1061.10">acquisition</entity> of <entity id="E03-1061.11">script</entity> <entity id="E03-1061.12">knowledge</entity> from a <entity id="E03-1061.13">Japanese</entity> <entity id="E03-1061.14">text</entity> <entity id="E03-1061.15">collection</entity> . <entity id="E03-1061.16">Script</entity> <entity id="E03-1061.17">knowledge</entity> represents a typical <entity id="E03-1061.18">sequence</entity> of <entity id="E03-1061.19">actions</entity> that occur in a particular <entity id="E03-1061.20">situation</entity> . We <entity id="E03-1061.21">extracted</entity> <entity id="E03-1061.22">sequences</entity> ( <entity id="E03-1061.23">pairs</entity> ) of <entity id="E03-1061.24">actions</entity> occurring in <entity id="E03-1061.25">time</entity> <entity id="E03-1061.26">order</entity> from a <entity id="E03-1061.27">Japanese</entity> <entity id="E03-1061.28">text</entity> <entity id="E03-1061.29">collection</entity> and then chose those that were typical of certain <entity id="E03-1061.30">situations</entity> by <entity id="E03-1061.31">ranking</entity> these <entity id="E03-1061.32">sequences</entity> ( <entity id="E03-1061.33">pairs</entity> ) in <entity id="E03-1061.34">terms</entity> of the <entity id="E03-1061.35">frequency</entity> of their <entity id="E03-1061.36">occurrence</entity> . To <entity id="E03-1061.37">extract</entity> <entity id="E03-1061.38">sequences</entity> of <entity id="E03-1061.39">actions</entity> occurring in <entity id="E03-1061.40">time</entity> <entity id="E03-1061.41">order</entity> , we <entity id="E03-1061.42">constructed</entity> a <entity id="E03-1061.43">text</entity> <entity id="E03-1061.44">collection</entity> in which <entity id="E03-1061.45">texts</entity> describing facts relating to a similar <entity id="E03-1061.46">situation</entity> were <entity id="E03-1061.47">clustered</entity> together and arranged in <entity id="E03-1061.48">time</entity> <entity id="E03-1061.49">order</entity> . We also describe a preliminary <entity id="E03-1061.50">experiment</entity> with our <entity id="E03-1061.51">acquisition</entity> <entity id="E03-1061.52">system</entity> and discuss the <entity id="E03-1061.53">results</entity> .
</abstract>


</text>

<text id="E06-1040">
<title>
Comparing <entity id="E06-1040.1">Automatic</entity> And Human <entity id="E06-1040.2">Evaluation</entity> Of NLG <entity id="E06-1040.3">Systems</entity></title>
<abstract>
We consider the <entity id="E06-1040.4">evaluation</entity> <entity id="E06-1040.5">problem</entity> in <entity id="E06-1040.6">Natural <entity id="E06-1040.7">Language Generation</entity></entity> (nlg) and present <entity id="E06-1040.8">results</entity> for <entity id="E06-1040.9">evaluating</entity> several nlg <entity id="E06-1040.10">systems</entity> with similar <entity id="E06-1040.11">functionality</entity> , <entity id="E06-1040.12">including</entity> a <entity id="E06-1040.13">knowledge-based</entity> <entity id="E06-1040.14">generator</entity> and several <entity id="E06-1040.15">statistical</entity> <entity id="E06-1040.16">systems</entity> . We compare <entity id="E06-1040.17">evaluation results</entity> for these <entity id="E06-1040.18">systems</entity> by human <entity id="E06-1040.19">domain</entity> <entity id="E06-1040.20">experts</entity> , human non-experts, and several automatic <entity id="E06-1040.21">evaluation metrics</entity> , <entity id="E06-1040.22">including</entity> nist, bleu, and <entity id="E06-1040.23">rouge</entity> . We find that nist scores correlate best (> 0.8) with human <entity id="E06-1040.24">judgments</entity> , but that all <entity id="E06-1040.25">automatic</entity> <entity id="E06-1040.26">metrics</entity> we examined are <entity id="E06-1040.27">biased</entity> in favour of <entity id="E06-1040.28">generators</entity> that select on the <entity id="E06-1040.29">basis</entity> of <entity id="E06-1040.30">frequency</entity> alone. We conclude that <entity id="E06-1040.31">automatic <entity id="E06-1040.32">evaluation</entity></entity> of nlg <entity id="E06-1040.33">systems</entity> has considerable potential, in particular where <entity id="E06-1040.34">high-quality</entity> <entity id="E06-1040.35">reference</entity> <entity id="E06-1040.36">texts</entity> and only a small <entity id="E06-1040.37">number</entity> of human evalua-tors are available. However, in general it is probably best for <entity id="E06-1040.38">automatic evaluations</entity> to be <entity id="E06-1040.39">supported</entity> by human-based <entity id="E06-1040.40">evaluations</entity> , or at least by <entity id="E06-1040.41">studies</entity> that demonstrate that a particular <entity id="E06-1040.42">metric</entity> correlates well with human <entity id="E06-1040.43">judgments</entity> in a given <entity id="E06-1040.44">domain</entity> .
</abstract>


</text>

<text id="N03-2017">
<title><entity id="N03-2017.1">Word <entity id="N03-2017.2">Alignment</entity></entity> With <entity id="N03-2017.3">Cohesion</entity> <entity id="N03-2017.4">Constraint</entity></title>
<abstract>
We present a <entity id="N03-2017.5">syntax-based</entity> <entity id="N03-2017.6">constraint</entity> for <entity id="N03-2017.7">word alignment</entity> , known as the <entity id="N03-2017.8">cohesion</entity> <entity id="N03-2017.9">constraint</entity> . It <entity id="N03-2017.10">requires</entity> disjoint <entity id="N03-2017.11">English</entity> <entity id="N03-2017.12">phrases</entity> to be <entity id="N03-2017.13">mapped</entity> to non-overlapping <entity id="N03-2017.14">intervals</entity> in the French <entity id="N03-2017.15">sentence</entity> . We <entity id="N03-2017.16">evaluate</entity> the <entity id="N03-2017.17">utility</entity> of this <entity id="N03-2017.18">constraint</entity> in two different <entity id="N03-2017.19">algorithms</entity> . The <entity id="N03-2017.20">results</entity> show that it can <entity id="N03-2017.21">provide</entity> a significant <entity id="N03-2017.22">improvement</entity> in <entity id="N03-2017.23">alignment</entity> <entity id="N03-2017.24">quality</entity> .
</abstract>


</text>

<text id="N03-2027">
<title>
Bayesian Nets For <entity id="N03-2027.1">Syntactic</entity> <entity id="N03-2027.2">Categorization</entity> Of Novel <entity id="N03-2027.3">Words</entity></title>
<abstract>
This <entity id="N03-2027.4">paper</entity> presents an <entity id="N03-2027.5">application</entity> of a Dynamic Bayesian <entity id="N03-2027.6">Network</entity> (DBN) to the <entity id="N03-2027.7">task</entity> of assigning <entity id="N03-2027.8">Part-of-</entity> <entity id="N03-2027.9">Speech</entity> (PoS) <entity id="N03-2027.10">tags</entity> to novel <entity id="N03-2027.11">text</entity> . This <entity id="N03-2027.12">task</entity> is particularly <entity id="N03-2027.13">challenging</entity> for <entity id="N03-2027.14">non-standard</entity> <entity id="N03-2027.15">corpora</entity> , such as Internet lingo, where a large proportion of <entity id="N03-2027.16">words</entity> are unknown. Previous work reveals that <entity id="N03-2027.17">PoS <entity id="N03-2027.18">tags</entity></entity> depend on a <entity id="N03-2027.19">variety</entity> of morphological and <entity id="N03-2027.20">contextual <entity id="N03-2027.21">features</entity></entity> . Representing these <entity id="N03-2027.22">dependencies</entity> in a DBN <entity id="N03-2027.23">results</entity> into an elegant and effective PoS tagger.
</abstract>


</text>

<text id="N04-1018">
<title>
Detecting <entity id="N04-1018.1">Structural</entity> Metadata With <entity id="N04-1018.2">Decision</entity> Trees And <entity id="N04-1018.3">Transformation-</entity> <entity id="N04-1018.4">Based</entity> Learning
</title>
<abstract>
The regular <entity id="N04-1018.5">occurrence</entity> of disfluencies is a distinguishing <entity id="N04-1018.6">characteristic</entity> of <entity id="N04-1018.7">spontaneous speech</entity> . Detecting and removing such disfluencies can substantially <entity id="N04-1018.8">improve</entity> the <entity id="N04-1018.9">usefulness</entity> of <entity id="N04-1018.10">spontaneous speech</entity> <entity id="N04-1018.11">transcripts</entity> . This <entity id="N04-1018.12">paper</entity> presents a <entity id="N04-1018.13">system</entity> that detects various <entity id="N04-1018.14">types</entity> of disfluencies and other <entity id="N04-1018.15">structural</entity> <entity id="N04-1018.16">information</entity> with <entity id="N04-1018.17">cues</entity> obtained from <entity id="N04-1018.18">lexical</entity> and prosodic <entity id="N04-1018.19">information <entity id="N04-1018.20">sources</entity></entity> . Specifically, <entity id="N04-1018.21">combinations</entity> of <entity id="N04-1018.22">decision trees</entity> and <entity id="N04-1018.23">language models</entity> are used to predict <entity id="N04-1018.24">sentence</entity> ends and interruption points and, given these <entity id="N04-1018.25">events</entity> , <entity id="N04-1018.26">transformation-based</entity> <entity id="N04-1018.27">learning</entity> is used to detect edit disfluencies and conversational <entity id="N04-1018.28">fillers</entity> . <entity id="N04-1018.29">Results</entity> are <entity id="N04-1018.30">reported</entity> on human and <entity id="N04-1018.31">automatic</entity> <entity id="N04-1018.32">transcripts</entity> of conversational <entity id="N04-1018.33">telephone</entity> <entity id="N04-1018.34">speech</entity> .
</abstract>


</text>

<text id="N04-4021">
<title><entity id="N04-4021.1">Feature-</entity> <entity id="N04-4021.2">Based</entity> <entity id="N04-4021.3">Pronunciation</entity> <entity id="N04-4021.4">Modeling</entity> For <entity id="N04-4021.5">Speech <entity id="N04-4021.6">Recognition</entity></entity></title>
<abstract>
We present an <entity id="N04-4021.7">approach</entity> to <entity id="N04-4021.8">pronunciation</entity> <entity id="N04-4021.9">modeling</entity> in which the <entity id="N04-4021.10">evolution</entity> of multiple <entity id="N04-4021.11">linguistic feature</entity> <entity id="N04-4021.12">streams</entity> is explicitly represented. This differs from <entity id="N04-4021.13">phone-based models</entity> in that <entity id="N04-4021.14">pronunciation</entity> <entity id="N04-4021.15">variation</entity> is viewed as the <entity id="N04-4021.16">result</entity> of <entity id="N04-4021.17">feature</entity> asynchrony and changes in <entity id="N04-4021.18">feature values</entity> , rather than <entity id="N04-4021.19">phone</entity> <entity id="N04-4021.20">substitutions</entity> , <entity id="N04-4021.21">insertions</entity> , and <entity id="N04-4021.22">deletions</entity> . We have <entity id="N04-4021.23">implemented</entity> a flexible <entity id="N04-4021.24">feature-based</entity> <entity id="N04-4021.25">pronunciation</entity> <entity id="N04-4021.26">model</entity> using dynamic Bayesian <entity id="N04-4021.27">networks</entity> . In this <entity id="N04-4021.28">paper</entity> , we describe our <entity id="N04-4021.29">approach</entity> and <entity id="N04-4021.30">report</entity> on a <entity id="N04-4021.31">pilot</entity> <entity id="N04-4021.32">experiment</entity> using phonetic <entity id="N04-4021.33">transcriptions</entity> of <entity id="N04-4021.34">utterances</entity> from the Switchboard <entity id="N04-4021.35">corpus</entity> . The <entity id="N04-4021.36">experimental</entity> <entity id="N04-4021.37">results</entity> , as well as the <entity id="N04-4021.38">model</entity> 's qualitative <entity id="N04-4021.39">behavior</entity> , suggest that this is a promising way of <entity id="N04-4021.40">accounting</entity> for the <entity id="N04-4021.41">types</entity> of <entity id="N04-4021.42">pronunciation</entity> <entity id="N04-4021.43">variation</entity> often seen in <entity id="N04-4021.44">spontaneous speech</entity> .
</abstract>


</text>

<text id="N04-4028">
<title><entity id="N04-4028.1">Confidence</entity> <entity id="N04-4028.2">Estimation</entity> For <entity id="N04-4028.3">Information Extraction</entity></title> 
<abstract><entity id="N04-4028.4">Information extraction</entity> <entity id="N04-4028.5">techniques</entity> automatically create <entity id="N04-4028.6">structured</entity> <entity id="N04-4028.7">databases</entity> from unstructured <entity id="N04-4028.8">data</entity> <entity id="N04-4028.9">sources</entity> , such as the Web or newswire <entity id="N04-4028.10">documents</entity> . Despite the <entity id="N04-4028.11">successes</entity> of these <entity id="N04-4028.12">systems</entity> , <entity id="N04-4028.13">accuracy</entity> will always be imperfect. For many <entity id="N04-4028.14">reasons</entity> , it is highly desirable to accurately estimate the <entity id="N04-4028.15">confidence</entity> the <entity id="N04-4028.16">system</entity> has in the <entity id="N04-4028.17">correctness</entity> of each <entity id="N04-4028.18">extracted</entity> <entity id="N04-4028.19">field</entity> . The <entity id="N04-4028.20">information <entity id="N04-4028.21">extraction system</entity></entity> we <entity id="N04-4028.22">evaluate</entity> is <entity id="N04-4028.23">based</entity> on a <entity id="N04-4028.24">linear-chain</entity> <entity id="N04-4028.25">conditional random <entity id="N04-4028.26">field</entity></entity> (CRF), a <entity id="N04-4028.27">probabilistic model</entity> which has <entity id="N04-4028.28">performed</entity> well on information <entity id="N04-4028.29">extraction tasks</entity> because of its <entity id="N04-4028.30">ability</entity> to capture arbitrary, overlapping <entity id="N04-4028.31">features</entity> of the <entity id="N04-4028.32">input</entity> in a <entity id="N04-4028.33">Markov model</entity> . We <entity id="N04-4028.34">implement</entity> several <entity id="N04-4028.35">techniques</entity> to estimate the <entity id="N04-4028.36">confidence</entity> of both <entity id="N04-4028.37">extracted</entity> <entity id="N04-4028.38">fields</entity> and entire <entity id="N04-4028.39">multi-field</entity> <entity id="N04-4028.40">records</entity> , obtaining an average <entity id="N04-4028.41">precision</entity> of 98% for retrieving correct <entity id="N04-4028.42">fields</entity> and 87% for <entity id="N04-4028.43">multi-field</entity> <entity id="N04-4028.44">records</entity> .
</abstract>


</text>

<text id="M91-1029">
<title>
PRC Inc: <entity id="M91-1029.1">Description</entity> Of The PAKTUS <entity id="M91-1029.2">System</entity> Used For MUC-3
</title>
<abstract>
The PRC Adaptive <entity id="M91-1029.3">Knowledge-based</entity> <entity id="M91-1029.4">Text</entity> <entity id="M91-1029.5">Understanding System</entity> (PAKTUS) has been under <entity id="M91-1029.6">development</entity> as an Independent <entity id="M91-1029.7">Research</entity> and <entity id="M91-1029.8">Development</entity> <entity id="M91-1029.9">project</entity> at PRC since 1984. The <entity id="M91-1029.10">objective</entity> is a generic <entity id="M91-1029.11">system</entity> of <entity id="M91-1029.12">tools</entity> , <entity id="M91-1029.13">including</entity> a <entity id="M91-1029.14">core</entity> <entity id="M91-1029.15">English</entity> <entity id="M91-1029.16">lexicon</entity> , grammar, and <entity id="M91-1029.17">concept</entity> <entity id="M91-1029.18">representations</entity> , for <entity id="M91-1029.19">building</entity> <entity id="M91-1029.20">natural language processing</entity> (NLP) <entity id="M91-1029.21">systems</entity> for <entity id="M91-1029.22">text</entity> <entity id="M91-1029.23">understanding</entity> . <entity id="M91-1029.24">Systems</entity> built with PAKTUS are intended to <entity id="M91-1029.25">generate</entity> <entity id="M91-1029.26">input</entity> to <entity id="M91-1029.27">knowledge based</entity> <entity id="M91-1029.28">systems</entity> or <entity id="M91-1029.29">data</entity> <entity id="M91-1029.30">base</entity> <entity id="M91-1029.31">systems</entity> . <entity id="M91-1029.32">Input</entity> to the <entity id="M91-1029.33">NLP system</entity> is typically derived from an existing electronic <entity id="M91-1029.34">message</entity> <entity id="M91-1029.35">stream</entity> , such as a <entity id="M91-1029.36">news</entity> wire. PAKTUS <entity id="M91-1029.37">supports</entity> the <entity id="M91-1029.38">adaptation</entity> of the generic <entity id="M91-1029.39">core</entity> to a <entity id="M91-1029.40">variety</entity> of <entity id="M91-1029.41">domains</entity> : JINTACCS <entity id="M91-1029.42">messages</entity> , RAINFORM <entity id="M91-1029.43">messages</entity> , <entity id="M91-1029.44">news</entity> <entity id="M91-1029.45">reports</entity> about a specific <entity id="M91-1029.46">type</entity> of <entity id="M91-1029.47">event</entity> , such as financial <entity id="M91-1029.48">transfers</entity> or terrorist acts, etc., by acquiring sublanguage and <entity id="M91-1029.49">domain-specific</entity> grammar, <entity id="M91-1029.50">words</entity> , conceptual <entity id="M91-1029.51">mappings</entity> , and <entity id="M91-1029.52">discourse</entity> <entity id="M91-1029.53">patterns</entity> . The <entity id="M91-1029.54">long-term</entity> <entity id="M91-1029.55">goal</entity> is a <entity id="M91-1029.56">system</entity> that can <entity id="M91-1029.57">support</entity> the <entity id="M91-1029.58">processing</entity> of relatively long <entity id="M91-1029.59">discourses</entity> in <entity id="M91-1029.60">domains</entity> that are fairly broad with a high <entity id="M91-1029.61">rate</entity> of <entity id="M91-1029.62">success</entity> .
</abstract>


</text>

<text id="A92-1035">
<title>
Practical World <entity id="A92-1035.1">Modeling</entity> For <entity id="A92-1035.2">NLP <entity id="A92-1035.3">Applications</entity></entity></title>


</text>

<abstract></abstract>

<text id="A92-1041">
<title><entity id="A92-1041.1">Lexicon</entity> <entity id="A92-1041.2">Design</entity> Using A Paradigmatic <entity id="A92-1041.3">Approach</entity></title>
<abstract>
The <entity id="A92-1041.4">paper</entity> describes <entity id="A92-1041.5">models</entity> for <entity id="A92-1041.6">representation</entity> and <entity id="A92-1041.7">methods</entity> to handle lexicographic <entity id="A92-1041.8">structures</entity> supplied by the
</abstract>


</text>

<text id="A94-1002">
<title>
Practical Issues In <entity id="A94-1002.1">Automatic</entity> <entity id="A94-1002.2">Documentation</entity> <entity id="A94-1002.3">Generation</entity></title>
<abstract>
PLANDoc, a <entity id="A94-1002.4">system</entity> under joint <entity id="A94-1002.5">development</entity> by Columbia and Bellcore, <entity id="A94-1002.6">documents</entity> the activity of planning engineers as they <entity id="A94-1002.7">study</entity> <entity id="A94-1002.8">telephone</entity> <entity id="A94-1002.9">routes</entity> . It takes as <entity id="A94-1002.10">input</entity> a trace of the engineer's <entity id="A94-1002.11">interaction</entity> with a <entity id="A94-1002.12">network</entity> planning <entity id="A94-1002.13">tool</entity> and produces 1-2 <entity id="A94-1002.14">page</entity> <entity id="A94-1002.15">summary</entity> . In this <entity id="A94-1002.16">paper</entity> , we describe the <entity id="A94-1002.17">user</entity> needs <entity id="A94-1002.18">analysis</entity> we <entity id="A94-1002.19">performed</entity> and how it <entity id="A94-1002.20">influenced</entity> the <entity id="A94-1002.21">development</entity> of PLANDoc. In particular, we show how it pinpointed the need for a sublanguage <entity id="A94-1002.22">specification</entity> , allowing us to identify <entity id="A94-1002.23">input</entity> <entity id="A94-1002.24">messages</entity> and to characterize the different <entity id="A94-1002.25">sentence</entity> paraphrases for realizing them. We <entity id="A94-1002.26">focus</entity> on the systematic use of <entity id="A94-1002.27">conjunction</entity> in <entity id="A94-1002.28">combination</entity> with paraphrase that we <entity id="A94-1002.29">developed</entity> for PLANDoc, which allows for the <entity id="A94-1002.30">generation</entity> of <entity id="A94-1002.31">summaries</entity> that are both concise-avoiding <entity id="A94-1002.32">repetition</entity> of similar <entity id="A94-1002.33">information</entity> , and fluent-avoiding <entity id="A94-1002.34">repetition</entity> of similar phrasing.
</abstract>


</text>

<text id="H90-1017">
<title>
The Dragon <entity id="H90-1017.1">Continuous Speech Recognition System</entity> : A Real- <entity id="H90-1017.2">Time</entity> <entity id="H90-1017.3">Implementation</entity></title>
<abstract>
We present a 1000- <entity id="H90-1017.4">word</entity> <entity id="H90-1017.5">continuous speech recognition</entity> (CSR) <entity id="H90-1017.6">system</entity> that operates in <entity id="H90-1017.7">real time</entity> on a personal <entity id="H90-1017.8">computer</entity> (PC). The <entity id="H90-1017.9">system</entity> , <entity id="H90-1017.10">designed</entity> for large <entity id="H90-1017.11">vocabulary</entity> <entity id="H90-1017.12">natural language</entity> <entity id="H90-1017.13">tasks</entity> , makes use of phonetic <entity id="H90-1017.14">Hidden Markov models</entity> (HMM) and incorporates acoustic, phonetic, and linguistic <entity id="H90-1017.15">sources</entity> of <entity id="H90-1017.16">knowledge</entity> to achieve high <entity id="H90-1017.17">recognition</entity> <entity id="H90-1017.18">performance</entity> . We describe the various <entity id="H90-1017.19">components</entity> of this <entity id="H90-1017.20">system</entity> . We also present our <entity id="H90-1017.21">strategy</entity> for achieving <entity id="H90-1017.22">real time</entity> <entity id="H90-1017.23">recognition</entity> on the PC. Using a 486- <entity id="H90-1017.24">based</entity> PC with a 29K- <entity id="H90-1017.25">based</entity> add-on board, the recognizer has been <entity id="H90-1017.26">timed</entity> at 1.1 <entity id="H90-1017.27">times</entity> <entity id="H90-1017.28">real time</entity> .
</abstract>


</text>

<text id="H90-1028">
<title>
Preliminary ATIS <entity id="H90-1028.1">Development</entity> At MIT
</title>
<abstract>
"DARPA has recently initiated a plan for a <entity id="H90-1028.2">common</entity> spoken <entity id="H90-1028.3">language</entity> <entity id="H90-1028.4">task</entity> , to be <entity id="H90-1028.5">developed</entity> independently by all members of the DARPA <entity id="H90-1028.6">community</entity> , with the hope that it will <entity id="H90-1028.7">provide</entity> a <entity id="H90-1028.8">mechanism</entity> leading to appropriate formal <entity id="H90-1028.9">evaluation</entity> <entity id="H90-1028.10">procedures</entity> at the <entity id="H90-1028.11">level</entity> of spoken <entity id="H90-1028.12">language</entity> . The <entity id="H90-1028.13">task</entity> that was selected for this <entity id="H90-1028.14">purpose</entity> is the Air Travel <entity id="H90-1028.15">Information System</entity> (ATIS) <entity id="H90-1028.16">task</entity> , <entity id="H90-1028.17">based</entity> on selected <entity id="H90-1028.18">tables</entity> from the Official Airline Guide (OAG). It was decided that the first <entity id="H90-1028.19">evaluation</entity> would be limited in <entity id="H90-1028.20">scope</entity> to <entity id="H90-1028.21">deal</entity> with <entity id="H90-1028.22">text</entity> <entity id="H90-1028.23">input</entity> only, and to cover only <entity id="H90-1028.24">sentences</entity> that could be understood unambiguously out of <entity id="H90-1028.25">context</entity> . <entity id="H90-1028.26">Data</entity> have been <entity id="H90-1028.27">recorded</entity> over the past several months at Texas Instruments, using an <entity id="H90-1028.28">interface</entity> that involves a ""wizard"" who fully interprets the meaning of the subject's <entity id="H90-1028.29">sentences</entity> , and <entity id="H90-1028.30">generates</entity> <entity id="H90-1028.31">database</entity> responses using a <entity id="H90-1028.32">menu</entity> driven <entity id="H90-1028.33">data</entity> <entity id="H90-1028.34">access</entity> <entity id="H90-1028.35">system</entity> . We have been actively engaged in the last few months in <entity id="H90-1028.36">developing</entity> the <entity id="H90-1028.37">natural language</entity> and back end <entity id="H90-1028.38">portions</entity> of the MIT <entity id="H90-1028.39">version</entity> of the ATIS <entity id="H90-1028.40">domain</entity> . This <entity id="H90-1028.41">paper</entity> describes our <entity id="H90-1028.42">progress</entity> to <entity id="H90-1028.43">date</entity> on this <entity id="H90-1028.44">effort</entity> , <entity id="H90-1028.45">including</entity> an <entity id="H90-1028.46">evaluation</entity> of the <entity id="H90-1028.47">performance</entity> of the <entity id="H90-1028.48">system</entity> on the recently released designated DARPA <entity id="H90-1028.49">test set</entity> . The <entity id="H90-1028.50">remainder</entity> of this <entity id="H90-1028.51">paper</entity> is organized as follows. First we will give a general <entity id="H90-1028.52">description</entity> of the <entity id="H90-1028.53">system</entity> we are <entity id="H90-1028.54">developing</entity> , emphasizing those <entity id="H90-1028.55">aspects</entity> that differ from the <entity id="H90-1028.56">current</entity> general conception of the <entity id="H90-1028.57">common</entity> <entity id="H90-1028.58">task</entity> . Next we will describe in greater <entity id="H90-1028.59">detail</entity> certain <entity id="H90-1028.60">aspects</entity> of the back end, <entity id="H90-1028.61">including</entity> <entity id="H90-1028.62">knowledge representation</entity> , <entity id="H90-1028.63">control</entity> <entity id="H90-1028.64">strategy</entity> , the <entity id="H90-1028.65">user interface</entity> , and our preliminary <entity id="H90-1028.66">treatment</entity> of <entity id="H90-1028.67">discourse</entity> history. This is followed by a <entity id="H90-1028.68">section</entity> describing changes made in the <entity id="H90-1028.69">parser</entity> , in the <entity id="H90-1028.70">areas</entity> of <entity id="H90-1028.71">semantics</entity> , the <entity id="H90-1028.72">interface</entity> with the back-end, and a preliminary <entity id="H90-1028.73">new-word</entity> <entity id="H90-1028.74">treatment</entity> . This <entity id="H90-1028.75">section</entity> also <entity id="H90-1028.76">includes</entity> a brief <entity id="H90-1028.77">discussion</entity> of some interesting <entity id="H90-1028.78">phenomena</entity> that occurred in the <entity id="H90-1028.79">training</entity> <entity id="H90-1028.80">sentences</entity> . An <entity id="H90-1028.81">evaluation</entity> <entity id="H90-1028.82">section</entity> follows, discussing our <entity id="H90-1028.83">system</entity> 's <entity id="H90-1028.84">performance</entity> on both <entity id="H90-1028.85">training</entity> and <entity id="H90-1028.86">test</entity> data, as well as a preliminary <entity id="H90-1028.87">assessment</entity> of the <entity id="H90-1028.88">perplexity</entity> of the <entity id="H90-1028.89">system</entity> . We conclude with a <entity id="H90-1028.90">summary</entity> of our <entity id="H90-1028.91">results</entity> and our position on the <entity id="H90-1028.92">nature</entity> of the <entity id="H90-1028.93">common</entity> <entity id="H90-1028.94">task</entity> . "
</abstract>


</text>

<text id="H91-1023">
<title>
Session 3: <entity id="H91-1023.1">Machine Translation</entity></title> 
<abstract><entity id="H91-1023.2">Semantic <entity id="H91-1023.3">analysis</entity></entity> to resolve <entity id="H91-1023.4">lexical</entity> and <entity id="H91-1023.5">syntactic</entity> <entity id="H91-1023.6">ambiguities</entity> during <entity id="H91-1023.7">parsing</entity> , and thus reduce <entity id="H91-1023.8">translation</entity> <entity id="H91-1023.9">errors</entity> very significantly. 
 <entity id="H91-1023.10">Unification</entity> grammars allowing <entity id="H91-1023.11">syntactic</entity> and <entity id="H91-1023.12">semantic</entity> <entity id="H91-1023.13">constraints</entity> to be <entity id="H91-1023.14">checked</entity> in a unified <entity id="H91-1023.15">manner</entity> while <entity id="H91-1023.16">parsing</entity> , and permitting reversible grammars
i.e., the same grammars to be used for <entity id="H91-1023.17">generation</entity> as well as for <entity id="H91-1023.18">analysis</entity> . 
 Advanced <entity id="H91-1023.19">parsing</entity> <entity id="H91-1023.20">methodologies</entity> , <entity id="H91-1023.21">including</entity> augmented-LR <entity id="H91-1023.22">compilation</entity> where <entity id="H91-1023.23">knowledge sources</entity> ( <entity id="H91-1023.24">syntactic</entity> grammars, <entity id="H91-1023.25">lexicons</entity> , and <entity id="H91-1023.26">semantic</entity> <entity id="H91-1023.27">ontologies</entity> ) can be defined and maintained separately but are jointly compiled to <entity id="H91-1023.28">apply</entity> simultaneously at run <entity id="H91-1023.29">time</entity> , both in <entity id="H91-1023.30">parsing</entity> and in <entity id="H91-1023.31">generation</entity> . 
 <entity id="H91-1023.32">Natural language generation</entity> , <entity id="H91-1023.33">focusing</entity> on how to <entity id="H91-1023.34">structure</entity> fluent <entity id="H91-1023.35">target-language</entity> <entity id="H91-1023.36">output</entity> , an activity not truly investigated in the pre-ALPAC days. 
 Automated <entity id="H91-1023.37">corpus</entity> <entity id="H91-1023.38">analysis</entity> <entity id="H91-1023.39">tools</entity> , <entity id="H91-1023.40">statistical</entity> and other means of <entity id="H91-1023.41">extracting</entity> useful <entity id="H91-1023.42">information</entity> from large bi- or <entity id="H91-1023.43">multi-lingual</entity> <entity id="H91-1023.44">corpora</entity> , <entity id="H91-1023.45">including</entity> <entity id="H91-1023.46">collocations</entity> , <entity id="H91-1023.47">transfers</entity> , and contextual <entity id="H91-1023.48">cues</entity> for <entity id="H91-1023.49">disambiguation</entity> . 
 MRDs => MTDs, use of electronic <entity id="H91-1023.50">machine-readable</entity> <entity id="H91-1023.51">dictionaries</entity> (MRDs) to partially automate the <entity id="H91-1023.52">creation</entity> of <entity id="H91-1023.53">machine-tractable</entity> <entity id="H91-1023.54">dictionaries</entity> (MTDs) in processable internal <entity id="H91-1023.55">form</entity> for <entity id="H91-1023.56">parsers</entity> and <entity id="H91-1023.57">generators</entity> , permitting principled <entity id="H91-1023.58">scaling</entity> up in MT <entity id="H91-1023.59">configurations</entity> .
</abstract>


</text>

<text id="H91-1045">
<title>
Calculating The <entity id="H91-1045.1">Probability</entity> Of A <entity id="H91-1045.2">Partial</entity> <entity id="H91-1045.3">Parse</entity> Of A <entity id="H91-1045.4">Sentence</entity></title>
<abstract>
A <entity id="H91-1045.5">standard</entity> <entity id="H91-1045.6">problem</entity> iu parsiug <entity id="H91-1045.7">algorithms</entity> is the <entity id="H91-1045.8">organization</entity> o[ branched <entity id="H91-1045.9">searches</entity> lo <entity id="H91-1045.10">deal</entity> with ambiguous <entity id="H91-1045.11">sentences</entity> . We discuss <entity id="H91-1045.12">shift-reduce</entity> parsiug of stochastic <entity id="H91-1045.13">context-free</entity> grammars and show how to <entity id="H91-1045.14">construct</entity> a probabilistic score for ranking competing <entity id="H91-1045.15">parse</entity> <entity id="H91-1045.16">hypotheses</entity> . The score we use is the likelihood that the <entity id="H91-1045.17">collection</entity> of subtrees can be completed into a full <entity id="H91-1045.18">parse tree</entity> by means of the <entity id="H91-1045.19">steps</entity> the <entity id="H91-1045.20">parser</entity> is constrained lo follow.
</abstract>


</text>

<text id="H91-1064">
<title><entity id="H91-1064.1">Discourse Structure</entity> In The TRAINS <entity id="H91-1064.2">Project</entity></title>
<abstract>
In a <entity id="H91-1064.3">natural</entity> <entity id="H91-1064.4">dialog</entity> , a considerable proportion of the <entity id="H91-1064.5">utterances</entity> actually relate to the <entity id="H91-1064.6">maintenance</entity> of the <entity id="H91-1064.7">dialog</entity> itself rather than to furthering the <entity id="H91-1064.8">task</entity> or <entity id="H91-1064.9">goals</entity> motivating the <entity id="H91-1064.10">conversation</entity> . For <entity id="H91-1064.11">example</entity> , many <entity id="H91-1064.12">utterances</entity> serve to acknowledge, clarify, correct a previous <entity id="H91-1064.13">utterance</entity> rather than pursue some <entity id="H91-1064.14">goal</entity> in the <entity id="H91-1064.15">domain</entity> . In <entity id="H91-1064.16">addition</entity> , <entity id="H91-1064.17">natural</entity> <entity id="H91-1064.18">dialog</entity> is full of false starts, ungrammatical <entity id="H91-1064.19">sentences</entity> and other <entity id="H91-1064.20">complexities</entity> not found in in written <entity id="H91-1064.21">language</entity> . This <entity id="H91-1064.22">paper</entity> describes our recent <entity id="H91-1064.23">efforts</entity> to define and <entity id="H91-1064.24">construct</entity> a <entity id="H91-1064.25">model</entity> of <entity id="H91-1064.26">discourse</entity> <entity id="H91-1064.27">interaction</entity> that handle <entity id="H91-1064.28">dialogs</entity> that are rich in these <entity id="H91-1064.29">natural</entity> <entity id="H91-1064.30">dialog-related</entity> <entity id="H91-1064.31">phenomena</entity> .
</abstract>


</text>

<text id="H93-1063">
<title>
Session 11: <entity id="H93-1063.1">Prosody</entity></title>
<abstract>
This <entity id="H93-1063.2">paper</entity> <entity id="H93-1063.3">provides</entity> a brief <entity id="H93-1063.4">introduction</entity> to <entity id="H93-1063.5">prosody</entity> <entity id="H93-1063.6">research</entity> in the <entity id="H93-1063.7">context</entity> of <entity id="H93-1063.8">human-computer</entity> <entity id="H93-1063.9">communication</entity> and an <entity id="H93-1063.10">overview</entity> of the <entity id="H93-1063.11">contributions</entity> of the <entity id="H93-1063.12">papers</entity> in the session.
</abstract>


</text>

<text id="H93-1078">
<title>
Gisting Continuous <entity id="H93-1078.1">Speech</entity></title>
<abstract>
"The <entity id="H93-1078.2">objective</entity> of this woik is <entity id="H93-1078.3">automatic</entity> , <entity id="H93-1078.4">real-time</entity> ""gisting"" of voice traffic for <entity id="H93-1078.5">updating</entity> of <entity id="H93-1078.6">information</entity> in <entity id="H93-1078.7">databases</entity> , for producing timely <entity id="H93-1078.8">reports</entity> , and for prompt notification of <entity id="H93-1078.9">events</entity> of interest. Specifically, the <entity id="H93-1078.10">goal</entity> is to build a <entity id="H93-1078.11">prototype</entity> , <entity id="H93-1078.12">real-time</entity> <entity id="H93-1078.13">system</entity> capable of <entity id="H93-1078.14">processing</entity> radio <entity id="H93-1078.15">communication</entity> between air traffic controllers and <entity id="H93-1078.16">pilots</entity> , identifying <entity id="H93-1078.17">dialogs</entity> and <entity id="H93-1078.18">extracting</entity> their ""gist"" (e.g., identifying flights, determining whether they are landing or taking off), and producing a continuous <entity id="H93-1078.19">output</entity> <entity id="H93-1078.20">stream</entity> with that <entity id="H93-1078.21">information</entity> . The <entity id="H93-1078.22">approach</entity> is intended to be general and applicable to other <entity id="H93-1078.23">domains</entity> . The <entity id="H93-1078.24">system</entity> is built upon state-of-the-art <entity id="H93-1078.25">techniques</entity> in <entity id="H93-1078.26">speech recognition</entity> , speaker <entity id="H93-1078.27">identification</entity> , <entity id="H93-1078.28">natural language analysis</entity> , and <entity id="H93-1078.29">topic</entity> <entity id="H93-1078.30">statistical</entity> <entity id="H93-1078.31">classification</entity> . These <entity id="H93-1078.32">techniques</entity> have been extended where necessary to address specific <entity id="H93-1078.33">aspects</entity> of the gisting <entity id="H93-1078.34">problem</entity> . Because various <entity id="H93-1078.35">sources</entity> of <entity id="H93-1078.36">information</entity> must be combined, the <entity id="H93-1078.37">system</entity> <entity id="H93-1078.38">design</entity> <entity id="H93-1078.39">features</entity> a high <entity id="H93-1078.40">degree</entity> of <entity id="H93-1078.41">interaction</entity> between the <entity id="H93-1078.42">natural language</entity> and <entity id="H93-1078.43">domain-knowledge</entity> <entity id="H93-1078.44">components</entity> and the <entity id="H93-1078.45">speech processing</entity> <entity id="H93-1078.46">components</entity> . "
</abstract>


</text>

<text id="H94-1029">
<title>
The <entity id="H94-1029.1">Automatic</entity> <entity id="H94-1029.2">Component</entity> Of The LINGSTAT <entity id="H94-1029.3">Machine-</entity> Aided <entity id="H94-1029.4">Translation <entity id="H94-1029.5">System</entity></entity></title>
<abstract>
We present the newest <entity id="H94-1029.6">implementation</entity> of the LINGSTAT <entity id="H94-1029.7">machine-aided</entity> <entity id="H94-1029.8">translation system</entity> . The most significant change from earlier <entity id="H94-1029.9">versions</entity> is a new set of <entity id="H94-1029.10">modules</entity> that produce a draft <entity id="H94-1029.11">translation</entity> of the <entity id="H94-1029.12">document</entity> for the <entity id="H94-1029.13">user</entity> to refer to or modify. This <entity id="H94-1029.14">paper</entity> describes these <entity id="H94-1029.15">modules</entity> , with special emphasis on an automatically <entity id="H94-1029.16">trained</entity> lexicalized grammar used in the <entity id="H94-1029.17">parsing</entity> <entity id="H94-1029.18">module</entity> . Some preliminary <entity id="H94-1029.19">results</entity> from the January  1994  ARPA <entity id="H94-1029.20">evaluation</entity> are <entity id="H94-1029.21">reported</entity> .
</abstract>


</text>

<text id="A97-1025">
<title>
Contextual Spelling <entity id="A97-1025.1">Correction</entity> Using <entity id="A97-1025.2">Latent <entity id="A97-1025.3">Semantic Analysis</entity></entity></title>
<abstract>Contextual <entity id="A97-1025.4">spelling</entity> <entity id="A97-1025.5">errors</entity> are denned as the use of an incorrect, though valid, <entity id="A97-1025.6">word</entity> in a particular <entity id="A97-1025.7">sentence</entity> or <entity id="A97-1025.8">context</entity> . Traditional <entity id="A97-1025.9">spelling</entity> checkers flag misspelled <entity id="A97-1025.10">words</entity> , but they do not typically attempt to identify <entity id="A97-1025.11">words</entity> that are used incorrectly in a <entity id="A97-1025.12">sentence</entity> . We explore the use of
</abstract>


</text>

<text id="X96-1046">
<title>
The <entity id="X96-1046.1">Text REtrieval</entity> Conferences (TRECs) - <entity id="X96-1046.2">Summary</entity> <entity id="X96-1046.3">Results</entity> Of TREC-3 And TREC-4
</title>
<abstract>
"There have been four <entity id="X96-1046.4">Text REtrieval</entity> Conferences (TRECs); TREC-1 in November  1992 , TREC-2 in August  1993 , TREC-3 in November  1994  and TREC-4 in November  1995 . The <entity id="X96-1046.5">number</entity> of participating <entity id="X96-1046.6">systems</entity> has grown from 25 in TREC-1 to 36 in TREC-4, <entity id="X96-1046.7">including</entity> most of the major <entity id="X96-1046.8">text retrieval</entity> <entity id="X96-1046.9">software</entity> companies and most of the <entity id="X96-1046.10">universities</entity> doing <entity id="X96-1046.11">research</entity> in <entity id="X96-1046.12">text <entity id="X96-1046.13">retrieval</entity></entity> (see <entity id="X96-1046.14">table</entity> for some of the <entity id="X96-1046.15">participants</entity> ). The <entity id="X96-1046.16">diversity</entity> of the participating groups has ensured that TREC represents many different <entity id="X96-1046.17">approaches</entity> to <entity id="X96-1046.18">text <entity id="X96-1046.19">retrieval</entity></entity> , while the emphasis on <entity id="X96-1046.20">individual</entity> <entity id="X96-1046.21">experiments</entity> <entity id="X96-1046.22">evaluated</entity> in a <entity id="X96-1046.23">common</entity> setting has proven to be a major <entity id="X96-1046.24">strength</entity> of TREC. The <entity id="X96-1046.25">test</entity> <entity id="X96-1046.26">design</entity> and <entity id="X96-1046.27">test</entity> <entity id="X96-1046.28">collection</entity> used for <entity id="X96-1046.29">document</entity> <entity id="X96-1046.30">detection</entity> in TIPSTER was also used in TREC. The <entity id="X96-1046.31">participants</entity> ran the various <entity id="X96-1046.32">tasks</entity> , sent <entity id="X96-1046.33">results</entity> into NIST for <entity id="X96-1046.34">evaluation</entity> , presented the <entity id="X96-1046.35">results</entity> at the TREC conferences, and submitted <entity id="X96-1046.36">papers</entity> for a <entity id="X96-1046.37">proceedings</entity> . The <entity id="X96-1046.38">test</entity> <entity id="X96-1046.39">collection</entity> consists of over 1 million <entity id="X96-1046.40">documents</entity> from diverse <entity id="X96-1046.41">full-text</entity> <entity id="X96-1046.42">sources</entity> , 250 <entity id="X96-1046.43">topics</entity> , and the set of relevant <entity id="X96-1046.44">documents</entity> or ""right answers"" to those <entity id="X96-1046.45">topics</entity> . A Spanish <entity id="X96-1046.46">collection</entity> has been built and used during TREC-3 and TREC-4, with a total of 50 <entity id="X96-1046.47">topics</entity> . TREC-1 <entity id="X96-1046.48">required</entity> significant <entity id="X96-1046.49">system</entity> rebuilding by most groups <entity id="X96-1046.50">due</entity> to the huge <entity id="X96-1046.51">increase</entity> in the <entity id="X96-1046.52">size</entity> of the <entity id="X96-1046.53">document</entity> <entity id="X96-1046.54">collection</entity> (from a traditional <entity id="X96-1046.55">test</entity> <entity id="X96-1046.56">collection</entity> of several megabytes in <entity id="X96-1046.57">size</entity> to the 2 gigabyte TIPSTER <entity id="X96-1046.58">collection</entity> ). The <entity id="X96-1046.59">results</entity> from TREC-2 showed significant <entity id="X96-1046.60">improvements</entity> over the TREC-1 <entity id="X96-1046.61">results</entity> , and should be viewed as the appropriate baseline representing state-of-the-art <entity id="X96-1046.62">retrieval</entity> <entity id="X96-1046.63">techniques</entity> as <entity id="X96-1046.64">scaled</entity> up to handling a 2 gigabyte <entity id="X96-1046.65">collection</entity> . TREC-3 therefore <entity id="X96-1046.66">provided</entity> the first opportunity for more <entity id="X96-1046.67">complex</entity> <entity id="X96-1046.68">experimentation</entity> . The major <entity id="X96-1046.69">experiments</entity> in TREC-3 <entity id="X96-1046.70">included</entity> the <entity id="X96-1046.71">development</entity> of <entity id="X96-1046.72">automatic</entity> <entity id="X96-1046.73">query expansion</entity> <entity id="X96-1046.74">techniques</entity> , the use of passages or sub-documents to <entity id="X96-1046.75">increase</entity> the <entity id="X96-1046.76">precision</entity> of <entity id="X96-1046.77">retrieval</entity> <entity id="X96-1046.78">results</entity> , and the use of the <entity id="X96-1046.79">training</entity> <entity id="X96-1046.80">information</entity> to select only the best <entity id="X96-1046.81">terms</entity> for <entity id="X96-1046.82">routing</entity> <entity id="X96-1046.83">queries</entity> . Some groups explored hybrid <entity id="X96-1046.84">approaches</entity> (such as the use of the Rocchio <entity id="X96-1046.85">methodology</entity> in <entity id="X96-1046.86">systems</entity> not using a <entity id="X96-1046.87">vector <entity id="X96-1046.88">space</entity> <entity id="X96-1046.89">model</entity></entity> ), and others tried <entity id="X96-1046.90">approaches</entity> that were radically different from their original <entity id="X96-1046.91">approaches</entity> . TREC-4 allowed a continuation of many of these <entity id="X96-1046.92">complex</entity> <entity id="X96-1046.93">experiments</entity> . The <entity id="X96-1046.94">topics</entity> were made much shorter and this change triggered extensive <entity id="X96-1046.95">investigations</entity> in <entity id="X96-1046.96">automatic</entity> <entity id="X96-1046.97">query <entity id="X96-1046.98">expansion</entity></entity> . There were also five new <entity id="X96-1046.99">tasks</entity> , <entity id="X96-1046.100">called</entity> tracks. These were added to <entity id="X96-1046.101">help</entity> <entity id="X96-1046.102">focus</entity> <entity id="X96-1046.103">research</entity> on certain known <entity id="X96-1046.104">problem</entity> <entity id="X96-1046.105">areas</entity> , and <entity id="X96-1046.106">included</entity> such <entity id="X96-1046.107">issues</entity> as investigating <entity id="X96-1046.108">searching</entity> as an interactive <entity id="X96-1046.109">task</entity> by examining the <entity id="X96-1046.110">process</entity> as well as the <entity id="X96-1046.111">outcome</entity> , investigating <entity id="X96-1046.112">techniques</entity> for merging <entity id="X96-1046.113">results</entity> from the various TREC subcollections, examining the <entity id="X96-1046.114">effects</entity> of corrupted data, and <entity id="X96-1046.115">evaluating</entity> <entity id="X96-1046.116">routing</entity> <entity id="X96-1046.117">systems</entity> using a specific <entity id="X96-1046.118">effectiveness</entity> measure. Additionally more groups participated in a track for Spanish <entity id="X96-1046.119">retrieval</entity> . The TREC conferences have proven to be very successful, allowing broad <entity id="X96-1046.120">participation</entity> in the overall DARPA TIPSTER <entity id="X96-1046.121">effort</entity> , and causing widespread use of a very large <entity id="X96-1046.122">test</entity> <entity id="X96-1046.123">collection</entity> . All conferences have had very open, honest <entity id="X96-1046.124">discussions</entity> of technical <entity id="X96-1046.125">issues</entity> , and there have been large <entity id="X96-1046.126">amounts</entity> of ""<entity id="X96-1046.127">cross-fertilization</entity> "" of ideas. This will be a continuing <entity id="X96-1046.128">effort</entity> , with a TREC-5 conference scheduled in November of 1996. "
</abstract>


</text>

<text id="W93-0110">
<title>
Acquiring Predicate- <entity id="W93-0110.1">Argument</entity> <entity id="W93-0110.2">Mapping</entity> <entity id="W93-0110.3">Information</entity> From Multilingual <entity id="W93-0110.4">Texts</entity></title>
<abstract>
This <entity id="W93-0110.5">paper</entity> discusses <entity id="W93-0110.6">automatic</entity> <entity id="W93-0110.7">acquisition</entity> of <entity id="W93-0110.8">predicate-argument</entity> <entity id="W93-0110.9">mapping</entity> <entity id="W93-0110.10">information</entity> from multilingual <entity id="W93-0110.11">texts</entity> . The <entity id="W93-0110.12">lexicon</entity> of our <entity id="W93-0110.13">NLP <entity id="W93-0110.14">system</entity></entity> <entity id="W93-0110.15">abstracts</entity> the <entity id="W93-0110.16">language-dependent</entity> <entity id="W93-0110.17">portion</entity> of <entity id="W93-0110.18">predicate-argument</entity> <entity id="W93-0110.19">mapping</entity> <entity id="W93-0110.20">information</entity> from the <entity id="W93-0110.21">core</entity> meaning of <entity id="W93-0110.22">verb</entity> senses (i.e.
</abstract>


</text>

<text id="W94-0203">
<title>
Constraints, Exceptions And Representations
</title>
<abstract>
This <entity id="W94-0203.1">paper</entity> shows that <entity id="W94-0203.2">default-based</entity> <entity id="W94-0203.3">phonologies</entity> have the potential to capture morphophonological generalisations which cannot be captured by <entity id="W94-0203.4">non-default</entity> <entity id="W94-0203.5">theories</entity> . In achieving this <entity id="W94-0203.6">result</entity> , I offer a characterisation of Underspecification <entity id="W94-0203.7">Theory</entity> and Optimality <entity id="W94-0203.8">Theory</entity> in <entity id="W94-0203.9">terms</entity> of their <entity id="W94-0203.10">methods</entity> for <entity id="W94-0203.11">ordering</entity> <entity id="W94-0203.12">defaults</entity> . The <entity id="W94-0203.13">result</entity> means that <entity id="W94-0203.14">machine</entity> learning <entity id="W94-0203.15">techniques</entity> for building declarative <entity id="W94-0203.16">analyses</entity> may not <entity id="W94-0203.17">provide</entity> an adequate <entity id="W94-0203.18">basis</entity> for morphophonological <entity id="W94-0203.19">analysis</entity> .
</abstract>


</text>

<abstract></abstract>

<text id="W97-1106">
<title>
A Czech Morphological <entity id="W97-1106.1">Lexicon</entity></title>
<abstract>
In this <entity id="W97-1106.2">paper</entity> , a <entity id="W97-1106.3">treatment</entity> of Czech phonological <entity id="W97-1106.4">rules</entity> in <entity id="W97-1106.5">two-level</entity> <entity id="W97-1106.6">morphology</entity> <entity id="W97-1106.7">approach</entity> is described . First the possible phonological <entity id="W97-1106.8">alternations</entity> in Czech are <entity id="W97-1106.9">listed</entity> and then their <entity id="W97-1106.10">treatment</entity> in a <entity id="W97-1106.11">practical application</entity> of a Czech morphological <entity id="W97-1106.12">lexicon</entity> .
</abstract>


</text>

<text id="W98-0208">
<title><entity id="W98-0208.1">Semantic</entity> <entity id="W98-0208.2">Visualization</entity></title>
<abstract>
This <entity id="W98-0208.3">paper</entity> summarizes several <entity id="W98-0208.4">initiatives</entity> at MITRE that are investigating the <entity id="W98-0208.5">visualization</entity> of a range of <entity id="W98-0208.6">content</entity> . We present <entity id="W98-0208.7">results</entity> of our work in relevancy <entity id="W98-0208.8">visualization</entity> , <entity id="W98-0208.9">news</entity> <entity id="W98-0208.10">visualization</entity> , world <entity id="W98-0208.11">events</entity> <entity id="W98-0208.12">visualization</entity> and sensor/battlefield <entity id="W98-0208.13">visualization</entity> to enhance <entity id="W98-0208.14">user</entity> <entity id="W98-0208.15">interaction</entity> in <entity id="W98-0208.16">information access</entity> and <entity id="W98-0208.17">exploitation</entity> <entity id="W98-0208.18">tasks</entity> . We summarize several <entity id="W98-0208.19">initiatives</entity> we are currently pursuing and enumerate unsolved <entity id="W98-0208.20">problems</entity> .
</abstract>


</text>

<text id="W98-0613">
<title>
Nominal Metonymy <entity id="W98-0613.1">Processing</entity></title> 
<abstract><entity id="W98-0613.2">Abstract</entity> . We argue for the <entity id="W98-0613.3">necessity</entity> of <entity id="W98-0613.4">resolution</entity> of metonymies for nominals (and other <entity id="W98-0613.5">cases</entity> ) in the <entity id="W98-0613.6">context</entity> of <entity id="W98-0613.7">semantics-based</entity> <entity id="W98-0613.8">machine translation</entity> . By using an <entity id="W98-0613.9">ontology</entity> as a <entity id="W98-0613.10">search space</entity> , we are able to identify and resolve m
tonymie <entity id="W98-0613.11">expressions</entity> with significant <entity id="W98-0613.12">accuracy</entity> , both for a pre-deterrnined <entity id="W98-0613.13">inventory</entity> of metonymie <entity id="W98-0613.14">types</entity> and for previously unseen <entity id="W98-0613.15">cases</entity> . The <entity id="W98-0613.16">entity</entity> replaced by the metonymy is made explicitly available in our meaning <entity id="W98-0613.17">representation</entity> , to <entity id="W98-0613.18">support</entity> <entity id="W98-0613.19">translation</entity> , anaphora, and other <entity id="W98-0613.20">mechanisms</entity> .
</abstract>


</text>

<text id="W98-0802">
<title>
Towards Multimodal <entity id="W98-0802.1">Spoken Language</entity> <entity id="W98-0802.2">Corpora</entity> : TransTool And SyncTool
</title>
<abstract>
This <entity id="W98-0802.3">paper</entity> argues for the <entity id="W98-0802.4">usefulness</entity> of multimodal spoken <entity id="W98-0802.5">language</entity> <entity id="W98-0802.6">corpora</entity> and specifies <entity id="W98-0802.7">components</entity> of a <entity id="W98-0802.8">platform</entity> for the <entity id="W98-0802.9">creation</entity> , <entity id="W98-0802.10">maintenance</entity> and <entity id="W98-0802.11">exploitation</entity> of such <entity id="W98-0802.12">corpora</entity> . Two of the <entity id="W98-0802.13">components</entity> , which have already been <entity id="W98-0802.14">implemented</entity> as <entity id="W98-0802.15">prototypes</entity> , are described in more <entity id="W98-0802.16">detail</entity> : TransTool and SyncTool. TransTool is a <entity id="W98-0802.17">transcription</entity> editor meant to facilitate and partially automate the <entity id="W98-0802.18">task</entity> of a human transcriber, while SyncTool is a <entity id="W98-0802.19">tool</entity> for aligning the <entity id="W98-0802.20">resulting</entity> <entity id="W98-0802.21">transcriptions</entity> with a digitized audio and <entity id="W98-0802.22">video</entity> <entity id="W98-0802.23">recording</entity> in <entity id="W98-0802.24">order</entity> to allow synchronized <entity id="W98-0802.25">presentation</entity> of different <entity id="W98-0802.26">representations</entity> (e.g., <entity id="W98-0802.27">text</entity> , audio, <entity id="W98-0802.28">video</entity> , acoustic <entity id="W98-0802.29">analysis</entity> ). Finally, a brief <entity id="W98-0802.30">comparison</entity> is made between these <entity id="W98-0802.31">tools</entity> and other <entity id="W98-0802.32">programs</entity> <entity id="W98-0802.33">developed</entity> for similar <entity id="W98-0802.34">purposes</entity> .
</abstract>


</text>

<text id="W98-1001">
<title>
Discovering <entity id="W98-1001.1">Lexical Information</entity> By <entity id="W98-1001.2">Tagging</entity> Arabic <entity id="W98-1001.3">Newspaper</entity> <entity id="W98-1001.4">Text</entity></title>
<abstract>
In this <entity id="W98-1001.5">paper</entity> we describe a <entity id="W98-1001.6">system</entity> for building an Arabic <entity id="W98-1001.7">lexicon</entity> automatically by <entity id="W98-1001.8">tagging</entity> Arabic <entity id="W98-1001.9">newspaper</entity> <entity id="W98-1001.10">text</entity> . In this <entity id="W98-1001.11">system</entity> we are using several <entity id="W98-1001.12">techniques</entity> for <entity id="W98-1001.13">tagging</entity> the <entity id="W98-1001.14">words</entity> in the <entity id="W98-1001.15">text</entity> and <entity id="W98-1001.16">figuring</entity> out their <entity id="W98-1001.17">types</entity> and their <entity id="W98-1001.18">features</entity> . The major <entity id="W98-1001.19">techniques</entity> that we are using are: finding <entity id="W98-1001.20">phrases</entity> , analyzing the affixes of the <entity id="W98-1001.21">words</entity> , and analyzing their <entity id="W98-1001.22">patterns</entity> . Proper <entity id="W98-1001.23">nouns</entity> are particularly difficult to identify in the Arabic <entity id="W98-1001.24">language</entity> ; we describe <entity id="W98-1001.25">techniques</entity> for isolating them.
</abstract>


</text>

<text id="W99-0310">
<title>
A <entity id="W99-0310.1">Recognition-</entity> <entity id="W99-0310.2">Based</entity> Meta- <entity id="W99-0310.3">Scheme</entity> For <entity id="W99-0310.4">Dialogue</entity> Acts Annotation
</title>
<abstract>
The <entity id="W99-0310.5">paper</entity> describes a new formal <entity id="W99-0310.6">framework</entity> for <entity id="W99-0310.7">comparison</entity> , <entity id="W99-0310.8">design</entity> and standardization of annotation <entity id="W99-0310.9">schemes</entity> for <entity id="W99-0310.10">dialogue acts</entity> . The <entity id="W99-0310.11">framework</entity> takes a <entity id="W99-0310.12"><entity id="W99-0310.13">recognition-based</entity> <entity id="W99-0310.14">approach</entity></entity> to <entity id="W99-0310.15">dialogue</entity> <entity id="W99-0310.16">tagging</entity> and defines four independent <entity id="W99-0310.17">taxonomies</entity> of <entity id="W99-0310.18">tags</entity> , one for each orthogonal <entity id="W99-0310.19">dimension</entity> of linguistic and contextual <entity id="W99-0310.20">analysis</entity> assumed to have a bearing on <entity id="W99-0310.21">identification</entity> of illocutionary acts. The <entity id="W99-0310.22">advantages</entity> and <entity id="W99-0310.23">limitations</entity> of this <entity id="W99-0310.24">proposal</entity> over other previous attempts are discussed and concretely exemplified.
</abstract>


</text>

<text id="W00-0409">
<title>
Multi- <entity id="W00-0409.1">Document Summarization</entity> By Visualizing Topical <entity id="W00-0409.2">Content</entity></title> 
<abstract>Mani , Inderjeet; House, David ; Klein , Gary ; Hirschman , Lynette ; Firmin <entity id="W00-0409.3">Hand</entity> , Therese ; Sundheim , Beth M. ,The TIPSTER SUMMAC <entity id="W00-0409.4">Text Summarization</entity> <entity id="W00-0409.5">Evaluation</entity> ,Conference Of The European <entity id="W00-0409.6">Association</entity> For <entity id="W00-0409.7">Computation</entity> al <entity id="W00-0409.8">Linguistics</entity> ,1999 *** Nagao , Katashi; Hasida, Koiti, <entity id="W00-0409.9">Automatic</entity> <entity id="W00-0409.10">Text Summarization</entity> <entity id="W00-0409.11">Based</entity> on the Global <entity id="W00-0409.12">Document</entity> Annotation,COLING-ACL,1998***Power, Richard ; Scott , Donia R.,Multilingual Authoring using <entity id="W00-0409.13">Feedback</entity> <entity id="W00-0409.14">Texts</entity> ,COLING-ACL,1998***Mani, Inderjeet; Gates , Barbara ; Bloedorn , Eric ,Improving Summaries By Revising Them,Annual Meeting Of The <entity id="W00-0409.15">Association</entity> For <entity id="W00-0409.16">Computation</entity> al <entity id="W00-0409.17">Linguistics</entity> ,1999</abstract>


</text>

<text id="W00-1009">
<title>
A <entity id="W00-1009.1">Common</entity> <entity id="W00-1009.2">Theory</entity> Of <entity id="W00-1009.3">Information</entity> <entity id="W00-1009.4">Fusion</entity> From Multiple <entity id="W00-1009.5">Text</entity> <entity id="W00-1009.6">Sources</entity> <entity id="W00-1009.7">Step</entity> One: <entity id="W00-1009.8">Cross-</entity> <entity id="W00-1009.9">Document</entity> <entity id="W00-1009.10">Structure</entity></title>
<abstract>
We introduce CST ( <entity id="W00-1009.11">cross-document</entity> <entity id="W00-1009.12">structure</entity> <entity id="W00-1009.13">theory</entity> ), a <entity id="W00-1009.14">paradigm</entity> for <entity id="W00-1009.15">multi-document</entity> <entity id="W00-1009.16">analysis</entity> . CST takes into account the rhetorical <entity id="W00-1009.17">structure</entity> of <entity id="W00-1009.18">clusters</entity> of related textual <entity id="W00-1009.19">documents</entity> . We present a <entity id="W00-1009.20">taxonomy</entity> of <entity id="W00-1009.21">cross-document</entity> <entity id="W00-1009.22">relationships</entity> . We argue that CST can be the <entity id="W00-1009.23">basis</entity> for <entity id="W00-1009.24">multi-document <entity id="W00-1009.25">summarization</entity></entity> guided by <entity id="W00-1009.26">user</entity> <entity id="W00-1009.27">preferences</entity> for <entity id="W00-1009.28">summary</entity> <entity id="W00-1009.29">length</entity> , <entity id="W00-1009.30">information</entity> provenance, <entity id="W00-1009.31">cross-source</entity> <entity id="W00-1009.32">agreement</entity> , and chronological <entity id="W00-1009.33">ordering</entity> of facts.
</abstract>


</text>

<text id="W00-1309">
<title><entity id="W00-1309.1">Error-</entity> Driven HMM-Based <entity id="W00-1309.2">Chunk</entity> Tagger With <entity id="W00-1309.3">Context-</entity> Dependent <entity id="W00-1309.4">Lexicon</entity></title>
<abstract>
This <entity id="W00-1309.5">paper</entity> <entity id="W00-1309.6">proposes</entity> a new <entity id="W00-1309.7">error-driven</entity> HMM-based <entity id="W00-1309.8">text</entity> <entity id="W00-1309.9">chunk</entity> tagger with <entity id="W00-1309.10">context-dependent</entity> <entity id="W00-1309.11">lexicon</entity> . Compared with <entity id="W00-1309.12">standard</entity> HMM-based tagger, this tagger uses a new Hidden Markov Modelling <entity id="W00-1309.13">approach</entity> which incorporates more <entity id="W00-1309.14">contextual <entity id="W00-1309.15">information</entity></entity> into a <entity id="W00-1309.16">lexical <entity id="W00-1309.17">entry</entity></entity> . Moreover, an <entity id="W00-1309.18">error-driven</entity> <entity id="W00-1309.19">learning <entity id="W00-1309.20">approach</entity></entity> is adopted to decrease the <entity id="W00-1309.21">memory</entity> <entity id="W00-1309.22">requirement</entity> by keeping only positive <entity id="W00-1309.23">lexical entries</entity> and makes it possible to further incorporate more <entity id="W00-1309.24">context-dependent</entity> <entity id="W00-1309.25">lexical entries</entity> . <entity id="W00-1309.26">Experiments</entity> show that this <entity id="W00-1309.27">technique</entity> achieves overall <entity id="W00-1309.28">precision</entity> and <entity id="W00-1309.29">recall</entity> <entity id="W00-1309.30">rates</entity> of 93.40% and 93.95% for all <entity id="W00-1309.31">chunk</entity> <entity id="W00-1309.32">types</entity> , 93.60% and 94.64% for <entity id="W00-1309.33">noun phrases</entity> , and 94.64% and 94.75% for <entity id="W00-1309.34">verb</entity> <entity id="W00-1309.35">phrases</entity> when <entity id="W00-1309.36">trained</entity> on PENN WSJ TreeBank <entity id="W00-1309.37">section</entity> 00-19 and <entity id="W00-1309.38">tested</entity> on <entity id="W00-1309.39">section</entity> 20-24, while 25-fold <entity id="W00-1309.40">validation</entity> <entity id="W00-1309.41">experiments</entity> of PENN WSJ TreeBank show overall <entity id="W00-1309.42">precision</entity> and <entity id="W00-1309.43">recall</entity> <entity id="W00-1309.44">rates</entity> of 96.40% and 96.47% for all <entity id="W00-1309.45">chunk</entity> <entity id="W00-1309.46">types</entity> , 96.49% and 96.99% for <entity id="W00-1309.47">noun phrases</entity> , and 97.13% and 97.36% for <entity id="W00-1309.48">verb</entity> <entity id="W00-1309.49">phrases</entity> .
</abstract>


</text>

<text id="W00-1326">
<title>
One <entity id="W00-1326.1">Sense</entity> Per <entity id="W00-1326.2">Collocation</entity> And <entity id="W00-1326.3">Genre</entity> / <entity id="W00-1326.4">Topic</entity> Variations
</title>
<abstract>
This <entity id="W00-1326.5">paper</entity> revisits the one <entity id="W00-1326.6">sense</entity> per <entity id="W00-1326.7">collocation</entity> <entity id="W00-1326.8">hypothesis</entity> using fine-grained <entity id="W00-1326.9">sense</entity> <entity id="W00-1326.10">distinctions</entity> and two different <entity id="W00-1326.11">corpora</entity> . We show that the <entity id="W00-1326.12">hypothesis</entity> is weaker for fine-grained <entity id="W00-1326.13">sense</entity> <entity id="W00-1326.14">distinctions</entity> (70% vs. 99% <entity id="W00-1326.15">reported</entity> earlier on 2-way <entity id="W00-1326.16">ambiguities</entity> ). We also show that one <entity id="W00-1326.17">sense</entity> per <entity id="W00-1326.18">collocation</entity> does hold across <entity id="W00-1326.19">corpora</entity> , but that <entity id="W00-1326.20">collocations</entity> vary from one <entity id="W00-1326.21">corpus</entity> to the other, following <entity id="W00-1326.22">genre</entity> and <entity id="W00-1326.23">topic</entity> <entity id="W00-1326.24">variations</entity> . This explains the low <entity id="W00-1326.25">results</entity> when <entity id="W00-1326.26">performing</entity> <entity id="W00-1326.27">word sense disambiguation</entity> across <entity id="W00-1326.28">corpora</entity> . In fact, we demonstrate that when two independent <entity id="W00-1326.29">corpora</entity> share a related <entity id="W00-1326.30">genre</entity> / <entity id="W00-1326.31">topic</entity> , the <entity id="W00-1326.32">word sense disambiguation</entity> <entity id="W00-1326.33">results</entity> would be better. Future work on <entity id="W00-1326.34">word sense disambiguation</entity> will have to take into account <entity id="W00-1326.35">genre</entity> and <entity id="W00-1326.36">topic</entity> as important <entity id="W00-1326.37">parameters</entity> on their <entity id="W00-1326.38">models</entity> .
</abstract>


</text>

<text id="W01-0701">
<title>
Multidimensional <entity id="W01-0701.1">Transformation-</entity> <entity id="W01-0701.2">Based</entity> Learning
</title>
<abstract>
This <entity id="W01-0701.3">paper</entity> presents a novel <entity id="W01-0701.4">method</entity> that allows a <entity id="W01-0701.5">machine</entity> learning <entity id="W01-0701.6">algorithm</entity> following the <entity id="W01-0701.7">transformation-based</entity> <entity id="W01-0701.8">learning</entity> <entity id="W01-0701.9">paradigm</entity> ( Brill, 1995 ) to be <entity id="W01-0701.10">applied</entity> to multiple <entity id="W01-0701.11">classification tasks</entity> by <entity id="W01-0701.12">training</entity> jointly and simultaneously on all <entity id="W01-0701.13">fields</entity> . The <entity id="W01-0701.14">motivation</entity> for <entity id="W01-0701.15">constructing</entity> such a <entity id="W01-0701.16">system</entity> <entity id="W01-0701.17">stems</entity> from the <entity id="W01-0701.18">observation</entity> that many <entity id="W01-0701.19">tasks</entity> in <entity id="W01-0701.20">natural language processing</entity> are naturally composed of multiple subtasks which need to be resolved simultaneously; also <entity id="W01-0701.21">tasks</entity> usually learned in <entity id="W01-0701.22">isolation</entity> can possibly <entity id="W01-0701.23">benefit</entity> from being learned in a joint <entity id="W01-0701.24">framework</entity> , as the <entity id="W01-0701.25">signals</entity> for the extra <entity id="W01-0701.26">tasks</entity> usually constitute inductive <entity id="W01-0701.27">bias</entity> . The <entity id="W01-0701.28">proposed</entity> <entity id="W01-0701.29">algorithm</entity> is <entity id="W01-0701.30">evaluated</entity> in two <entity id="W01-0701.31">experiments</entity> : in one, the <entity id="W01-0701.32">system</entity> is used to jointly predict the <entity id="W01-0701.33">part-of-speech</entity> and <entity id="W01-0701.34">text</entity> <entity id="W01-0701.35">chunks</entity> /baseNP <entity id="W01-0701.36">chunks</entity> of an <entity id="W01-0701.37">English</entity> <entity id="W01-0701.38">corpus</entity> ; and in the second it is used to learn the joint <entity id="W01-0701.39">prediction</entity> of <entity id="W01-0701.40">word</entity> <entity id="W01-0701.41">segment</entity> <entity id="W01-0701.42">boundaries</entity> and <entity id="W01-0701.43">part-of-speech</entity> <entity id="W01-0701.44">tagging</entity> for <entity id="W01-0701.45">Chinese</entity> . The <entity id="W01-0701.46">results</entity> show that the simultaneous <entity id="W01-0701.47">learning</entity> of multiple <entity id="W01-0701.48">tasks</entity> does achieve an <entity id="W01-0701.49">improvement</entity> in each <entity id="W01-0701.50">task</entity> upon <entity id="W01-0701.51">training</entity> the same <entity id="W01-0701.52">tasks</entity> sequentially. The <entity id="W01-0701.53">part-of-speech</entity> <entity id="W01-0701.54">tagging</entity> <entity id="W01-0701.55">result</entity> of 96.63% is state-of-the-art for <entity id="W01-0701.56">individual</entity> <entity id="W01-0701.57">systems</entity> on the particular <entity id="W01-0701.58">train</entity> / <entity id="W01-0701.59">test</entity> split.
</abstract>


</text>

<text id="W02-0303">
<title><entity id="W02-0303.1">Contrast</entity> And <entity id="W02-0303.2">Variability</entity> In <entity id="W02-0303.3">Gene</entity> Names
</title>
<abstract>
We <entity id="W02-0303.4">studied</entity> <entity id="W02-0303.5">contrast</entity> and <entity id="W02-0303.6">variability</entity> in a <entity id="W02-0303.7">corpus</entity> of <entity id="W02-0303.8">gene</entity> <entity id="W02-0303.9">names</entity> to identify potential heuristics for use in <entity id="W02-0303.10">performing</entity> <entity id="W02-0303.11">entity</entity> <entity id="W02-0303.12">identification</entity> in the molecular <entity id="W02-0303.13">biology</entity> <entity id="W02-0303.14">domain</entity> . <entity id="W02-0303.15">Based</entity> on our <entity id="W02-0303.16">findings</entity> , we <entity id="W02-0303.17">developed</entity> heuristics for <entity id="W02-0303.18">mapping</entity> weakly <entity id="W02-0303.19">matching</entity> <entity id="W02-0303.20">gene</entity> <entity id="W02-0303.21">names</entity> to their official <entity id="W02-0303.22">gene</entity> <entity id="W02-0303.23">names</entity> . We then <entity id="W02-0303.24">tested</entity> these heuristics against a large body of Medline <entity id="W02-0303.25">abstracts</entity> , and found that using these heuristics can <entity id="W02-0303.26">increase</entity> <entity id="W02-0303.27">recall</entity> , with varying <entity id="W02-0303.28">levels</entity> of <entity id="W02-0303.29">precision</entity> . Our <entity id="W02-0303.30">findings</entity> also underscored the <entity id="W02-0303.31">importance</entity> of good <entity id="W02-0303.32">information retrieval</entity> and of the <entity id="W02-0303.33">ability</entity> to <entity id="W02-0303.34">disambiguate</entity> between <entity id="W02-0303.35">genes</entity> , <entity id="W02-0303.36">proteins</entity> , RNA, and a <entity id="W02-0303.37">variety</entity> of other <entity id="W02-0303.38">referents</entity> for <entity id="W02-0303.39">performing</entity> <entity id="W02-0303.40">entity</entity> <entity id="W02-0303.41">identification</entity> with high <entity id="W02-0303.42">precision</entity> .
</abstract>


</text>

<text id="W02-1706">
<title>
XML-Based NLP Tools For Analysing And Annotating Medical <entity id="W02-1706.1">Language</entity></title>
<abstract>
We describe the use of a <entity id="W02-1706.2">suite</entity> of highly flexible xml-based nlp <entity id="W02-1706.3">tools</entity> in a <entity id="W02-1706.4">project</entity> for <entity id="W02-1706.5">processing</entity> and interpreting <entity id="W02-1706.6">text</entity> in the medical <entity id="W02-1706.7">domain</entity> . The <entity id="W02-1706.8">main</entity> aim of the <entity id="W02-1706.9">paper</entity> is to demonstrate the central <entity id="W02-1706.10">role</entity> that xml mark-up and xml nlp <entity id="W02-1706.11">tools</entity> have played in the <entity id="W02-1706.12">analysis</entity> <entity id="W02-1706.13">process</entity> and to describe the resultant annotated <entity id="W02-1706.14">corpus</entity> of medline <entity id="W02-1706.15">abstracts</entity> . In <entity id="W02-1706.16">addition</entity> to the xml <entity id="W02-1706.17">tools</entity> , we have succeeded in integrating a <entity id="W02-1706.18">variety</entity> of non-xml 'off the shelf' nlp <entity id="W02-1706.19">tools</entity> into our <entity id="W02-1706.20">pipelines</entity> , so that their <entity id="W02-1706.21">output</entity> is added into the mark-up. We demonstrate the <entity id="W02-1706.22">utility</entity> of the annotations that <entity id="W02-1706.23">result</entity> in two ways. First, we investigate how they can be used to <entity id="W02-1706.24">improve</entity> <entity id="W02-1706.25">parse</entity> <entity id="W02-1706.26">coverage</entity> ofa <entity id="W02-1706.27">hand-crafted</entity> grammar that <entity id="W02-1706.28">generates</entity> <entity id="W02-1706.29">logical forms</entity> . And second, we investigate how they contribute to <entity id="W02-1706.30">automatic</entity> <entity id="W02-1706.31">lexical</entity> <entity id="W02-1706.32">semantic</entity> <entity id="W02-1706.33">acquisition</entity> <entity id="W02-1706.34">processes</entity> .
</abstract>


</text>

<text id="W03-0305">
<title>
Reducing <entity id="W03-0305.1">Parameter</entity> <entity id="W03-0305.2">Space</entity> For <entity id="W03-0305.3">Word Alignment</entity></title>
<abstract>
This <entity id="W03-0305.4">paper</entity> presents the <entity id="W03-0305.5">experimental</entity> <entity id="W03-0305.6">results</entity> of our attemps to reduce the <entity id="W03-0305.7">size</entity> of the <entity id="W03-0305.8">parameter</entity> <entity id="W03-0305.9">space</entity> in <entity id="W03-0305.10">word alignment</entity> <entity id="W03-0305.11">algorithm</entity> . We use <entity id="W03-0305.12">IBM Model</entity> 4 as a baseline. In <entity id="W03-0305.13">order</entity> to reduce the <entity id="W03-0305.14">parameter</entity> <entity id="W03-0305.15">space</entity> , we pre-processed the <entity id="W03-0305.16">training <entity id="W03-0305.17">corpus</entity></entity> using a <entity id="W03-0305.18">word</entity> lemmatizer and a bilingual <entity id="W03-0305.19">term</entity> <entity id="W03-0305.20">extraction</entity> <entity id="W03-0305.21">algorithm</entity> . Using these additional <entity id="W03-0305.22">components</entity> , we obtained an <entity id="W03-0305.23">improvement</entity> in the <entity id="W03-0305.24">alignment</entity> <entity id="W03-0305.25">error <entity id="W03-0305.26">rate</entity></entity> .
</abstract>


</text>

<text id="W03-0902">
<title><entity id="W03-0902.1">Extracting</entity> And <entity id="W03-0902.2">Evaluating</entity> General <entity id="W03-0902.3">World <entity id="W03-0902.4">Knowledge</entity></entity> From The <entity id="W03-0902.5">Brown <entity id="W03-0902.6">Corpus</entity></entity></title>
<abstract>
"We have been <entity id="W03-0902.7">developing</entity> <entity id="W03-0902.8">techniques</entity> for <entity id="W03-0902.9">extracting</entity> general <entity id="W03-0902.10">world knowledge</entity> from miscellaneous <entity id="W03-0902.11">texts</entity> by a <entity id="W03-0902.12">process</entity> of approximate <entity id="W03-0902.13">interpretation</entity> and <entity id="W03-0902.14">abstraction</entity> , <entity id="W03-0902.15">focusing</entity> initially on the <entity id="W03-0902.16">Brown corpus</entity> . We <entity id="W03-0902.17">apply</entity> interpretive <entity id="W03-0902.18">rules</entity> to clausal <entity id="W03-0902.19">patterns</entity> and <entity id="W03-0902.20">patterns</entity> of <entity id="W03-0902.21">modification</entity> , and concurrently <entity id="W03-0902.22">abstract</entity> general ""possi-bilistic"" propositions from the <entity id="W03-0902.23">resulting</entity> <entity id="W03-0902.24">formulas</entity> . Two <entity id="W03-0902.25">examples</entity> are ""A person may believe a proposition"", and ""Children may live with <entity id="W03-0902.26">relatives</entity> "". Our <entity id="W03-0902.27">methods</entity> currently <entity id="W03-0902.28">yield</entity> over 117,000 such propositions (of <entity id="W03-0902.29">variable</entity> <entity id="W03-0902.30">quality</entity> ) for the <entity id="W03-0902.31">Brown <entity id="W03-0902.32">corpus</entity></entity> (more than 2 per <entity id="W03-0902.33">sentence</entity> ). We <entity id="W03-0902.34">report</entity> here on our <entity id="W03-0902.35">efforts</entity> to <entity id="W03-0902.36">evaluate</entity> these <entity id="W03-0902.37">results</entity> with a <entity id="W03-0902.38">judging</entity> <entity id="W03-0902.39">scheme</entity> aimed at determining how many ofthese propositions pass muster as ""reasonable general <entity id="W03-0902.40">claims</entity> "" about the world in the <entity id="W03-0902.41">opinion</entity> of humanjudges. We find that nearly 60% of the <entity id="W03-0902.42">extracted</entity> propositions are favorably <entity id="W03-0902.43">judged</entity> according to our <entity id="W03-0902.44">scheme</entity> by any given <entity id="W03-0902.45">judge</entity> . The <entity id="W03-0902.46">percentage</entity> unanimously <entity id="W03-0902.47">judged</entity> to be reasonable <entity id="W03-0902.48">claims</entity> by multiple <entity id="W03-0902.49">judges</entity> is lower, but still sufficiently high to suggest that our <entity id="W03-0902.50">techniques</entity> may be of some use in tackling the long-standing ""<entity id="W03-0902.51">knowledge</entity> <entity id="W03-0902.52">acquisition</entity> bottleneck"" in AI. "
</abstract>


</text>

<text id="W03-1026">
<title>
How To Get A <entity id="W03-1026.1">Chinese</entity> <entity id="W03-1026.2">Name</entity> ( <entity id="W03-1026.3">Entity</entity> ): Segmentation And <entity id="W03-1026.4">Combination</entity> Issues
</title>
<abstract>
When building a <entity id="W03-1026.5">Chinese</entity> <entity id="W03-1026.6">named</entity> <entity id="W03-1026.7">entity</entity> <entity id="W03-1026.8">recognition system</entity> , one must <entity id="W03-1026.9">deal</entity> with certain <entity id="W03-1026.10">language-specific</entity> <entity id="W03-1026.11">issues</entity> such as whether the <entity id="W03-1026.12">model</entity> should be <entity id="W03-1026.13">based</entity> on characters or <entity id="W03-1026.14">words</entity> . While there is no unique answer to this <entity id="W03-1026.15">question</entity> , we discuss in <entity id="W03-1026.16">detail</entity> <entity id="W03-1026.17">advantages</entity> and disadvantages of each <entity id="W03-1026.18">model</entity> , identify <entity id="W03-1026.19">problems</entity> in segmentation and suggest possible <entity id="W03-1026.20">solutions</entity> , presenting our <entity id="W03-1026.21">observations</entity> , <entity id="W03-1026.22">analysis</entity> , and <entity id="W03-1026.23">experimental</entity> <entity id="W03-1026.24">results</entity> . The second <entity id="W03-1026.25">topic</entity> of this <entity id="W03-1026.26">paper</entity> is <entity id="W03-1026.27">classifier</entity> <entity id="W03-1026.28">combination</entity> . We present and describe four <entity id="W03-1026.29">classifiers</entity> for <entity id="W03-1026.30">Chinese</entity> <entity id="W03-1026.31">named</entity> <entity id="W03-1026.32">entity</entity> <entity id="W03-1026.33">recognition</entity> and describe various <entity id="W03-1026.34">methods</entity> for combining their <entity id="W03-1026.35">outputs</entity> . The <entity id="W03-1026.36">results</entity> demonstrate that <entity id="W03-1026.37">classifier</entity> <entity id="W03-1026.38">combination</entity> is an effective <entity id="W03-1026.39">technique</entity> of <entity id="W03-1026.40">improving</entity> <entity id="W03-1026.41">system</entity> <entity id="W03-1026.42">performance</entity> : <entity id="W03-1026.43">experiments</entity> over a large annotated <entity id="W03-1026.44">corpus</entity> of fine-grained <entity id="W03-1026.45">entity types</entity> exhibit a 10% <entity id="W03-1026.46">relative</entity> <entity id="W03-1026.47">reduction</entity> in F-measure <entity id="W03-1026.48">error</entity> .
</abstract>


</text>

<text id="W03-1101">
<title>
Improving <entity id="W03-1101.1">Summarization</entity> <entity id="W03-1101.2">Performance</entity> By <entity id="W03-1101.3">Sentence</entity> <entity id="W03-1101.4">Compression</entity> - A <entity id="W03-1101.5">Pilot</entity> <entity id="W03-1101.6">Study</entity></title>
<abstract>
In this <entity id="W03-1101.7">paper</entity> we <entity id="W03-1101.8">study</entity> the <entity id="W03-1101.9">effectiveness</entity> of <entity id="W03-1101.10">applying</entity> <entity id="W03-1101.11">sentence</entity> <entity id="W03-1101.12">compression</entity> on an <entity id="W03-1101.13">extraction</entity> <entity id="W03-1101.14">based</entity> multi-document <entity id="W03-1101.15">summarization system</entity> . Our <entity id="W03-1101.16">results</entity> show that pure <entity id="W03-1101.17">syntactic-based</entity> <entity id="W03-1101.18">compression</entity> does not <entity id="W03-1101.19">improve</entity> <entity id="W03-1101.20">system</entity> <entity id="W03-1101.21">performance</entity> . <entity id="W03-1101.22">Topic</entity> signature-based reranking of compressed <entity id="W03-1101.23">sentences</entity> does not <entity id="W03-1101.24">help</entity> much either. However reranking using an <entity id="W03-1101.25">oracle</entity> showed a significant <entity id="W03-1101.26">improvement</entity> remains possible. Keywords: <entity id="W03-1101.27">Text Summarization</entity> , <entity id="W03-1101.28">Sentence</entity> <entity id="W03-1101.29">Extraction</entity> , <entity id="W03-1101.30">Sentence</entity> <entity id="W03-1101.31">Compression</entity> , <entity id="W03-1101.32">Evaluation</entity> .
</abstract>


</text>

<text id="W99-0611">
<title><entity id="W99-0611.1">Noun Phrase</entity> Coreference As Clustering
</title>
<abstract>
This <entity id="W99-0611.2">paper</entity> introduces a new, unsupervised <entity id="W99-0611.3">algorithm</entity> for <entity id="W99-0611.4">noun phrase</entity> <entity id="W99-0611.5">coreference <entity id="W99-0611.6">resolution</entity></entity> . It differs from existing <entity id="W99-0611.7">methods</entity> in that it views <entity id="W99-0611.8">coreference resolution</entity> as a <entity id="W99-0611.9">clustering</entity> <entity id="W99-0611.10">task</entity> . In an <entity id="W99-0611.11">evaluation</entity> on the MUC-6 <entity id="W99-0611.12">coreference resolution</entity> <entity id="W99-0611.13">corpus</entity> , the <entity id="W99-0611.14">algorithm</entity> achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) <entity id="W99-0611.15">systems</entity> in the MUC-6 <entity id="W99-0611.16">evaluation</entity> . More importantly, the <entity id="W99-0611.17">clustering</entity> <entity id="W99-0611.18">approach</entity> outperforms the only MUC-6 <entity id="W99-0611.19">system</entity> to treat <entity id="W99-0611.20">coreference resolution</entity> as a learning <entity id="W99-0611.21">problem</entity> . The <entity id="W99-0611.22">clustering</entity> <entity id="W99-0611.23">algorithm</entity> appears to <entity id="W99-0611.24">provide</entity> a flexible <entity id="W99-0611.25">mechanism</entity> for coordinating the <entity id="W99-0611.26">application</entity> of <entity id="W99-0611.27">context-independent</entity> and <entity id="W99-0611.28">context-dependent</entity> <entity id="W99-0611.29">constraints</entity> and <entity id="W99-0611.30">preferences</entity> for accurate <entity id="W99-0611.31">partitioning</entity> of <entity id="W99-0611.32">noun phrases</entity> into coreference <entity id="W99-0611.33">equivalence classes</entity> .
</abstract>


</text>

<text id="W08-0215">
<title>
Psychocomputational <entity id="W08-0215.1">Linguistics</entity> : A Gateway to the <entity id="W08-0215.2">Computational Linguistics</entity> Curriculum
</title>
<abstract><entity id="W08-0215.3">Computational</entity> <entity id="W08-0215.4">modeling</entity> of <entity id="W08-0215.5">human language</entity> <entity id="W08-0215.6">processes</entity> is a small but growing subfield of <entity id="W08-0215.7">computational <entity id="W08-0215.8">linguistics</entity></entity> . This <entity id="W08-0215.9">paper</entity> describes a course that makes use of recent <entity id="W08-0215.10">research</entity> in psychocomputational <entity id="W08-0215.11">modeling</entity> as a <entity id="W08-0215.12">framework</entity> to introduce a <entity id="W08-0215.13">number</entity> of <entity id="W08-0215.14">mainstream</entity> <entity id="W08-0215.15">computational linguistics</entity> <entity id="W08-0215.16">concepts</entity> to an audience of <entity id="W08-0215.17">linguistics</entity> , <entity id="W08-0215.18">cognitive science</entity> and <entity id="W08-0215.19">computer science</entity> doctoral students. The emphasis on what I take to be the largely interdisciplinary <entity id="W08-0215.20">nature</entity> of <entity id="W08-0215.21">computational linguistics</entity> is particularly germane for the <entity id="W08-0215.22">computer science</entity> students. Since 2002  the course has been taught three <entity id="W08-0215.23">times</entity> under the auspices of the MA/PhD <entity id="W08-0215.24">program</entity> in <entity id="W08-0215.25">Linguistics</entity> at The City <entity id="W08-0215.26">University</entity> of New York's <entity id="W08-0215.27">Graduate</entity> <entity id="W08-0215.28">Center</entity> . A brief <entity id="W08-0215.29">description</entity> of some of the students' <entity id="W08-0215.30">experiences</entity> after having taken the course is also <entity id="W08-0215.31">provided</entity> .
</abstract>


</text>

<text id="J80-3003">
<title>
A Plan- <entity id="J80-3003.1">Based</entity> <entity id="J80-3003.2">Analysis</entity> Of Indirect <entity id="J80-3003.3">Speech Act</entity></title>
<abstract>
We <entity id="J80-3003.4">propose</entity> an account of indirect <entity id="J80-3003.5">forms</entity> of <entity id="J80-3003.6">speech acts</entity> to request and inform <entity id="J80-3003.7">based</entity> on the <entity id="J80-3003.8">hypothesis</entity> that <entity id="J80-3003.9">language</entity> <entity id="J80-3003.10">users</entity> can recognize <entity id="J80-3003.11">actions</entity> being <entity id="J80-3003.12">performed</entity> by others, infer <entity id="J80-3003.13">goals</entity> being sought, and cooperate in their achievement. This cooperative <entity id="J80-3003.14">behaviour</entity> is independently motivated and may or may not be intended by speakers. If the hearer believes it is intended, he or she can recognize the <entity id="J80-3003.15">speech act</entity> as indirect; otherwise it is interpreted directly. Heuristics are suggested to decide among the <entity id="J80-3003.16">interpretations</entity> .
</abstract>


</text>

<text id="J92-4002">
<title>
Ambiguous <entity id="J92-4002.1">Noun</entity> Phrases In <entity id="J92-4002.2">Logical Form</entity></title>
<abstract>
	"Schubert , Lenhart K.; Pelletier , Francis Jeffry ,From <entity id="J92-4002.3">English</entity> To <entity id="J92-4002.4">Logic</entity> : <entity id="J92-4002.5">Context-</entity> Free <entity id="J92-4002.6">Computation</entity> Of ""Conventional"" Logical <entity id="J92-4002.7">Translation</entity> ,American <entity id="J92-4002.8">Journal</entity> Of <entity id="J92-4002.9">Computation</entity> al <entity id="J92-4002.10">Linguistics</entity> ,1982 *** Hobbs , Jerry R. ,An Improper <entity id="J92-4002.11">Treatment</entity> Of <entity id="J92-4002.12">Quantification</entity> In Ordinary <entity id="J92-4002.13">English</entity> ,Annual Meeting Of The <entity id="J92-4002.14">Association</entity> For <entity id="J92-4002.15">Computation</entity> al <entity id="J92-4002.16">Linguistics</entity> ,1983 *** Pollack , Martha E. ; Pereira , Fernando ,An Integrated <entity id="J92-4002.17">Framework</entity> For <entity id="J92-4002.18">Semantic</entity> And Pragmatic <entity id="J92-4002.19">Interpretation</entity> ,Annual Meeting Of The <entity id="J92-4002.20">Association</entity> For <entity id="J92-4002.21">Computation</entity> al <entity id="J92-4002.22">Linguistics</entity> ,1988 *** Alshawi , Hiyan ; <entity id="J92-4002.23">Van</entity> Eijck , Jan,Logical Forms In The <entity id="J92-4002.24">Core</entity> <entity id="J92-4002.25">Language</entity> <entity id="J92-4002.26">Engine</entity> ,Annual Meeting Of The <entity id="J92-4002.27">Association</entity> For <entity id="J92-4002.28">Computation</entity> al <entity id="J92-4002.29">Linguistics</entity> ,1989 "
</abstract>


</text>

<abstract></abstract>

<text id="J00-3001">
<title><entity id="J00-3001.1">Extracting</entity> The Lowest- <entity id="J00-3001.2">Frequency</entity> <entity id="J00-3001.3">Words</entity> : <entity id="J00-3001.4">Pitfalls</entity> And Possibilities
</title>
<abstract>
	Church, Kenneth Ward ; Hanks , Patrick , <entity id="J00-3001.5">Word</entity> <entity id="J00-3001.6">Association</entity> <entity id="J00-3001.7">Norms</entity> , <entity id="J00-3001.8">Mutual Information</entity> , And Lexicography, <entity id="J00-3001.9">Computation</entity> al <entity id="J00-3001.10">Linguistics</entity> ,1990 *** Dunning , Ted E. ,Accurate <entity id="J00-3001.11">Methods</entity> For The <entity id="J00-3001.12">Statistics</entity> Of Surprise And Coincidence, <entity id="J00-3001.13">Computation</entity> al <entity id="J00-3001.14">Linguistics</entity> ,1993 *** Smadja , Frank A. ,Retrieving Collocations From <entity id="J00-3001.15">Text</entity> : Xtract, <entity id="J00-3001.16">Computation</entity> al <entity id="J00-3001.17">Linguistics</entity> ,1993</abstract>


</text>

<text id="P98-2228">
<title><entity id="P98-2228.1">Word <entity id="P98-2228.2">Sense Disambiguation</entity></entity> using Optimised Combinations of <entity id="P98-2228.3">Knowledge Sources</entity></title> 
<abstract><entity id="P98-2228.4">Word sense disambiguation</entity> <entity id="P98-2228.5">algorithms</entity> , with few <entity id="P98-2228.6">exceptions</entity> , have made use of only one lexical <entity id="P98-2228.7">knowledge <entity id="P98-2228.8">source</entity></entity> . We describe a <entity id="P98-2228.9">system</entity> which <entity id="P98-2228.10">performs</entity> <entity id="P98-2228.11">word sense disambiguation</entity> on all <entity id="P98-2228.12">content <entity id="P98-2228.13">words</entity></entity> in free <entity id="P98-2228.14">text</entity> by combining different <entity id="P98-2228.15">knowledge sources</entity> : <entity id="P98-2228.16">semantic</entity> <entity id="P98-2228.17">preferences</entity> , <entity id="P98-2228.18">dictionary definitions</entity> and subject/ <entity id="P98-2228.19">domain</entity> <entity id="P98-2228.20">codes</entity> along with <entity id="P98-2228.21">part-of-speech</entity> <entity id="P98-2228.22">tags</entity> , optimised by means of a learning <entity id="P98-2228.23">algorithm</entity> . We also describe the <entity id="P98-2228.24">creation</entity> of a new <entity id="P98-2228.25">sense</entity> <entity id="P98-2228.26">tagged</entity> <entity id="P98-2228.27">corpus</entity> by combining existing <entity id="P98-2228.28">resources</entity> . Tested <entity id="P98-2228.29">accuracy</entity> of our <entity id="P98-2228.30">approach</entity> on this <entity id="P98-2228.31">corpus</entity> exceeds 92% , demonstrating the viability of <entity id="P98-2228.32">all-word</entity> <entity id="P98-2228.33">disambiguation</entity> rather than restricting oneself to a small <entity id="P98-2228.34">sample</entity> .
</abstract>


</text>

<text id="W08-1138">
<title>
GRAPH: The <entity id="W08-1138.1">Costs</entity> of <entity id="W08-1138.2">Redundancy</entity> in Referring Expressions
</title>
<abstract>
We describe a graph-based <entity id="W08-1138.3">generation <entity id="W08-1138.4">system</entity></entity> that participated in the Tuna attribute <entity id="W08-1138.5">selection</entity> and <entity id="W08-1138.6">realisation</entity> <entity id="W08-1138.7">task</entity> of the reg 2008 <entity id="W08-1138.8">Challenge</entity> . Using a stochastic <entity id="W08-1138.9">cost</entity> <entity id="W08-1138.10">function</entity> (with certain <entity id="W08-1138.11">properties</entity> for free), and trying attributes from cheapest to more expensive, the <entity id="W08-1138.12">system</entity> achieves overall .76 dice and .54 masi scores for attribute <entity id="W08-1138.13">selection</entity> on the <entity id="W08-1138.14">development</entity> set. For <entity id="W08-1138.15">realisation</entity> , it turns out that in some <entity id="W08-1138.16">cases</entity> higher attribute <entity id="W08-1138.17">selection</entity> <entity id="W08-1138.18">accuracy</entity> leads to larger <entity id="W08-1138.19">differences</entity> between <entity id="W08-1138.20">system-generated</entity> and human <entity id="W08-1138.21">descriptions</entity> .
</abstract>


</text>

<text id="W03-2805">
<title>
Colouring Summaries BLEU
</title>
<abstract>
In this <entity id="W03-2805.1">paper</entity> we attempt to <entity id="W03-2805.2">apply</entity> the IBM <entity id="W03-2805.3">algorithm</entity> , BLEU, to the <entity id="W03-2805.4">output</entity> of four different summarizers in <entity id="W03-2805.5">order</entity> to <entity id="W03-2805.6">perform</entity> an intrinsic <entity id="W03-2805.7">evaluation</entity> of their <entity id="W03-2805.8">output</entity> . The <entity id="W03-2805.9">objective</entity> of this <entity id="W03-2805.10">experiment</entity> is to explore whether a <entity id="W03-2805.11">metric</entity> , originally <entity id="W03-2805.12">developed</entity> for the <entity id="W03-2805.13">evaluation</entity> of <entity id="W03-2805.14">machine <entity id="W03-2805.15">translation</entity></entity> <entity id="W03-2805.16">output</entity> , could be used for assessing another <entity id="W03-2805.17">type</entity> of <entity id="W03-2805.18">output</entity> reliably. Changing the <entity id="W03-2805.19">type</entity> of <entity id="W03-2805.20">text</entity> to be <entity id="W03-2805.21">evaluated</entity> by BLEU into automatically <entity id="W03-2805.22">generated</entity> <entity id="W03-2805.23">extracts</entity> and setting the conditions and <entity id="W03-2805.24">parameters</entity> of the <entity id="W03-2805.25">evaluation</entity> <entity id="W03-2805.26">experiment</entity> according to the idiosyncrasies of the <entity id="W03-2805.27">task</entity> , we put the feasibility of <entity id="W03-2805.28">porting</entity> BLEU in different <entity id="W03-2805.29">Natural Language Processing</entity> <entity id="W03-2805.30">research</entity> <entity id="W03-2805.31">areas</entity> under <entity id="W03-2805.32">test</entity> . Furthermore, some important <entity id="W03-2805.33">conclusions</entity> relevant to the <entity id="W03-2805.34">resources</entity> needed for <entity id="W03-2805.35">evaluating</entity> <entity id="W03-2805.36">summaries</entity> have come up as a <entity id="W03-2805.37">side-effect</entity> of running the whole <entity id="W03-2805.38">experiment</entity> .
</abstract>


</text>

<text id="I08-1029">
<title><entity id="I08-1029.1">Automatic</entity> Prosodic Labeling with Conditional Random Fields and Rich Acoustic <entity id="I08-1029.2">Features</entity></title>
<abstract>
Many acoustic <entity id="I08-1029.3">approaches</entity> to prosodic labeling in <entity id="I08-1029.4">English</entity> have employed only local <entity id="I08-1029.5">classifiers</entity> , although <entity id="I08-1029.6">text-based</entity> <entity id="I08-1029.7">classification</entity> has employed some sequential <entity id="I08-1029.8">models</entity> . In this <entity id="I08-1029.9">paper</entity> we employ linear <entity id="I08-1029.10">chain</entity> and factorial <entity id="I08-1029.11">conditional random <entity id="I08-1029.12">fields</entity></entity> (CRFs) in <entity id="I08-1029.13">conjunction</entity> with rich, contextually-based prosodic <entity id="I08-1029.14">features</entity> , to exploit sequential <entity id="I08-1029.15">dependencies</entity> and to facilitate <entity id="I08-1029.16">integration</entity> with <entity id="I08-1029.17">lexical features</entity> . <entity id="I08-1029.18">Integration</entity> of <entity id="I08-1029.19">lexical</entity> and prosodic <entity id="I08-1029.20">features</entity> <entity id="I08-1029.21">improves</entity> <entity id="I08-1029.22">pitch</entity> accent <entity id="I08-1029.23">prediction</entity> over either <entity id="I08-1029.24">feature set</entity> alone, and for lower <entity id="I08-1029.25">accuracy</entity> <entity id="I08-1029.26">feature sets</entity> , factorial <entity id="I08-1029.27">CRF <entity id="I08-1029.28">models</entity></entity> can <entity id="I08-1029.29">improve</entity> over linear <entity id="I08-1029.30">chain</entity> <entity id="I08-1029.31">based</entity> <entity id="I08-1029.32">prediction</entity> of <entity id="I08-1029.33">pitch</entity> accent.
</abstract>


</text>

<text id="W04-0829">
<title>
WSD <entity id="W04-0829.1">Based</entity> On <entity id="W04-0829.2">Mutual Information</entity> And <entity id="W04-0829.3">Syntactic Patterns</entity></title>
<abstract>
This <entity id="W04-0829.4">paper</entity> describes a hybrid <entity id="W04-0829.5">system</entity> for WSD, presented to the <entity id="W04-0829.6">English</entity> <entity id="W04-0829.7">all-words</entity> and <entity id="W04-0829.8">lexical-sample</entity> <entity id="W04-0829.9">tasks</entity> , that relies on two different unsupervised <entity id="W04-0829.10">approaches</entity> . The first one selects the senses according to <entity id="W04-0829.11">mutual information</entity> <entity id="W04-0829.12">proximity</entity> between a <entity id="W04-0829.13">context word</entity> a <entity id="W04-0829.14">variant</entity> of the <entity id="W04-0829.15">sense</entity> . The second heuristic analyzes the <entity id="W04-0829.16">examples</entity> of use in the glosses of the senses so that <entity id="W04-0829.17">simple</entity> <entity id="W04-0829.18">syntactic patterns</entity> are inferred. This <entity id="W04-0829.19">patterns</entity> are <entity id="W04-0829.20">matched</entity> against the <entity id="W04-0829.21">disambiguation</entity> <entity id="W04-0829.22">contexts</entity> . We show that the first heuristic obtains a <entity id="W04-0829.23">precision</entity> and <entity id="W04-0829.24">recall</entity> of .58 and .35 respectively in the all <entity id="W04-0829.25">words</entity> <entity id="W04-0829.26">task</entity> while the second obtains .80 and .25. The high <entity id="W04-0829.27">precision</entity> obtained recommends deeper <entity id="W04-0829.28">research</entity> of the <entity id="W04-0829.29">techniques</entity> . <entity id="W04-0829.30">Results</entity> for the <entity id="W04-0829.31">lexical</entity> <entity id="W04-0829.32">sample</entity> <entity id="W04-0829.33">task</entity> are also <entity id="W04-0829.34">provided</entity> .
</abstract>


</text>

<text id="W04-0863">
<title>
Joining Forces To Resolve <entity id="W04-0863.1">Lexical</entity> <entity id="W04-0863.2">Ambiguity</entity> : East Meets West In Barcelona
</title>
<abstract>
"This <entity id="W04-0863.3">paper</entity> describes the <entity id="W04-0863.4">component</entity> <entity id="W04-0863.5">models</entity> and <entity id="W04-0863.6">combination</entity> <entity id="W04-0863.7">model</entity> built as a joint <entity id="W04-0863.8">effort</entity> between Swarthmore College, <entity id="W04-0863.9">Hong</entity> Kong PolyU, and HKUST. Though other <entity id="W04-0863.10">models</entity> described elsewhere contributed to the final <entity id="W04-0863.11">combination</entity> <entity id="W04-0863.12">model</entity> , this <entity id="W04-0863.13">paper</entity> <entity id="W04-0863.14">focuses</entity> solely on the joint <entity id="W04-0863.15">contributions</entity> to the ""Swat-HK"" <entity id="W04-0863.16">effort</entity> . "
</abstract>


</text>

<text id="W04-0914">
<title><entity id="W04-0914.1">Semantic</entity> <entity id="W04-0914.2">Forensics</entity> : An <entity id="W04-0914.3">Application</entity> Of Ontological <entity id="W04-0914.4">Semantics</entity> To <entity id="W04-0914.5">Information</entity> Assurance
</title>
<abstract>
"The <entity id="W04-0914.6">paper</entity> <entity id="W04-0914.7">deals</entity> with the latest <entity id="W04-0914.8">application</entity> of <entity id="W04-0914.9">natural language processing</entity> (NLP), specifically of ontological <entity id="W04-0914.10">semantics</entity> (ONSE) to <entity id="W04-0914.11">natural language</entity> <entity id="W04-0914.12">information</entity> assurance and security (NL IAS). It demonstrates how the existing ideas, <entity id="W04-0914.13">methods</entity> , and <entity id="W04-0914.14">resources</entity> of ontological <entity id="W04-0914.15">semantics</entity> can be <entity id="W04-0914.16">applied</entity> to detect deception in NL <entity id="W04-0914.17">text</entity> (and, eventually, in <entity id="W04-0914.18">data</entity> and other media as well). After stating the <entity id="W04-0914.19">problem</entity> , the <entity id="W04-0914.20">paper</entity> <entity id="W04-0914.21">proceeds</entity> to a brief <entity id="W04-0914.22">introduction</entity> to ONSE, followed by an equally brief <entity id="W04-0914.23">survey</entity> of our 5-year-old <entity id="W04-0914.24">effort</entity> in ""colonizing"" IAS. The <entity id="W04-0914.25">main</entity> <entity id="W04-0914.26">part</entity> of the <entity id="W04-0914.27">paper</entity> <entity id="W04-0914.28">deals</entity> with the <entity id="W04-0914.29">following</entity> <entity id="W04-0914.30">issues</entity> : 
 human deception <entity id="W04-0914.31">detection</entity> <entity id="W04-0914.32">abilities</entity> and NLP <entity id="W04-0914.33">modeling</entity> of it; 
 <entity id="W04-0914.34">manipulation</entity> of fact <entity id="W04-0914.35">repositories</entity> for this <entity id="W04-0914.36">purpose</entity> beyond the <entity id="W04-0914.37">current</entity> state of the art; 
 <entity id="W04-0914.38">acquisition</entity> of <entity id="W04-0914.39">scripts</entity> for <entity id="W04-0914.40">complex</entity> ontological <entity id="W04-0914.41">concepts</entity> ; 
 <entity id="W04-0914.42">degrees</entity> of lying <entity id="W04-0914.43">complexity</entity> and feasibility of their <entity id="W04-0914.44">automatic</entity> <entity id="W04-0914.45">detection</entity> . This is not a <entity id="W04-0914.46">report</entity> on a <entity id="W04-0914.47">system</entity> <entity id="W04-0914.48">implementation</entity> but rather an <entity id="W04-0914.49">application-establishing</entity> <entity id="W04-0914.50">proof-of-concept</entity> <entity id="W04-0914.51">effort</entity> <entity id="W04-0914.52">based</entity> on the algorithmic and <entity id="W04-0914.53">machine-tractable</entity> recombination and <entity id="W04-0914.54">extension</entity> of the previously <entity id="W04-0914.55">implemented</entity> ONSE <entity id="W04-0914.56">modules</entity> . The <entity id="W04-0914.57">strength</entity> of the <entity id="W04-0914.58">approach</entity> is that it emphasizes the use of the existing <entity id="W04-0914.59">NLP applications</entity> , with very few <entity id="W04-0914.60">domain-</entity> and <entity id="W04-0914.61">goal-specific</entity> adjustments, in a most promising and growing new <entity id="W04-0914.62">area</entity> of IAS. So, while clearly <entity id="W04-0914.63">dealing</entity> with a new <entity id="W04-0914.64">application</entity> , the <entity id="W04-0914.65">paper</entity> addresses theoretical and methodological <entity id="W04-0914.66">extensions</entity> of ONSE, as defined currently, that will be useful for other <entity id="W04-0914.67">applications</entity> as well. 1"
</abstract>


</text>

<text id="W04-1217">
<title>
Exploiting <entity id="W04-1217.1">Context</entity> For Biomedical <entity id="W04-1217.2">Entity</entity> <entity id="W04-1217.3">Recognition</entity> : From <entity id="W04-1217.4">Syntax</entity> To The Web
</title>
<abstract>
We describe a <entity id="W04-1217.5">machine</entity> learning <entity id="W04-1217.6">system</entity> for the <entity id="W04-1217.7">recognition</entity> of <entity id="W04-1217.8">names</entity> in biomedical <entity id="W04-1217.9">texts</entity> . The <entity id="W04-1217.10">system</entity> makes extensive use of local and <entity id="W04-1217.11">syntactic <entity id="W04-1217.12">features</entity></entity> within the <entity id="W04-1217.13">text</entity> , as well as external <entity id="W04-1217.14">resources</entity> <entity id="W04-1217.15">including</entity> the web and <entity id="W04-1217.16">gazetteers</entity> . It achieves an F-score of 70% on the Coling 2004  NLPBA/BioNLP shared <entity id="W04-1217.17">task</entity> of identifying five biomedical <entity id="W04-1217.18">named</entity> <entity id="W04-1217.19">entities</entity> in the GENIA <entity id="W04-1217.20">corpus</entity> .
</abstract>


</text>

<text id="W04-1221">
<title>
Biomedical <entity id="W04-1221.1">Named</entity> <entity id="W04-1221.2">Entity</entity> <entity id="W04-1221.3">Recognition</entity> Using Conditional Random Fields And Rich <entity id="W04-1221.4">Feature</entity> Sets
</title>
<abstract>
As the wealth of biomedical <entity id="W04-1221.5">knowledge</entity> in the <entity id="W04-1221.6">form</entity> of <entity id="W04-1221.7">literature</entity> <entity id="W04-1221.8">increases</entity> , there is a rising need for effective <entity id="W04-1221.9">natural language processing</entity> <entity id="W04-1221.10">tools</entity> to assist in organizing, curating, and retrieving this <entity id="W04-1221.11">information</entity> . To that end, <entity id="W04-1221.12">named</entity> <entity id="W04-1221.13">entity</entity> <entity id="W04-1221.14">recognition</entity> (the <entity id="W04-1221.15">task</entity> of identifying <entity id="W04-1221.16">words</entity> and <entity id="W04-1221.17">phrases</entity> in free <entity id="W04-1221.18">text</entity> that belong to certain <entity id="W04-1221.19">classes</entity> of interest) is an important <entity id="W04-1221.20">first step</entity> for many of these larger <entity id="W04-1221.21">information</entity> <entity id="W04-1221.22">management</entity> <entity id="W04-1221.23">goals</entity> . In recent years, much attention has been <entity id="W04-1221.24">focused</entity> on the <entity id="W04-1221.25">problem</entity> of recognizing <entity id="W04-1221.26">gene</entity> and <entity id="W04-1221.27">protein</entity> <entity id="W04-1221.28">mentions</entity> in biomedical <entity id="W04-1221.29">abstracts</entity> . This <entity id="W04-1221.30">paper</entity> presents a <entity id="W04-1221.31">framework</entity> for simultaneously recognizing <entity id="W04-1221.32">occurrences</entity> of <entity id="W04-1221.33">PROTEIN</entity> , DNA, RNA, CELL-LINE, <entity id="W04-1221.34">CELL-TYPE</entity> F1
</abstract>


</text>

<abstract></abstract>

<text id="W04-2903">
<title>
Audio Hot Spotting And <entity id="W04-2903.1">Retrieval</entity> Using Multiple <entity id="W04-2903.2">Features</entity></title>
<abstract>
This <entity id="W04-2903.3">paper</entity> <entity id="W04-2903.4">reports</entity> our on-going <entity id="W04-2903.5">efforts</entity> to exploit multiple <entity id="W04-2903.6">features</entity> derived from an audio <entity id="W04-2903.7">stream</entity> using <entity id="W04-2903.8">source</entity> material such as <entity id="W04-2903.9">broadcast news</entity> , teleconferences, and <entity id="W04-2903.10">meetings</entity> . These <entity id="W04-2903.11">features</entity> are derived from <entity id="W04-2903.12">algorithms</entity> <entity id="W04-2903.13">including</entity> <entity id="W04-2903.14">automatic speech recognition</entity> , <entity id="W04-2903.15">automatic</entity> <entity id="W04-2903.16">speech</entity> <entity id="W04-2903.17">indexing</entity> , speaker <entity id="W04-2903.18">identification</entity> , prosodic and audio <entity id="W04-2903.19">feature</entity> <entity id="W04-2903.20">extraction</entity> . We describe our <entity id="W04-2903.21">research</entity> <entity id="W04-2903.22">prototype</entity> - the Audio Hot Spotting <entity id="W04-2903.23">System</entity> -that allows <entity id="W04-2903.24">users</entity> to <entity id="W04-2903.25">query</entity> and retrieve <entity id="W04-2903.26">data</entity> from multimedia <entity id="W04-2903.27">sources</entity> utilizing these multiple <entity id="W04-2903.28">features</entity> . The <entity id="W04-2903.29">system</entity> aims to accurately find <entity id="W04-2903.30">segments</entity> of <entity id="W04-2903.31">user</entity> interest, i.e., audio hot spots within seconds of the actual <entity id="W04-2903.32">event</entity> . In <entity id="W04-2903.33">addition</entity> to spoken <entity id="W04-2903.34">keywords</entity> , the <entity id="W04-2903.35">system</entity> also retrieves audio hot spots by speaker identity, <entity id="W04-2903.36">word</entity> spoken by a specific speaker, a change of <entity id="W04-2903.37">speech</entity> <entity id="W04-2903.38">rate</entity> , and other <entity id="W04-2903.39">non-lexical features</entity> , <entity id="W04-2903.40">including</entity> applause and laughter. Finally, we discuss our <entity id="W04-2903.41">approach</entity> to <entity id="W04-2903.42">semantic</entity> , morphological, phonetic <entity id="W04-2903.43">query <entity id="W04-2903.44">expansion</entity></entity> to <entity id="W04-2903.45">improve</entity> audio <entity id="W04-2903.46">retrieval</entity> <entity id="W04-2903.47">performance</entity> and to <entity id="W04-2903.48">access</entity> <entity id="W04-2903.49">cross-lingual</entity> <entity id="W04-2903.50">data</entity> .
</abstract>


</text>

<text id="W04-3006">
<title><entity id="W04-3006.1">Error</entity> <entity id="W04-3006.2">Detection</entity> And <entity id="W04-3006.3">Recovery</entity> In <entity id="W04-3006.4">Spoken Dialogue</entity> <entity id="W04-3006.5">Systems</entity></title>
<abstract>
"This <entity id="W04-3006.6">paper</entity> describes our <entity id="W04-3006.7">research</entity> on both the <entity id="W04-3006.8">detection</entity> and subsequent <entity id="W04-3006.9">resolution</entity> of <entity id="W04-3006.10">recognition</entity> <entity id="W04-3006.11">errors</entity> in spoken <entity id="W04-3006.12">dialogue systems</entity> . The <entity id="W04-3006.13">paper</entity> consists of two major <entity id="W04-3006.14">components</entity> . The first half <entity id="W04-3006.15">concerns</entity> the <entity id="W04-3006.16">design</entity> of the <entity id="W04-3006.17">error</entity> <entity id="W04-3006.18">detection</entity> <entity id="W04-3006.19">mechanism</entity> for resolving city <entity id="W04-3006.20">names</entity> in our mercury flight reservation <entity id="W04-3006.21">system</entity> , and an <entity id="W04-3006.22">investigation</entity> of the behavioral <entity id="W04-3006.23">patterns</entity> of <entity id="W04-3006.24">users</entity> in subsequent subdialogues involving keypad <entity id="W04-3006.25">entry</entity> for <entity id="W04-3006.26">disambiguation</entity> . An important <entity id="W04-3006.27">observation</entity> is that, upon a request for keypad <entity id="W04-3006.28">entry</entity> , <entity id="W04-3006.29">users</entity> are frequently unresponsive to the <entity id="W04-3006.30">extent</entity> of waiting for a <entity id="W04-3006.31">time-out</entity> or hanging up the <entity id="W04-3006.32">phone</entity> . The second half <entity id="W04-3006.33">concerns</entity> a <entity id="W04-3006.34">pilot</entity> <entity id="W04-3006.35">experiment</entity> investigating the feasibility of replacing the solicitation of a keypad <entity id="W04-3006.36">entry</entity> with that of a ""<entity id="W04-3006.37">speak-and-spell</entity> "" <entity id="W04-3006.38">entry</entity> . A <entity id="W04-3006.39">novelty</entity> of our work is the <entity id="W04-3006.40">introduction</entity> of a <entity id="W04-3006.41">speech</entity> synthesizer to simulate the <entity id="W04-3006.42">user</entity> , which facilitates <entity id="W04-3006.43">development</entity> and <entity id="W04-3006.44">evaluation</entity> of our <entity id="W04-3006.45">proposed</entity> <entity id="W04-3006.46">strategy</entity> . We have found that the <entity id="W04-3006.47">speak-and-spell</entity> <entity id="W04-3006.48">strategy</entity> is quite effective in <entity id="W04-3006.49">simulation</entity> <entity id="W04-3006.50">mode</entity> , but it remains to be <entity id="W04-3006.51">tested</entity> in real <entity id="W04-3006.52">user</entity> <entity id="W04-3006.53">dialogues</entity> . "
</abstract>


</text>

<text id="W05-0707">
<title><entity id="W05-0707.1">Part Of Speech Tagging</entity> For Amharic Using Conditional Random Fields
</title>
<abstract>
We <entity id="W05-0707.2">applied</entity> Conditional Random Fields (CRFs) to the <entity id="W05-0707.3">tasks</entity> of Amharic <entity id="W05-0707.4">word segmentation</entity> and <entity id="W05-0707.5">POS tagging</entity> using a small annotated <entity id="W05-0707.6">corpus</entity> of 1000 <entity id="W05-0707.7">words</entity> . Given the <entity id="W05-0707.8">size</entity> of the <entity id="W05-0707.9">data</entity> and the large <entity id="W05-0707.10">number</entity> of <entity id="W05-0707.11">unknown <entity id="W05-0707.12">words</entity></entity> in the <entity id="W05-0707.13">test</entity> <entity id="W05-0707.14">corpus</entity> (80%), an <entity id="W05-0707.15">accuracy</entity> of 84% for Amharic <entity id="W05-0707.16">word segmentation</entity> and 74% for <entity id="W05-0707.17">POS tagging</entity> is encouraging, indicating the <entity id="W05-0707.18">applicability</entity> of CRFs for a morphologically <entity id="W05-0707.19">complex</entity> <entity id="W05-0707.20">language</entity> like Amharic.
</abstract>


</text>

<text id="W05-0801">
<title><entity id="W05-0801.1">Association-</entity> <entity id="W05-0801.2">Based</entity> Bilingual <entity id="W05-0801.3">Word Alignment</entity></title>
<abstract>
Bilingual <entity id="W05-0801.4">word alignment</entity> <entity id="W05-0801.5">forms</entity> the foundation of <entity id="W05-0801.6">current</entity> work on <entity id="W05-0801.7">statistical machine translation</entity> . <entity id="W05-0801.8">Standard</entity> <entity id="W05-0801.9">word-alignment</entity> <entity id="W05-0801.10">methods</entity> involve the use of probabilistic <entity id="W05-0801.11">generative <entity id="W05-0801.12">models</entity></entity> that are <entity id="W05-0801.13">complex</entity> to <entity id="W05-0801.14">implement</entity> and slow to <entity id="W05-0801.15">train</entity> . In this <entity id="W05-0801.16">paper</entity> we show that it is possible to <entity id="W05-0801.17">approach</entity> the <entity id="W05-0801.18">alignment</entity> <entity id="W05-0801.19">accuracy</entity> of the <entity id="W05-0801.20">standard</entity> <entity id="W05-0801.21">models</entity> using <entity id="W05-0801.22">algorithms</entity> that are much faster, and in some ways simpler, <entity id="W05-0801.23">based</entity> on <entity id="W05-0801.24">basic</entity> <entity id="W05-0801.25">word-association</entity> <entity id="W05-0801.26">statistics</entity> .
</abstract>


</text>

<text id="W06-0103">
<title>
Mining Atomic <entity id="W06-0103.1">Chinese</entity> <entity id="W06-0103.2">Abbreviation</entity> Pairs: A <entity id="W06-0103.3">Probabilistic <entity id="W06-0103.4">Model</entity></entity> For Single Character <entity id="W06-0103.5">Word</entity> <entity id="W06-0103.6">Recovery</entity></title>
<abstract>
"An HMM-based Single Character <entity id="W06-0103.7">Recovery</entity> (SCR) <entity id="W06-0103.8">Model</entity> is <entity id="W06-0103.9">proposed</entity> in this <entity id="W06-0103.10">paper</entity> to <entity id="W06-0103.11">extract</entity> a large set of ""atomic <entity id="W06-0103.12">abbreviation</entity> <entity id="W06-0103.13">pairs</entity> "" from a large <entity id="W06-0103.14">text</entity> <entity id="W06-0103.15">corpus</entity> . By an ""atomic <entity id="W06-0103.16">abbreviation</entity> <entity id="W06-0103.17">pair</entity> ,"" it refers to an abbreviated <entity id="W06-0103.18">word</entity> and its root <entity id="W06-0103.19">word</entity> (i.e., unabbreviated <entity id="W06-0103.20">form</entity> ) in which the <entity id="W06-0103.21">abbreviation</entity> is a single <entity id="W06-0103.22">Chinese</entity> character. This <entity id="W06-0103.23">task</entity> is interesting since the <entity id="W06-0103.24">abbreviation</entity> <entity id="W06-0103.25">process</entity> for <entity id="W06-0103.26">Chinese</entity> compound <entity id="W06-0103.27">words</entity> seems to be ""compositional""; in other <entity id="W06-0103.28">words</entity> , one can often decode an abbreviated <entity id="W06-0103.29">word</entity> , such as ""nX"" (Taiwan <entity id="W06-0103.30">University</entity> ), character-by-character back to its root <entity id="W06-0103.31">form</entity> . With a large atomic <entity id="W06-0103.32">abbreviation</entity> <entity id="W06-0103.33">dictionary</entity> , one may be able to recover multiple-character <entity id="W06-0103.34">abbreviations</entity> more easily. With only a few <entity id="W06-0103.35">training</entity> <entity id="W06-0103.36">iterations</entity> , the <entity id="W06-0103.37">acquisition</entity> <entity id="W06-0103.38">accuracy</entity> of the <entity id="W06-0103.39">proposed</entity> SCR <entity id="W06-0103.40">model</entity> achieves 62% and 50 % <entity id="W06-0103.41">precision</entity> for <entity id="W06-0103.42">training set</entity> and <entity id="W06-0103.43">test set</entity> , respectively, from the ASWSC-2001 <entity id="W06-0103.44">corpus</entity> . "
</abstract>


</text>

<text id="W06-0308">
<title>
Towards A Validated <entity id="W06-0308.1">Model</entity> For Affective <entity id="W06-0308.2">Classification</entity> Of <entity id="W06-0308.3">Texts</entity></title>
<abstract>
In this <entity id="W06-0308.4">paper</entity> , we present the <entity id="W06-0308.5">results</entity> of <entity id="W06-0308.6">experiments</entity> aiming to validate a two-dimensional <entity id="W06-0308.7">typology</entity> of affective states as a suitable <entity id="W06-0308.8">basis</entity> for affective <entity id="W06-0308.9">classification</entity> of <entity id="W06-0308.10">texts</entity> . Using a <entity id="W06-0308.11">corpus</entity> of <entity id="W06-0308.12">English</entity> weblog posts, annotated for <entity id="W06-0308.13">mood</entity> by their authors, we <entity id="W06-0308.14">trained</entity> <entity id="W06-0308.15">support <entity id="W06-0308.16">vector</entity> <entity id="W06-0308.17">machine</entity></entity> binary <entity id="W06-0308.18">classifiers</entity> to distinguish <entity id="W06-0308.19">texts</entity> on the <entity id="W06-0308.20">basis</entity> of their affiliation with one <entity id="W06-0308.21">region</entity> of the <entity id="W06-0308.22">space</entity> . We then <entity id="W06-0308.23">report</entity> on <entity id="W06-0308.24">experiments</entity> which go a <entity id="W06-0308.25">step</entity> further, using <entity id="W06-0308.26">four-class</entity> <entity id="W06-0308.27">classifiers</entity> <entity id="W06-0308.28">based</entity> on automated scoring of <entity id="W06-0308.29">texts</entity> for each <entity id="W06-0308.30">dimension</entity> of the <entity id="W06-0308.31">typology</entity> . Our <entity id="W06-0308.32">results</entity> indicate that it is possible to extend the <entity id="W06-0308.33">standard</entity> binary <entity id="W06-0308.34">sentiment analysis</entity> (positive/negative) <entity id="W06-0308.35">approach</entity> to a two dimensional <entity id="W06-0308.36">model</entity> (positive/negative; active/passive), and <entity id="W06-0308.37">provide</entity> some <entity id="W06-0308.38">evidence</entity> to <entity id="W06-0308.39">support</entity> a more fine-grained <entity id="W06-0308.40">classification</entity> along these two axes.
</abstract>


</text>

<text id="W06-0903">
<title><entity id="W06-0903.1">Automatic</entity> Dating Of <entity id="W06-0903.2">Documents</entity> And Temporal <entity id="W06-0903.3">Text Classification</entity></title>
<abstract>
The <entity id="W06-0903.4">frequency</entity> of <entity id="W06-0903.5">occurrence</entity> of <entity id="W06-0903.6">words</entity> in <entity id="W06-0903.7">natural languages</entity> exhibits a periodic and a non-periodic <entity id="W06-0903.8">component</entity> when analysed as a <entity id="W06-0903.9">time</entity> <entity id="W06-0903.10">series</entity> . This work presents an unsupervised <entity id="W06-0903.11">method</entity> of <entity id="W06-0903.12">extracting</entity> periodicity <entity id="W06-0903.13">information</entity> from <entity id="W06-0903.14">text</entity> , enabling <entity id="W06-0903.15">time</entity> <entity id="W06-0903.16">series</entity> <entity id="W06-0903.17">creation</entity> and filtering to be used in the <entity id="W06-0903.18">creation</entity> of sophisticated <entity id="W06-0903.19">language models</entity> that can discern between repetitive <entity id="W06-0903.20">trends</entity> and non-repetitive writing <entity id="W06-0903.21">patterns</entity> . The <entity id="W06-0903.22">algorithm</entity> <entity id="W06-0903.23">performs</entity> in O(n
</abstract>


</text>

<text id="W06-1107">
<title><entity id="W06-1107.1">Evaluation</entity> Of Several Phonetic <entity id="W06-1107.2">Similarity</entity> <entity id="W06-1107.3">Algorithms</entity> On The <entity id="W06-1107.4">Task</entity> Of Cognate <entity id="W06-1107.5">Identification</entity></title>
<abstract>
We investigate the <entity id="W06-1107.6">problem</entity> of measuring phonetic <entity id="W06-1107.7">similarity</entity> , <entity id="W06-1107.8">focusing</entity> on the <entity id="W06-1107.9">identification</entity> of cognates, <entity id="W06-1107.10">words</entity> of the same origin in different <entity id="W06-1107.11">languages</entity> . We compare <entity id="W06-1107.12">representatives</entity> of two principal <entity id="W06-1107.13">approaches</entity> to <entity id="W06-1107.14">computing</entity> phonetic <entity id="W06-1107.15">similarity</entity> : manually-designed <entity id="W06-1107.16">metrics</entity> , and learning <entity id="W06-1107.17">algorithms</entity> . In particular, we consider a stochastic transducer, a <entity id="W06-1107.18">Pair</entity> HMM, several DBN <entity id="W06-1107.19">models</entity> , and two <entity id="W06-1107.20">constructed</entity> <entity id="W06-1107.21">schemes</entity> . We <entity id="W06-1107.22">test</entity> those <entity id="W06-1107.23">approaches</entity> on the <entity id="W06-1107.24">task</entity> of identifying cognates among Indoeuropean <entity id="W06-1107.25">languages</entity> , both in the supervised and unsupervised <entity id="W06-1107.26">context</entity> . Our <entity id="W06-1107.27">results</entity> suggest that the averaged <entity id="W06-1107.28">context</entity> DBN <entity id="W06-1107.29">model</entity> and the <entity id="W06-1107.30">Pair</entity> HMM achieve the highest <entity id="W06-1107.31">accuracy</entity> given a large <entity id="W06-1107.32">training set</entity> of positive <entity id="W06-1107.33">examples</entity> .
</abstract>


</text>

<text id="W06-1620">
<title>
Multilingual Deep <entity id="W06-1620.1">Lexical</entity> <entity id="W06-1620.2">Acquisition</entity> For HPSGs Via Supertagging
</title>
<abstract>
We <entity id="W06-1620.3">propose</entity> a <entity id="W06-1620.4">conditional random <entity id="W06-1620.5">field-based</entity></entity> <entity id="W06-1620.6">method</entity> for supertagging, and <entity id="W06-1620.7">apply</entity> it to the <entity id="W06-1620.8">task</entity> of learning new <entity id="W06-1620.9">lexical <entity id="W06-1620.10">items</entity></entity> for HPSG-based <entity id="W06-1620.11">precision</entity> grammars of <entity id="W06-1620.12">English</entity> and <entity id="W06-1620.13">Japanese</entity> . Using a pseudo-likelihood <entity id="W06-1620.14">approximation</entity> we are able to <entity id="W06-1620.15">scale</entity> our <entity id="W06-1620.16">model</entity> to hundreds of supertags and tens-of-thousands of <entity id="W06-1620.17">training</entity> <entity id="W06-1620.18">sentences</entity> . We show that it is possible to achieve start-of-the-art <entity id="W06-1620.19">results</entity> for both <entity id="W06-1620.20">languages</entity> using maximally <entity id="W06-1620.21">language-independent</entity> <entity id="W06-1620.22">lexical features</entity> . Further, we explore the <entity id="W06-1620.23">performance</entity> of the <entity id="W06-1620.24">models</entity> at the <entity id="W06-1620.25">type-</entity> and <entity id="W06-1620.26">token-level</entity> , demonstrating their superior <entity id="W06-1620.27">performance</entity> when compared to a unigram-based baseline and a <entity id="W06-1620.28">transformation-based</entity> <entity id="W06-1620.29">learning approach</entity> .
</abstract>


</text>

<text id="W04-0402">
<title>
Paraphrasing Of <entity id="W04-0402.1">Japanese</entity> Light- <entity id="W04-0402.2">Verb</entity> Constructions <entity id="W04-0402.3">Based</entity> On <entity id="W04-0402.4">Lexical</entity> <entity id="W04-0402.5">Conceptual Structure</entity></title>
<abstract>
Some particular <entity id="W04-0402.6">classes</entity> of <entity id="W04-0402.7">lexical</entity> paraphrases such as <entity id="W04-0402.8">verb</entity> alteration and compound <entity id="W04-0402.9">noun</entity> <entity id="W04-0402.10">decomposition</entity> can be handled by a <entity id="W04-0402.11">handful</entity> of general <entity id="W04-0402.12">rules</entity> and <entity id="W04-0402.13">lexical</entity> <entity id="W04-0402.14">semantic knowledge</entity> . In this <entity id="W04-0402.15">paper</entity> , we attempt to capture the <entity id="W04-0402.16">regularity</entity> underlying these <entity id="W04-0402.17">classes</entity> of paraphrases, <entity id="W04-0402.18">focusing</entity> on the paraphrasing of <entity id="W04-0402.19">Japanese</entity> <entity id="W04-0402.20">light-verb</entity> <entity id="W04-0402.21">constructions</entity> (LVCs). We <entity id="W04-0402.22">propose</entity> a paraphrasing <entity id="W04-0402.23">model</entity> for LVCs that is <entity id="W04-0402.24">based</entity> on transforming the <entity id="W04-0402.25">Lexical</entity> Conceptual <entity id="W04-0402.26">Structures</entity> (LCSs) of <entity id="W04-0402.27">verbal</entity> elements. We also <entity id="W04-0402.28">propose</entity> a <entity id="W04-0402.29">refinement</entity> of an existing LCS <entity id="W04-0402.30">dictionary</entity> . <entity id="W04-0402.31">Experimental</entity> <entity id="W04-0402.32">results</entity> show that our LCS-based paraphrasing <entity id="W04-0402.33">model</entity> characterizes some of the <entity id="W04-0402.34">semantic features</entity> of those <entity id="W04-0402.35">verbs</entity> <entity id="W04-0402.36">required</entity> for <entity id="W04-0402.37">generating</entity> paraphrases, such as the <entity id="W04-0402.38">direction</entity> of an <entity id="W04-0402.39">action</entity> and the <entity id="W04-0402.40">relationship</entity> between <entity id="W04-0402.41">arguments</entity> and <entity id="W04-0402.42">surface</entity> <entity id="W04-0402.43">cases</entity> .
</abstract>


</text>

<text id="W06-2718">
<title>
A Standoff Annotation <entity id="W06-2718.1">Interface</entity> Between DELPH-In Components
</title>
<abstract>
We present a standoff annotation <entity id="W06-2718.2">framework</entity> for the <entity id="W06-2718.3">integration</entity> of NLP <entity id="W06-2718.4">components</entity> , currently <entity id="W06-2718.5">implemented</entity> in the <entity id="W06-2718.6">context</entity> of the DELPH-IN <entity id="W06-2718.7">tools</entity></abstract>


</text>

<text id="C00-2134">
<title>
Lexicalized <entity id="C00-2134.1">Tree</entity> <entity id="C00-2134.2">Automata-</entity> <entity id="C00-2134.3">Based</entity> Grammars For Translating Conversational <entity id="C00-2134.4">Texts</entity></title>
<abstract>
We <entity id="C00-2134.5">propose</entity> a new lexicalized grammar <entity id="C00-2134.6">formalism</entity> <entity id="C00-2134.7">called</entity> Lexicalized <entity id="C00-2134.8">Tree</entity> <entity id="C00-2134.9">Automata-based</entity> Grammar, which lexicalizes <entity id="C00-2134.10">tree</entity> acceptors instead of <entity id="C00-2134.11">trees</entity> themselves. We discuss the <entity id="C00-2134.12">properties</entity> of the grammar and present a chart <entity id="C00-2134.13">parsing</entity> <entity id="C00-2134.14">algorithm</entity> . We have <entity id="C00-2134.15">implemented</entity> a <entity id="C00-2134.16">translation</entity> <entity id="C00-2134.17">module</entity> for conversational <entity id="C00-2134.18">texts</entity> using this <entity id="C00-2134.19">formalism</entity> , and <entity id="C00-2134.20">applied</entity> it to an <entity id="C00-2134.21">experimental</entity> <entity id="C00-2134.22">automatic</entity> <entity id="C00-2134.23">interpretation</entity> <entity id="C00-2134.24">system</entity> (speech <entity id="C00-2134.25">translation system</entity> ).
</abstract>


</text>

<text id="W04-3203">
<title><entity id="W04-3203.1">Induction</entity> Of Greedy Controllers For Deterministic Treebank Parsers
</title>
<abstract>
Most <entity id="W04-3203.2">statistical</entity> <entity id="W04-3203.3">parsers</entity> have used the <entity id="W04-3203.4">grammar induction</entity> <entity id="W04-3203.5">approach</entity> , in which a stochastic grammar is induced from a treebank. An <entity id="W04-3203.6">alternative</entity> <entity id="W04-3203.7">approach</entity> is to induce a controller for a given <entity id="W04-3203.8">parsing</entity> automaton. Such controllers may be stochastic; here, we <entity id="W04-3203.9">focus</entity> on greedy controllers, which <entity id="W04-3203.10">result</entity> in deterministic <entity id="W04-3203.11">parsers</entity> . We use <entity id="W04-3203.12">decision trees</entity> to learn the controllers. The <entity id="W04-3203.13">resulting</entity> <entity id="W04-3203.14">parsers</entity> are surprisingly accurate and <entity id="W04-3203.15">robust</entity> , considering their <entity id="W04-3203.16">speed</entity> and <entity id="W04-3203.17">simplicity</entity> . They are almost as fast as <entity id="W04-3203.18">current</entity> <entity id="W04-3203.19">part-of-speech</entity> taggers, and considerably more accurate than a <entity id="W04-3203.20">basic</entity> unlexicalized PCFG <entity id="W04-3203.21">parser</entity> . We also describe Markov <entity id="W04-3203.22">parsing</entity> <entity id="W04-3203.23">models</entity> , a general <entity id="W04-3203.24">framework</entity> for <entity id="W04-3203.25">parser</entity> <entity id="W04-3203.26">modeling</entity> and <entity id="W04-3203.27">control</entity> , of which the <entity id="W04-3203.28">parsers</entity> <entity id="W04-3203.29">reported</entity> here are a <entity id="W04-3203.30">special case</entity> .
</abstract>


</text>

<text id="W01-0511">
<title>
Classifying The <entity id="W01-0511.1">Semantic</entity> <entity id="W01-0511.2">Relations</entity> In <entity id="W01-0511.3">Noun</entity> Compounds Via A <entity id="W01-0511.4">Domain-</entity> Specific <entity id="W01-0511.5">Lexical</entity> Hierarchy
</title>
<abstract>
We are <entity id="W01-0511.6">developing</entity> <entity id="W01-0511.7">corpus-based</entity> <entity id="W01-0511.8">techniques</entity> for identifying <entity id="W01-0511.9">semantic <entity id="W01-0511.10">relations</entity></entity> at an intermediate <entity id="W01-0511.11">level</entity> of <entity id="W01-0511.12">description</entity> (more specific than those used in <entity id="W01-0511.13">case</entity> <entity id="W01-0511.14">frames</entity> , but more general than those used in traditional <entity id="W01-0511.15">knowledge representation</entity> <entity id="W01-0511.16">systems</entity> ). In this <entity id="W01-0511.17">paper</entity> we describe a <entity id="W01-0511.18">classification</entity> <entity id="W01-0511.19">algorithm</entity> for identifying <entity id="W01-0511.20">relationships</entity> between <entity id="W01-0511.21">two-word</entity> <entity id="W01-0511.22">noun</entity> compounds. We find that a very <entity id="W01-0511.23">simple</entity> <entity id="W01-0511.24">approach</entity> using a <entity id="W01-0511.25">machine</entity> learning <entity id="W01-0511.26">algorithm</entity> and a <entity id="W01-0511.27">domain-specific</entity> <entity id="W01-0511.28">lexical</entity> hierarchy successfully generalizes from <entity id="W01-0511.29">training</entity> <entity id="W01-0511.30">instances</entity> , <entity id="W01-0511.31">performing</entity> better on previously unseen <entity id="W01-0511.32">words</entity> than a baseline consisting of <entity id="W01-0511.33">training</entity> on the <entity id="W01-0511.34">words</entity> themselves.
</abstract>


</text>

<text id="W07-0401">
<title><entity id="W07-0401.1">Chunk-</entity> <entity id="W07-0401.2">Level</entity> Reordering of <entity id="W07-0401.3">Source Language</entity> <entity id="W07-0401.4">Sentences</entity> with Automatically Learned Rules for <entity id="W07-0401.5">Statistical Machine Translation</entity></title>
<abstract>
In this <entity id="W07-0401.6">paper</entity> , we describe a <entity id="W07-0401.7">source-side</entity> reordering <entity id="W07-0401.8">method</entity> <entity id="W07-0401.9">based</entity> on <entity id="W07-0401.10">syntactic</entity> <entity id="W07-0401.11">chunks</entity> for <entity id="W07-0401.12">phrase-based</entity> <entity id="W07-0401.13">statistical machine translation</entity> . First, we shallow <entity id="W07-0401.14">parse</entity> the <entity id="W07-0401.15">source language</entity> <entity id="W07-0401.16">sentences</entity> . Then, reordering <entity id="W07-0401.17">rules</entity> are automatically learned from <entity id="W07-0401.18">source-side</entity> <entity id="W07-0401.19">chunks</entity> and <entity id="W07-0401.20">word alignments</entity> . During <entity id="W07-0401.21">translation</entity> , the <entity id="W07-0401.22">rules</entity> are used to <entity id="W07-0401.23">generate</entity> a reordering <entity id="W07-0401.24">lattice</entity> for each <entity id="W07-0401.25">sentence</entity> . <entity id="W07-0401.26">Experimental</entity> <entity id="W07-0401.27">results</entity> are <entity id="W07-0401.28">reported</entity> for a <entity id="W07-0401.29">Chinese-to-</entity> <entity id="W07-0401.30">English</entity> <entity id="W07-0401.31">task</entity> , showing an <entity id="W07-0401.32">improvement</entity> of 0.5%-1.8% BLEU score absolute on various <entity id="W07-0401.33">test sets</entity> and better <entity id="W07-0401.34">computational</entity> <entity id="W07-0401.35">efficiency</entity> than reordering during decoding. The <entity id="W07-0401.36">experiments</entity> also show that the reordering at the <entity id="W07-0401.37">chunk-level</entity> <entity id="W07-0401.38">performs</entity> better than at the <entity id="W07-0401.39">POS-level</entity> .
</abstract>


</text>

<text id="D07-1059">
<title><entity id="D07-1059.1">Generating</entity> <entity id="D07-1059.2">Lexical</entity> Analogies Using <entity id="D07-1059.3">Dependency</entity> <entity id="D07-1059.4">Relations</entity></title>
<abstract>
A <entity id="D07-1059.5">lexical</entity> <entity id="D07-1059.6">analogy</entity> is a <entity id="D07-1059.7">pair</entity> of <entity id="D07-1059.8">word-pairs</entity> that share a similar <entity id="D07-1059.9">semantic relation</entity> . <entity id="D07-1059.10">Lexical</entity> <entity id="D07-1059.11">analogies</entity> occur frequently in <entity id="D07-1059.12">text</entity> and are useful in various <entity id="D07-1059.13">natural language processing tasks</entity> . In this <entity id="D07-1059.14">study</entity> , we present a <entity id="D07-1059.15">system</entity> that <entity id="D07-1059.16">generates</entity> <entity id="D07-1059.17">lexical</entity> <entity id="D07-1059.18">analogies</entity> automatically from <entity id="D07-1059.19">text</entity> <entity id="D07-1059.20">data</entity> . Our <entity id="D07-1059.21">system</entity> discovers semantically related <entity id="D07-1059.22">pairs</entity> of <entity id="D07-1059.23">words</entity> by using <entity id="D07-1059.24">dependency <entity id="D07-1059.25">relations</entity></entity> , and <entity id="D07-1059.26">applies</entity> novel <entity id="D07-1059.27">machine</entity> learning <entity id="D07-1059.28">algorithms</entity> to <entity id="D07-1059.29">match</entity> these <entity id="D07-1059.30">word-pairs</entity> to <entity id="D07-1059.31">form</entity> <entity id="D07-1059.32">lexical</entity> <entity id="D07-1059.33">analogies</entity> . Empirical <entity id="D07-1059.34">evaluation</entity> shows that our <entity id="D07-1059.35">system</entity> <entity id="D07-1059.36">generates</entity> valid <entity id="D07-1059.37">lexical</entity> <entity id="D07-1059.38">analogies</entity> with a <entity id="D07-1059.39">precision</entity> of 70% , and produces <entity id="D07-1059.40">quality</entity> <entity id="D07-1059.41">output</entity> although not at the <entity id="D07-1059.42">level</entity> of the best human-generated <entity id="D07-1059.43">lexical</entity> <entity id="D07-1059.44">analogies</entity> .
</abstract>


</text>

<text id="I08-3014">
<title>
An <entity id="I08-3014.1">Optimal</entity> <entity id="I08-3014.2">Order</entity> of Factors for the <entity id="I08-3014.3">Computational</entity> <entity id="I08-3014.4">Treatment</entity> of Personal Anaphoric Devices in Urdu <entity id="I08-3014.5">Discourse</entity></title>
<abstract>
Handling of <entity id="I08-3014.6">human language</entity> by <entity id="I08-3014.7">computer</entity> is a very intricate and <entity id="I08-3014.8">complex</entity> <entity id="I08-3014.9">task</entity> . In <entity id="I08-3014.10">natural languages</entity> , <entity id="I08-3014.11">sentences</entity> are usually <entity id="I08-3014.12">part</entity> of <entity id="I08-3014.13">discourse</entity> <entity id="I08-3014.14">units</entity> just as <entity id="I08-3014.15">words</entity> are <entity id="I08-3014.16">part</entity> of <entity id="I08-3014.17">sentences</entity> . <entity id="I08-3014.18">Anaphora <entity id="I08-3014.19">resolution</entity></entity> plays a significant <entity id="I08-3014.20">role</entity> in <entity id="I08-3014.21">discourse <entity id="I08-3014.22">analysis</entity></entity> for chopping larger <entity id="I08-3014.23">discourse</entity> <entity id="I08-3014.24">units</entity> into smaller ones. This <entity id="I08-3014.25">process</entity> is done for the <entity id="I08-3014.26">purpose</entity> of better <entity id="I08-3014.27">understanding</entity> and making easier the further <entity id="I08-3014.28">processing</entity> of <entity id="I08-3014.29">text</entity> by <entity id="I08-3014.30">computer</entity> . This <entity id="I08-3014.31">paper</entity> is <entity id="I08-3014.32">focused</entity> on the <entity id="I08-3014.33">discussion</entity> of various <entity id="I08-3014.34">factors</entity> and their <entity id="I08-3014.35">optimal</entity> <entity id="I08-3014.36">order</entity> that play an important <entity id="I08-3014.37">role</entity> in personal <entity id="I08-3014.38">anaphora <entity id="I08-3014.39">resolution</entity></entity> in Urdu. <entity id="I08-3014.40">Algorithms</entity> are <entity id="I08-3014.41">developed</entity> that resolves pronominal anaphoric <entity id="I08-3014.42">devices</entity> with 77-80% <entity id="I08-3014.43">success</entity> <entity id="I08-3014.44">rate</entity> .
</abstract>


</text>

<text id="I08-4004">
<title>
An Effective Hybrid <entity id="I08-4004.1">Machine <entity id="I08-4004.2">Learning Approach</entity></entity> for <entity id="I08-4004.3">Coreference <entity id="I08-4004.4">Resolution</entity></entity></title>
<abstract>
We present a hybrid <entity id="I08-4004.5">machine</entity> learning <entity id="I08-4004.6">approach</entity> for <entity id="I08-4004.7">coreference <entity id="I08-4004.8">resolution</entity></entity> . In our <entity id="I08-4004.9">method</entity> , we use CRFs as <entity id="I08-4004.10">basic</entity> <entity id="I08-4004.11">training</entity> <entity id="I08-4004.12">model</entity> , use active <entity id="I08-4004.13">learning <entity id="I08-4004.14">method</entity></entity> to <entity id="I08-4004.15">generate</entity> combined <entity id="I08-4004.16">features</entity> so as to make existed <entity id="I08-4004.17">features</entity> used more effectively; at last, we <entity id="I08-4004.18">proposed</entity> a novel <entity id="I08-4004.19">clustering</entity> <entity id="I08-4004.20">algorithm</entity> which used both the <entity id="I08-4004.21">linguistics</entity> <entity id="I08-4004.22">knowledge</entity> and the <entity id="I08-4004.23">statistical</entity> <entity id="I08-4004.24">knowledge</entity> . We built a <entity id="I08-4004.25">coreference <entity id="I08-4004.26">resolution</entity> <entity id="I08-4004.27">system</entity></entity> <entity id="I08-4004.28">based</entity> on the <entity id="I08-4004.29">proposed</entity> <entity id="I08-4004.30">method</entity> and <entity id="I08-4004.31">evaluate</entity> its <entity id="I08-4004.32">performance</entity> from three <entity id="I08-4004.33">aspects</entity> : the <entity id="I08-4004.34">contributions</entity> of <entity id="I08-4004.35">active learning</entity> ; the <entity id="I08-4004.36">effects</entity> of different <entity id="I08-4004.37">clustering</entity> <entity id="I08-4004.38">algorithms</entity> ; and the <entity id="I08-4004.39">resolution</entity> <entity id="I08-4004.40">performance</entity> of different <entity id="I08-4004.41">kinds</entity> of NPs. <entity id="I08-4004.42">Experimental</entity> <entity id="I08-4004.43">results</entity> show that additional <entity id="I08-4004.44">performance</entity> <entity id="I08-4004.45">gain</entity> can be obtained by using <entity id="I08-4004.46">active learning</entity> <entity id="I08-4004.47">method</entity> ; <entity id="I08-4004.48">clustering</entity> <entity id="I08-4004.49">algorithm</entity> has a great <entity id="I08-4004.50">effect</entity> on <entity id="I08-4004.51">coreference resolution</entity> 's <entity id="I08-4004.52">performance</entity> and our <entity id="I08-4004.53">clustering</entity> <entity id="I08-4004.54">algorithm</entity> is very effective; and the key of <entity id="I08-4004.55">coreference resolution</entity> is to <entity id="I08-4004.56">improve</entity> the <entity id="I08-4004.57">performance</entity> of the normal <entity id="I08-4004.58">noun</entity> 's <entity id="I08-4004.59">resolution</entity> , especially the pronoun's <entity id="I08-4004.60">resolution</entity> .
</abstract>


</text>

<text id="P07-1122">
<title>
Generalizing <entity id="P07-1122.1">Tree</entity> Transformations for Inductive <entity id="P07-1122.2">Dependency</entity> <entity id="P07-1122.3">Parsing</entity></title>
<abstract>
Previous <entity id="P07-1122.4">studies</entity> in <entity id="P07-1122.5">data-driven</entity> <entity id="P07-1122.6">dependency</entity> <entity id="P07-1122.7">parsing</entity> have shown that <entity id="P07-1122.8">tree</entity> <entity id="P07-1122.9">transformations</entity> can <entity id="P07-1122.10">improve</entity> <entity id="P07-1122.11">parsing</entity> <entity id="P07-1122.12">accuracy</entity> for specific <entity id="P07-1122.13">parsers</entity> and <entity id="P07-1122.14">data</entity> sets. We investigate to what <entity id="P07-1122.15">extent</entity> this can be <entity id="P07-1122.16">generalized</entity> across <entity id="P07-1122.17">languages</entity> /treebanks and <entity id="P07-1122.18">parsers</entity> , <entity id="P07-1122.19">focusing</entity> on pseudo-projective <entity id="P07-1122.20">parsing</entity> , as a way of capturing non-projective <entity id="P07-1122.21">dependencies</entity> , and <entity id="P07-1122.22">transformations</entity> used to facilitate <entity id="P07-1122.23">parsing</entity> of coordinate <entity id="P07-1122.24">structures</entity> and <entity id="P07-1122.25">verb</entity> groups. The <entity id="P07-1122.26">results</entity> indicate that the beneficial <entity id="P07-1122.27">effect</entity> of pseudo-projective <entity id="P07-1122.28">parsing</entity> is independent of <entity id="P07-1122.29">parsing</entity> <entity id="P07-1122.30">strategy</entity> but sensitive to <entity id="P07-1122.31">language</entity> or treebank specific <entity id="P07-1122.32">properties</entity> . By <entity id="P07-1122.33">contrast</entity> , the <entity id="P07-1122.34">construction</entity> specific <entity id="P07-1122.35">transformations</entity> appear to be more sensitive to <entity id="P07-1122.36">parsing</entity> <entity id="P07-1122.37">strategy</entity> but have a constant positive <entity id="P07-1122.38">effect</entity> over several <entity id="P07-1122.39">languages</entity> .
</abstract>


</text>

<text id="P07-2035">
<title><entity id="P07-2035.1">Construction</entity> of <entity id="P07-2035.2">Domain</entity> <entity id="P07-2035.3">Dictionary</entity> for Fundamental <entity id="P07-2035.4">Vocabulary</entity></title> 
<abstract>Guthrie , Joe A. ; Guthrie , Louise ; Aidinejad, Homa; Wilks , Yorick,Subject-Dependent Co- <entity id="P07-2035.5">Occurrence</entity> And <entity id="P07-2035.6">Word Sense Disambiguation</entity> ,Annual Meeting Of The <entity id="P07-2035.7">Association</entity> For <entity id="P07-2035.8">Computation</entity> al <entity id="P07-2035.9">Linguistics</entity> ,1991</abstract>


</text>

<text id="P08-2061">
<title><entity id="P08-2061.1">Extracting</entity> a <entity id="P08-2061.2">Representation</entity> from <entity id="P08-2061.3">Text</entity> for <entity id="P08-2061.4">Semantic Analysis</entity></title>
<abstract>
We present a novel fine-grained <entity id="P08-2061.5">semantic <entity id="P08-2061.6">representation</entity></entity> of <entity id="P08-2061.7">text</entity> and an <entity id="P08-2061.8">approach</entity> to <entity id="P08-2061.9">constructing</entity> it. This <entity id="P08-2061.10">representation</entity> is largely extractable by today's <entity id="P08-2061.11">technologies</entity> and facilitates more detailed <entity id="P08-2061.12">semantic analysis</entity> . We discuss the <entity id="P08-2061.13">requirements</entity> driving the <entity id="P08-2061.14">representation</entity> , suggest how it might be of value in the automated tutoring <entity id="P08-2061.15">domain</entity> , and <entity id="P08-2061.16">provide</entity> <entity id="P08-2061.17">evidence</entity> of its <entity id="P08-2061.18">validity</entity> .
</abstract>


</text>

<text id="C08-1021">
<title>
KnowNet: Building a Large Net of <entity id="C08-1021.1">Knowledge</entity> from the Web
</title>
<abstract>
This <entity id="C08-1021.2">paper</entity> presents a new fully <entity id="C08-1021.3">automatic</entity> <entity id="C08-1021.4">method</entity> for <entity id="C08-1021.5">building</entity> highly dense and accurate <entity id="C08-1021.6">knowledge <entity id="C08-1021.7">bases</entity></entity> from existing <entity id="C08-1021.8">semantic</entity> <entity id="C08-1021.9">resources</entity> . Basically, the <entity id="C08-1021.10">method</entity> uses a <entity id="C08-1021.11">wide-coverage</entity> and accurate <entity id="C08-1021.12">knowledge-based</entity> <entity id="C08-1021.13">Word Sense Disambiguation</entity> <entity id="C08-1021.14">algorithm</entity> to assign the most appropriate senses to large sets of topically related <entity id="C08-1021.15">words</entity> acquired from the web. KnowNet, the <entity id="C08-1021.16">resulting</entity> <entity id="C08-1021.17">knowledge-base</entity> which connects large sets of <entity id="C08-1021.18">semantically-related</entity> <entity id="C08-1021.19">concepts</entity> is a major <entity id="C08-1021.20">step</entity> towards the autonomous <entity id="C08-1021.21">acquisition</entity> of <entity id="C08-1021.22">knowledge</entity> from raw <entity id="C08-1021.23">corpora</entity> . In fact, KnowNet is several <entity id="C08-1021.24">times</entity> larger than any available <entity id="C08-1021.25">knowledge</entity> <entity id="C08-1021.26">resource</entity> encoding <entity id="C08-1021.27">relations</entity> between synsets, and the <entity id="C08-1021.28">knowledge</entity> KnowNet contains outperform any other <entity id="C08-1021.29">resource</entity> when is empirically <entity id="C08-1021.30">evaluated</entity> in a <entity id="C08-1021.31">common</entity> <entity id="C08-1021.32">framework</entity> .
</abstract>


</text>

<text id="D07-1122">
<title>
A Two-Stage <entity id="D07-1122.1">Parser</entity> for Multilingual <entity id="D07-1122.2">Dependency</entity> <entity id="D07-1122.3">Parsing</entity></title>
<abstract>
We present a two-stage multilingual <entity id="D07-1122.4">dependency</entity> <entity id="D07-1122.5">parsing</entity> <entity id="D07-1122.6">system</entity> submitted to the Multilingual Track of CoNLL-2007. The <entity id="D07-1122.7">parser</entity> first identifies <entity id="D07-1122.8">dependencies</entity> using a deterministic <entity id="D07-1122.9">parsing</entity> <entity id="D07-1122.10">method</entity> and then labels those <entity id="D07-1122.11">dependencies</entity> as a <entity id="D07-1122.12">sequence</entity> labeling <entity id="D07-1122.13">problem</entity> . We describe the <entity id="D07-1122.14">features</entity> used in each stage. For four <entity id="D07-1122.15">languages</entity> with different values of ROOT, we <entity id="D07-1122.16">design</entity> some special <entity id="D07-1122.17">features</entity> for the ROOT labeler. Then we present <entity id="D07-1122.18">evaluation results</entity> and <entity id="D07-1122.19">error analyses</entity> <entity id="D07-1122.20">focusing</entity> on <entity id="D07-1122.21">Chinese</entity> .
</abstract>


</text>

<text id="P02-1035">
<title><entity id="P02-1035.1">Parsing</entity> The <entity id="P02-1035.2">Wall Street <entity id="P02-1035.3">Journal</entity></entity> Using A <entity id="P02-1035.4">Lexical-</entity> Functional Grammar And Discriminative <entity id="P02-1035.5">Estimation</entity> <entity id="P02-1035.6">Techniques</entity></title>
<abstract>
We present a stochastic <entity id="P02-1035.7">parsing</entity> <entity id="P02-1035.8">system</entity> consisting of a <entity id="P02-1035.9">Lexical-</entity> Functional Grammar (LFG), a <entity id="P02-1035.10">constraint-based</entity> <entity id="P02-1035.11">parser</entity> and a stochastic <entity id="P02-1035.12">disambiguation</entity> <entity id="P02-1035.13">model</entity> . We <entity id="P02-1035.14">report</entity> on the <entity id="P02-1035.15">results</entity> of <entity id="P02-1035.16">applying</entity> this <entity id="P02-1035.17">system</entity> to <entity id="P02-1035.18">parsing</entity> the UPenn <entity id="P02-1035.19">Wall Street <entity id="P02-1035.20">Journal</entity></entity> (WSJ) treebank. The <entity id="P02-1035.21">model</entity> combines full and <entity id="P02-1035.22">partial</entity> <entity id="P02-1035.23">parsing</entity> <entity id="P02-1035.24">techniques</entity> to <entity id="P02-1035.25">reach</entity> full grammar <entity id="P02-1035.26">coverage</entity> on unseen <entity id="P02-1035.27">data</entity> . The treebank annotations are used to <entity id="P02-1035.28">provide</entity> partially labeled <entity id="P02-1035.29">data</entity> for discriminative <entity id="P02-1035.30">statistical</entity> <entity id="P02-1035.31">estimation</entity> using exponential <entity id="P02-1035.32">models</entity> . <entity id="P02-1035.33">Disambiguation</entity> <entity id="P02-1035.34">performance</entity> is <entity id="P02-1035.35">evaluated</entity> by measuring <entity id="P02-1035.36">matches</entity> of <entity id="P02-1035.37">predicate-argument</entity> <entity id="P02-1035.38">relations</entity> on two distinct <entity id="P02-1035.39">test sets</entity> . On a <entity id="P02-1035.40">gold standard</entity> of manually annotated <entity id="P02-1035.41">f-structures</entity> for a subset of the WSJ treebank, this <entity id="P02-1035.42">evaluation</entity> <entity id="P02-1035.43">reaches</entity> 79% F-score. An <entity id="P02-1035.44">evaluation</entity> on a <entity id="P02-1035.45">gold <entity id="P02-1035.46">standard</entity></entity> of <entity id="P02-1035.47">dependency relations</entity> for <entity id="P02-1035.48">Brown corpus</entity> <entity id="P02-1035.49">data</entity> achieves 76% F-score.
</abstract>


</text>

<text id="P02-1053">
<title>
Thumbs Up Or Thumbs Down? <entity id="P02-1053.1">Semantic</entity> <entity id="P02-1053.2">Orientation</entity> <entity id="P02-1053.3">Applied</entity> To Unsupervised <entity id="P02-1053.4">Classification</entity> Of Reviews
</title>
<abstract>
This <entity id="P02-1053.5">paper</entity> presents a <entity id="P02-1053.6">simple</entity> unsupervised <entity id="P02-1053.7">learning <entity id="P02-1053.8">algorithm</entity></entity> for classifying <entity id="P02-1053.9">reviews</entity> as recommended not recommended <entity id="P02-1053.10">semantic</entity> <entity id="P02-1053.11">orientation</entity></abstract>


</text>

<text id="P03-1038">
<title>
Self-Organizing Markov <entity id="P03-1038.1">Models</entity> And Their <entity id="P03-1038.2">Application</entity> To <entity id="P03-1038.3">Part-</entity> Of- <entity id="P03-1038.4">Speech</entity> <entity id="P03-1038.5">Tagging</entity></title>
<abstract>
This <entity id="P03-1038.6">paper</entity> presents a <entity id="P03-1038.7">method</entity> to <entity id="P03-1038.8">develop</entity> a <entity id="P03-1038.9">class</entity> of <entity id="P03-1038.10">variable</entity> <entity id="P03-1038.11">memory</entity> <entity id="P03-1038.12">Markov <entity id="P03-1038.13">models</entity></entity> that have higher <entity id="P03-1038.14">memory</entity> <entity id="P03-1038.15">capacity</entity> than traditional (uniform <entity id="P03-1038.16">memory</entity> ) <entity id="P03-1038.17">Markov <entity id="P03-1038.18">models</entity></entity> . The <entity id="P03-1038.19">structure</entity> of the <entity id="P03-1038.20">variable</entity> <entity id="P03-1038.21">memory</entity> <entity id="P03-1038.22">models</entity> is induced from a manually annotated <entity id="P03-1038.23">corpus</entity> through a <entity id="P03-1038.24">decision tree</entity> learning <entity id="P03-1038.25">algorithm</entity> . A <entity id="P03-1038.26">series</entity> of comparative <entity id="P03-1038.27">experiments</entity> show the <entity id="P03-1038.28">resulting</entity> <entity id="P03-1038.29">models</entity> outperform uniform <entity id="P03-1038.30">memory</entity> <entity id="P03-1038.31">Markov <entity id="P03-1038.32">models</entity></entity> in a <entity id="P03-1038.33">part-of-speech</entity> <entity id="P03-1038.34">tagging</entity> <entity id="P03-1038.35">task</entity> .
</abstract>


</text>

<text id="P04-1037">
<title>
Unsupervised <entity id="P04-1037.1">Sense <entity id="P04-1037.2">Disambiguation</entity></entity> Using Bilingual Probabilistic <entity id="P04-1037.3">Models</entity></title>
<abstract>
We describe two <entity id="P04-1037.4">probabilistic <entity id="P04-1037.5">models</entity></entity> for unsupervised <entity id="P04-1037.6"><entity id="P04-1037.7">word-sense</entity> <entity id="P04-1037.8">disambiguation</entity></entity> using <entity id="P04-1037.9">parallel corpora</entity> . The first <entity id="P04-1037.10">model</entity> , which we <entity id="P04-1037.11">call</entity> the <entity id="P04-1037.12">Sense</entity> <entity id="P04-1037.13">model</entity> , builds on the work of Diab and Resnik (2002) that uses both <entity id="P04-1037.14">parallel text</entity> and a <entity id="P04-1037.15">sense</entity> <entity id="P04-1037.16">inventory</entity> for the <entity id="P04-1037.17">target language</entity> , and recasts their <entity id="P04-1037.18">approach</entity> in a probabilistic <entity id="P04-1037.19">framework</entity> . The second <entity id="P04-1037.20">model</entity> , which we <entity id="P04-1037.21">call</entity> the <entity id="P04-1037.22">Concept</entity> <entity id="P04-1037.23">model</entity> , is a hierarchical <entity id="P04-1037.24">model</entity> that uses a <entity id="P04-1037.25">concept</entity> latent <entity id="P04-1037.26">variable</entity> to relate different <entity id="P04-1037.27">language</entity> specific <entity id="P04-1037.28">sense</entity> labels. We show that both <entity id="P04-1037.29">models</entity> <entity id="P04-1037.30">improve</entity> <entity id="P04-1037.31">performance</entity> on the <entity id="P04-1037.32">word sense disambiguation</entity> <entity id="P04-1037.33">task</entity> over previous unsu-pervised <entity id="P04-1037.34">approaches</entity> , with the <entity id="P04-1037.35">Concept</entity> <entity id="P04-1037.36">model</entity> showing the largest <entity id="P04-1037.37">improvement</entity> . Furthermore, in learning the <entity id="P04-1037.38">Concept</entity> <entity id="P04-1037.39">model</entity> , as a <entity id="P04-1037.40">by-product</entity> , we learn a <entity id="P04-1037.41">sense</entity> <entity id="P04-1037.42">inventory</entity> for the parallel <entity id="P04-1037.43">language</entity> .
</abstract>


</text>

<text id="P04-1059">
<title>
Adaptive <entity id="P04-1059.1">Chinese Word Segmentation</entity></title>
<abstract>
This <entity id="P04-1059.2">paper</entity> presents a <entity id="P04-1059.3">Chinese word segmentation</entity> <entity id="P04-1059.4">system</entity> which can <entity id="P04-1059.5">adapt</entity> to different <entity id="P04-1059.6">domains</entity> and <entity id="P04-1059.7">standards</entity> . We first present a <entity id="P04-1059.8">statistical</entity> <entity id="P04-1059.9">framework</entity> where <entity id="P04-1059.10">domain-specific</entity> <entity id="P04-1059.11">words</entity> are identified in a unified <entity id="P04-1059.12">approach</entity> to <entity id="P04-1059.13">word segmentation</entity> <entity id="P04-1059.14">based</entity> on <entity id="P04-1059.15">linear <entity id="P04-1059.16">models</entity></entity> . We explore several <entity id="P04-1059.17">features</entity> and describe how to create <entity id="P04-1059.18">training</entity> <entity id="P04-1059.19">data</entity> by <entity id="P04-1059.20">sampling</entity> . We then describe a <entity id="P04-1059.21">transformation-based</entity> <entity id="P04-1059.22">learning method</entity> used to <entity id="P04-1059.23">adapt</entity> our <entity id="P04-1059.24">system</entity> to different <entity id="P04-1059.25">word segmentation</entity> <entity id="P04-1059.26">standards</entity> . <entity id="P04-1059.27">Evaluation</entity> of the <entity id="P04-1059.28">proposed</entity> <entity id="P04-1059.29">system</entity> on five <entity id="P04-1059.30">test sets</entity> with different <entity id="P04-1059.31">standards</entity> shows that the <entity id="P04-1059.32">system</entity> achieves stateof-the-art <entity id="P04-1059.33">performance</entity> on all of them.
</abstract>


</text>

<text id="P04-3029">
<title>
Multimodal <entity id="P04-3029.1">Database</entity> <entity id="P04-3029.2">Access</entity> On Handheld Devices
</title>
<abstract>
We present the final MIAMM <entity id="P04-3029.3">system</entity> , a multimodal <entity id="P04-3029.4">dialogue <entity id="P04-3029.5">system</entity></entity> that employs <entity id="P04-3029.6">speech</entity> , haptic <entity id="P04-3029.7">interaction</entity> and novel <entity id="P04-3029.8">techniques</entity> of <entity id="P04-3029.9">information</entity> <entity id="P04-3029.10">visualization</entity> to allow a <entity id="P04-3029.11">natural</entity> and fast <entity id="P04-3029.12">access</entity> to large multimedia <entity id="P04-3029.13">databases</entity> on small handheld <entity id="P04-3029.14">devices</entity> .
</abstract>


</text>

<text id="P05-2024">
<title><entity id="P05-2024.1">Corpus-</entity> Oriented <entity id="P05-2024.2">Development</entity> Of <entity id="P05-2024.3">Japanese</entity> HPSG Parsers
</title>
<abstract>
This <entity id="P05-2024.4">paper</entity> <entity id="P05-2024.5">reports</entity> the <entity id="P05-2024.6">corpus-oriented</entity> <entity id="P05-2024.7">development</entity> of a <entity id="P05-2024.8">wide-coverage</entity> <entity id="P05-2024.9">Japanese</entity> HPSG <entity id="P05-2024.10">parser</entity> . We first created an HPSG treebank from the EDR <entity id="P05-2024.11">corpus</entity> by using heuristic <entity id="P05-2024.12">conversion</entity> <entity id="P05-2024.13">rules</entity> , and then <entity id="P05-2024.14">extracted</entity> <entity id="P05-2024.15">lexical <entity id="P05-2024.16">entries</entity></entity> from the <entity id="P05-2024.17">tree-bank</entity> . The grammar <entity id="P05-2024.18">developed</entity> using this <entity id="P05-2024.19">method</entity> attained wide <entity id="P05-2024.20">coverage</entity> that could hardly be obtained by conventional <entity id="P05-2024.21">manual</entity> <entity id="P05-2024.22">development</entity> . We also <entity id="P05-2024.23">trained</entity> a <entity id="P05-2024.24">statistical</entity> <entity id="P05-2024.25">parser</entity> for the grammar on the <entity id="P05-2024.26">tree-bank</entity> , and <entity id="P05-2024.27">evaluated</entity> the <entity id="P05-2024.28">parser</entity> in <entity id="P05-2024.29">terms</entity> of the <entity id="P05-2024.30">accuracy</entity> of <entity id="P05-2024.31">semantic-role</entity> <entity id="P05-2024.32">identification</entity> and <entity id="P05-2024.33">dependency</entity> <entity id="P05-2024.34">analysis</entity> .
</abstract>


</text>

<text id="C82-1067">
<title>
Man-Assisted <entity id="C82-1067.1">Machine</entity> <entity id="C82-1067.2">Construction</entity> Of A <entity id="C82-1067.3">Semantic</entity> <entity id="C82-1067.4">Dictionary</entity> For <entity id="C82-1067.5">Natural Language</entity> <entity id="C82-1067.6">Processing</entity></title>
<abstract>
This is a <entity id="C82-1067.7">report</entity> on the <entity id="C82-1067.8">semantic</entity> <entity id="C82-1067.9">dictionary</entity> for <entity id="C82-1067.10">natural language processing</entity> we are <entity id="C82-1067.11">constructing</entity> now.   This <entity id="C82-1067.12">paper</entity> explains how to obtain the <entity id="C82-1067.13">semantic <entity id="C82-1067.14">information</entity></entity> for the <entity id="C82-1067.15">dictionary</entity> from an ordinary <entity id="C82-1067.16">Japanese</entity> <entity id="C82-1067.17">language</entity> <entity id="C82-1067.18">dictionary</entity> with about 60,000 <entity id="C82-1067.19">items</entity> (which had already been put into <entity id="C82-1067.20">machine</entity> readable <entity id="C82-1067.21">form</entity> ) and also explains what should be the <entity id="C82-1067.22">frame</entity> for the <entity id="C82-1067.23">representation</entity> of meaning of each <entity id="C82-1067.24">item</entity> ( <entity id="C82-1067.25">word</entity> ).   Then a man-assisted <entity id="C82-1067.26">machine</entity> <entity id="C82-1067.27">procedure</entity> that embeds the <entity id="C82-1067.28">semantic</entity> graph with <entity id="C82-1067.29">respect</entity> to the <entity id="C82-1067.30">head word</entity> of the ordinary <entity id="C82-1067.31">dictionary</entity> into the <entity id="C82-1067.32">frame</entity> of a <entity id="C82-1067.33">head word</entity> is discussed.
</abstract>


</text>

<text id="C86-1077">
<title><entity id="C86-1077.1">Strategies</entity> For Interactive <entity id="C86-1077.2">Machine Translation</entity> : The <entity id="C86-1077.3">Experience</entity> And Implications Of The UMIST <entity id="C86-1077.4">Japanese</entity> <entity id="C86-1077.5">Project</entity></title>
<abstract>
At the Contre for <entity id="C86-1077.6">Computational Linguistics</entity> , we are <entity id="C86-1077.7">designing</entity> and <entity id="C86-1077.8">implementing</entity> an Eng.lish-to- <entity id="C86-1077.9">Japanese</entity> interactive <entity id="C86-1077.10">machine translation system</entity> . The <entity id="C86-1077.11">project</entity> is funded jointly by the Alvey Directorate and International <entity id="C86-1077.12">Computers</entity> Limited (ICL). The <entity id="C86-1077.13">prototype</entity> <entity id="C86-1077.14">system</entity> runs on the ICL PERQ, though much of the <entity id="C86-1077.15">development</entity> work has been done on a VAX 11/750. It is <entity id="C86-1077.16">implemented</entity> in Prolog, in the interests of rapid <entity id="C86-1077.17">prototyping</entity> , but intended for <entity id="C86-1077.18">optimization</entity> . The informing <entity id="C86-1077.19">principles</entity> are those of modern comp1 <entity id="C86-1077.20">ex-feature-based</entity> <entity id="C86-1077.21">linguistic <entity id="C86-1077.22">theories</entity></entity> , in particular <entity id="C86-1077.23">Lexical-</entity> Functional Grammar ( Bresnan (ed.) 1982, Kaplan and  Bresnan 1982 ), and <entity id="C86-1077.24">Generalized Phrase Structure Grammar</entity> ( Gazdar et al. 1985 ). For <entity id="C86-1077.25">development</entity> <entity id="C86-1077.26">purposes</entity> we are using an existing <entity id="C86-1077.27">corpus</entity> of 10,000 <entity id="C86-1077.28">words</entity> of continuous prose from the PERQ's graphics <entity id="C86-1077.29">documentation</entity> ; in the long <entity id="C86-1077.30">term</entity> , the <entity id="C86-1077.31">system</entity> will be extended for use by technical writers in <entity id="C86-1077.32">fields</entity> other than <entity id="C86-1077.33">software</entity> . At the <entity id="C86-1077.34">time</entity> of writing, we have well-developed <entity id="C86-1077.35">system</entity> <entity id="C86-1077.36">development</entity> <entity id="C86-1077.37">software</entity> , <entity id="C86-1077.38">user interface</entity> , and grammar and <entity id="C86-1077.39">dictionary</entity> handling <entity id="C86-1077.40">utilities</entity> . The <entity id="C86-1077.41">English</entity> <entity id="C86-1077.42">analysis</entity> grammar handles most of the <entity id="C86-1077.43">syntactic structures</entity> of the <entity id="C86-1077.44">corpus</entity> , and we have a range of <entity id="C86-1077.45">formats</entity> for <entity id="C86-1077.46">output</entity> of linguistic <entity id="C86-1077.47">representations</entity> and <entity id="C86-1077.48">Japanese</entity> <entity id="C86-1077.49">text</entity> . A <entity id="C86-1077.50">transfer</entity> grammar for <entity id="C86-1077.51">English-</entity> <entity id="C86-1077.52">Japanese</entity> has been prototyped, but is not not yet fully adequate to handle all <entity id="C86-1077.53">constructions</entity> in the <entity id="C86-1077.54">corpus</entity> ; a facility for <entity id="C86-1077.55">dictionary</entity> <entity id="C86-1077.56">entry</entity> in <entity id="C86-1077.57">kanji</entity> is incorporated. The <entity id="C86-1077.58">aspect</entity> of the <entity id="C86-1077.59">system</entity> we will <entity id="C86-1077.60">focus</entity> on in the present <entity id="C86-1077.61">paper</entity> is its interactive <entity id="C86-1077.62">nature</entity> , discussing the range of different <entity id="C86-1077.63">types</entity> of <entity id="C86-1077.64">interaction</entity> which are <entity id="C86-1077.65">provided</entity> or permitted for different <entity id="C86-1077.66">types</entity> of <entity id="C86-1077.67">user</entity> .
</abstract>


</text>

<text id="C88-1065">
<title>
Exploiting <entity id="C88-1065.1">Lexical</entity> Regularities In Designing <entity id="C88-1065.2">Natural Language</entity> <entity id="C88-1065.3">Systems</entity></title>
<abstract>
This <entity id="C88-1065.4">paper</entity> presents the <entity id="C88-1065.5">lexical</entity> <entity id="C88-1065.6">component</entity> of the START <entity id="C88-1065.7">Question <entity id="C88-1065.8">Answering system</entity></entity> <entity id="C88-1065.9">developed</entity> at the MIT <entity id="C88-1065.10">Artificial Intelligence</entity> <entity id="C88-1065.11">Laboratory</entity> . START is able to interpret correctly a wide range of <entity id="C88-1065.12">semantic</entity> <entity id="C88-1065.13">relationships</entity> associated with alternate <entity id="C88-1065.14">expressions</entity> of the <entity id="C88-1065.15">arguments</entity> of <entity id="C88-1065.16">verbs</entity> . The <entity id="C88-1065.17">design</entity> of the <entity id="C88-1065.18">system</entity> takes <entity id="C88-1065.19">advantage</entity> of the <entity id="C88-1065.20">results</entity> of recent linguistic <entity id="C88-1065.21">research</entity> into the <entity id="C88-1065.22">structure</entity> of the <entity id="C88-1065.23">lexicon</entity> , allowing START to attain a broader range of <entity id="C88-1065.24">coverage</entity> than many existing <entity id="C88-1065.25">systems</entity> while maintaining modular <entity id="C88-1065.26">organization</entity> .
</abstract>


</text>

<text id="C88-2095">
<title><entity id="C88-2095.1">Reasons</entity> Why I Do Not Care Grammar <entity id="C88-2095.2">Formalism</entity></title>
<abstract>
has borrowed a lot of ideas from We could not have <entity id="C88-2095.3">developed</entity> even a <entity id="C88-2095.4">simple</entity> <entity id="C88-2095.5">parser</entity> without the <entity id="C88-2095.6">research</entity> <entity id="C88-2095.7">results</entity> in It is obviously nonsense to <entity id="C88-2095.8">claim</entity> that we, <entity id="C88-2095.9">computational linguists</entity> , do not care <entity id="C88-2095.10">research</entity> <entity id="C88-2095.11">results</entity> in However, the <entity id="C88-2095.12">researchers</entity> in it seems to me, are very fond of especially, those who are <entity id="C88-2095.13">called</entity> They always fight with each other by asserting that their are superior to the others'. They are oversensitive and tend to distinguish people into two groups, and A <entity id="C88-2095.14">computational linguist</entity> using LFG (or LFG) as a small <entity id="C88-2095.15">part</entity> in his total <entity id="C88-2095.16">system</entity> is taken as the <entity id="C88-2095.17">ally</entity> of LFG, and is certainly accused by the other groups. They promptly demonstrate that LFG is wrong, by showing a lot of peculiar <entity id="C88-2095.18">sentences</entity> which rarely appear in real <entity id="C88-2095.19">texts</entity> .Formalisms are prepared for accomplishing specific <entity id="C88-2095.20">purposes</entity> . The <entity id="C88-2095.21">formalisms</entity> in have been <entity id="C88-2095.22">proposed</entity> , roughly speaking, for describing the of distinguishing from arbitrary /ramraa/ica/ <entity id="C88-2095.23">sequences</entity> , and of relating the grammatical <entity id="C88-2095.24">sequences</entity> with the other representational <entity id="C88-2095.25">levels</entity> .On the other <entity id="C88-2095.26">hand</entity> , a <entity id="C88-2095.27">formalism</entity> we need in CL is for different <entity id="C88-2095.28">purposes</entity> . That is, we need a <entity id="C88-2095.29">formalism</entity> for describing the <entity id="C88-2095.30">rules</entity> of distinguishing the most feasible grammatical <entity id="C88-2095.31">structures</entity> from other less feasible but still ones of the same <entity id="C88-2095.32">sentences</entity> We also need a <entity id="C88-2095.33">formalism</entity> in which we can manage systematically a large <entity id="C88-2095.34">amount</entity> of <entity id="C88-2095.35">knowledge</entity> of various sorts necessary for NLP.Formalisms for different <entity id="C88-2095.36">purposes</entity> , of course, should be <entity id="C88-2095.37">evaluated</entity> <entity id="C88-2095.38">based</entity> on different <entity id="C88-2095.39">standards</entity> . The <entity id="C88-2095.40">current</entity> <entity id="C88-2095.41">discussions</entity> of different <entity id="C88-2095.42">formalisms</entity> in TL are irrelevant to our <entity id="C88-2095.43">standards</entity> , though they may be important for their The following is a <entity id="C88-2095.44">list</entity> of the <entity id="C88-2095.45">reasons</entity> why I think so.
</abstract>


</text>

<text id="C88-2130">
<title>
Directing The <entity id="C88-2130.1">Generation</entity> Of Living <entity id="C88-2130.2">Space</entity> Descriptions
</title>
<abstract>
We have <entity id="C88-2130.3">developed</entity> a <entity id="C88-2130.4">Computational model</entity> of the <entity id="C88-2130.5">process</entity> of describing the <entity id="C88-2130.6">layout</entity> of an apartment or house, a much-studied <entity id="C88-2130.7">discourse</entity> <entity id="C88-2130.8">task</entity> first characterized linguistically by Linde (1974). The <entity id="C88-2130.9">model</entity> is embodied in a <entity id="C88-2130.10">program</entity> , APT, that can reproduce <entity id="C88-2130.11">segments</entity> of actual tape-recorded <entity id="C88-2130.12">descriptions</entity> , using organizational and <entity id="C88-2130.13">discourse</entity> <entity id="C88-2130.14">strategies</entity> derived through <entity id="C88-2130.15">analysis</entity> of our <entity id="C88-2130.16">corpus</entity> .
</abstract>


</text>

<text id="C88-2148">
<title><entity id="C88-2148.1">Topic</entity> / <entity id="C88-2148.2">Focus</entity> Articulation And Intensional <entity id="C88-2148.3">Logic</entity></title>
<abstract>
A <entity id="C88-2148.4">semantic <entity id="C88-2148.5">analysis</entity></entity> of <entity id="C88-2148.6">topic</entity> and <entity id="C88-2148.7">focus</entity> as two <entity id="C88-2148.8">parts</entity> of tectograamatical <entity id="C88-2148.9">representation</entity> by means of transparent intensional <entity id="C88-2148.10">logic</entity> (TIL) ia presented. It is pointed out that two <entity id="C88-2148.11">sentences</entity> (more precisely, their tectograamatical <entity id="C88-2148.12">representations</entity> ) differing just in the <entity id="C88-2148.13">topic</entity> / <entity id="C88-2148.14">focus</entity> articulation (TFA) denote different propositions, i.e. that TFA has an <entity id="C88-2148.15">effect</entity> upon the <entity id="C88-2148.16">semantic</entity> <entity id="C88-2148.17">content</entity> of the <entity id="C88-2148.18">sentence</entity> . An inforaal short <entity id="C88-2148.19">description</entity> of an <entity id="C88-2148.20">algorithm</entity> handling the TFA in the <entity id="C88-2148.21">translation</entity> of tectogrammatical <entity id="C88-2148.22">representations</entity> into the <entity id="C88-2148.23">constructions</entity> of TIL is added. The TFA <entity id="C88-2148.24">algorithm</entity> divides a <entity id="C88-2148.25">representation</entity> into two <entity id="C88-2148.26">parts</entity> corresponding to the <entity id="C88-2148.27">topic</entity> and <entity id="C88-2148.28">focus</entity> ; every <entity id="C88-2148.29">part</entity> is analyzed ( <entity id="C88-2148.30">translated</entity> ) in <entity id="C88-2148.31">isolation</entity> and then the <entity id="C88-2148.32">resulting</entity> <entity id="C88-2148.33">construction</entity> is put together. The TIL <entity id="C88-2148.34">construction</entity> discussed here
</abstract>


</text>

<text id="C90-1021">
<title>
Bilingual <entity id="C90-1021.1">Generation</entity> Of Weather Forecasts In An Operations <entity id="C90-1021.2">Environment</entity></title>
<abstract>
In  1986  the first <entity id="C90-1021.3">experiments</entity> in <entity id="C90-1021.4">text generation</entity> <entity id="C90-1021.5">applied</entity> to weather forecasts <entity id="C90-1021.6">resulted</entity> in a <entity id="C90-1021.7">prototype</entity> <entity id="C90-1021.8">system</entity> (RAREAS[6,3]) for producing <entity id="C90-1021.9">English</entity> marine bulletins from forecast <entity id="C90-1021.10">data</entity> . Subsequent work in 1987 added French <entity id="C90-1021.11">output</entity> to make the initial <entity id="C90-1021.12">system</entity> bilingual (RAREAS-2[llj). During 1988 -1989 a <entity id="C90-1021.13">full-scale</entity> operational <entity id="C90-1021.14">system</entity> was created to meet the needs of daily marine forecast production for three regional centres in the Canadian Atmospheric <entity id="C90-1021.15">Environment</entity> Service
</abstract>


</text>

<text id="C90-3009">
<title>
Syllable- <entity id="C90-3009.1">Based</entity> <entity id="C90-3009.2">Morphology</entity></title>
<abstract>
This <entity id="C90-3009.3">paper</entity> presents a <entity id="C90-3009.4">language</entity> for the <entity id="C90-3009.5">description</entity> of morphological <entity id="C90-3009.6">alternations</entity> which is <entity id="C90-3009.7">based</entity> on syllable <entity id="C90-3009.8">structure</entity> . The <entity id="C90-3009.9">justification</entity> for such an <entity id="C90-3009.10">approach</entity> is discussed with <entity id="C90-3009.11">reference</entity> to <entity id="C90-3009.12">examples</entity> from a <entity id="C90-3009.13">variety</entity> of <entity id="C90-3009.14">languages</entity> and the <entity id="C90-3009.15">approach</entity> is compared to Koskenniemi 's <entity id="C90-3009.16">two-level</entity> account of morphonoiogy. Keywords: <entity id="C90-3009.17">morphology</entity> , <entity id="C90-3009.18">phonology</entity> , syllables.
</abstract>


</text>

<text id="C92-4211">
<title><entity id="C92-4211.1">Knowledge</entity> <entity id="C92-4211.2">Acquisition</entity> And <entity id="C92-4211.3">Chinese</entity> <entity id="C92-4211.4">Parsing</entity> <entity id="C92-4211.5">Based</entity> On <entity id="C92-4211.6">Corpus</entity></title>
<abstract>
In <entity id="C92-4211.7">Natural Language Processing</entity> (NLP), one key <entity id="C92-4211.8">problem</entity> is how to <entity id="C92-4211.9">design</entity> a <entity id="C92-4211.10">robust</entity> and effective <entity id="C92-4211.11">parsing</entity> <entity id="C92-4211.12">system</entity> . In this <entity id="C92-4211.13">paper</entity> , we will introduce a <entity id="C92-4211.14">corpus</entity> <entity id="C92-4211.15">based</entity> <entity id="C92-4211.16">Chinese</entity> <entity id="C92-4211.17">parsing</entity> <entity id="C92-4211.18">system</entity> . Our <entity id="C92-4211.19">efforts</entity> <entity id="C92-4211.20">arc</entity> concctrated on: (1) <entity id="C92-4211.21">knowledge</entity> <entity id="C92-4211.22">acquisition</entity> and <entity id="C92-4211.23">representation</entity> ; and (2) the <entity id="C92-4211.24">parsing</entity> <entity id="C92-4211.25">scheme</entity> . The <entity id="C92-4211.26">knowledge</entity> of this <entity id="C92-4211.27">system</entity> is principally <entity id="C92-4211.28">extracted</entity> from analyzed <entity id="C92-4211.29">corpus</entity> , others are a few grammatical <entity id="C92-4211.30">principles</entity> , i.e. the four <entity id="C92-4211.31">axioms</entity> of the <entity id="C92-4211.32">Dependency Grammar</entity> (DG). In <entity id="C92-4211.33">addition</entity> , we also <entity id="C92-4211.34">propose</entity> the fifth <entity id="C92-4211.35">axiom</entity> of DG to <entity id="C92-4211.36">support</entity> the <entity id="C92-4211.37">parsing</entity> of <entity id="C92-4211.38">Chinese</entity> <entity id="C92-4211.39">sentences</entity> .
</abstract>


</text>

<text id="C94-1023">
<title><entity id="C94-1023.1">Automatic</entity> <entity id="C94-1023.2">Model</entity> <entity id="C94-1023.3">Refinement</entity> - With An <entity id="C94-1023.4">Application</entity> To <entity id="C94-1023.5">Tagging</entity></title> 
<abstract><entity id="C94-1023.6">Statistical</entity> NLP <entity id="C94-1023.7">models</entity> usually only consider coarse <entity id="C94-1023.8">information</entity> and very restricted <entity id="C94-1023.9">context</entity> to make the <entity id="C94-1023.10">estimation</entity> of <entity id="C94-1023.11">parameters</entity> feasible. To reduce the <entity id="C94-1023.12">modeling</entity> <entity id="C94-1023.13">error</entity> introduced by a simplified <entity id="C94-1023.14">probabilistic <entity id="C94-1023.15">model</entity></entity> , the <entity id="C94-1023.16">Classification</entity> and <entity id="C94-1023.17">Regression</entity> <entity id="C94-1023.18">Tree</entity> (CART) <entity id="C94-1023.19">method</entity> was adopted in this <entity id="C94-1023.20">paper</entity> to select more discriminative <entity id="C94-1023.21">features</entity> for <entity id="C94-1023.22">automatic</entity> <entity id="C94-1023.23">model</entity> <entity id="C94-1023.24">refinement</entity> . Because the <entity id="C94-1023.25">features</entity> are adopted dependently during <entity id="C94-1023.26">splitting</entity> the <entity id="C94-1023.27">classification</entity> <entity id="C94-1023.28">tree</entity> in CART, the <entity id="C94-1023.29">number</entity> of <entity id="C94-1023.30">training</entity> <entity id="C94-1023.31">data</entity> in each <entity id="C94-1023.32">terminal</entity> <entity id="C94-1023.33">node</entity> is small, which makes the labeling <entity id="C94-1023.34">process</entity> of <entity id="C94-1023.35">terminal</entity> <entity id="C94-1023.36">nodes</entity> not <entity id="C94-1023.37">robust</entity> . This over-tuning <entity id="C94-1023.38">phenomenon</entity> cannot be completely removed by <entity id="C94-1023.39">cross-validation</entity> <entity id="C94-1023.40">process</entity> (i.e., pruning <entity id="C94-1023.41">process</entity> ). A probabilistic <entity id="C94-1023.42">classification <entity id="C94-1023.43">model</entity></entity> <entity id="C94-1023.44">based</entity> on the selected discriminative <entity id="C94-1023.45">features</entity> is thus <entity id="C94-1023.46">proposed</entity> to use the <entity id="C94-1023.47">training</entity> <entity id="C94-1023.48">data</entity> more efficiently. In <entity id="C94-1023.49">tagging</entity> the <entity id="C94-1023.50">Brown <entity id="C94-1023.51">Corpus</entity></entity> , our probabilistic <entity id="C94-1023.52">classification <entity id="C94-1023.53">model</entity></entity> reduces the <entity id="C94-1023.54">error <entity id="C94-1023.55">rate</entity></entity> of the top 10 <entity id="C94-1023.56">error</entity> dominant <entity id="C94-1023.57">words</entity> from 5.71% to 4.35%, which shows 23.82% <entity id="C94-1023.58">improvement</entity> over the unrefined <entity id="C94-1023.59">model</entity> .
</abstract>


</text>

<text id="C94-1055">
<title><entity id="C94-1055.1">Generating</entity> Multilingual <entity id="C94-1055.2">Documents</entity> From A <entity id="C94-1055.3">Knowledge <entity id="C94-1055.4">Base</entity></entity> The TECHDOC <entity id="C94-1055.5">Project</entity></title>
<abstract>
TECHDOC is an <entity id="C94-1055.6">implemented</entity> <entity id="C94-1055.7">system</entity> demonstrating the feasibility of <entity id="C94-1055.8">generating</entity> multilingu.il technical <entity id="C94-1055.9">documents</entity> on the <entity id="C94-1055.10">basis</entity> of a <entity id="C94-1055.11">language-independent</entity> <entity id="C94-1055.12">knowledge <entity id="C94-1055.13">base</entity></entity> . Its <entity id="C94-1055.14">application</entity> <entity id="C94-1055.15">domain</entity> is use]- and <entity id="C94-1055.16">maintenance</entity> <entity id="C94-1055.17">instructions</entity> , which are produced from underlying plan <entity id="C94-1055.18">structures</entity> representing the activities, the participating <entity id="C94-1055.19">objects</entity> with their <entity id="C94-1055.20">properties</entity> , <entity id="C94-1055.21">relations</entity> , and so on. This <entity id="C94-1055.22">paper</entity> gives a brief <entity id="C94-1055.23">outline</entity> of the <entity id="C94-1055.24">system <entity id="C94-1055.25">architecture</entity></entity> and discusses some recent <entity id="C94-1055.26">developments</entity> in the <entity id="C94-1055.27">project</entity> : the <entity id="C94-1055.28">addition</entity> of actual <entity id="C94-1055.29">event</entity> <entity id="C94-1055.30">simulation</entity> in the KM, <entity id="C94-1055.31">steps</entity> towards a <entity id="C94-1055.32">document</entity> authoring <entity id="C94-1055.33">tool</entity> , and a multimodal <entity id="C94-1055.34">user interface</entity> .
</abstract>


</text>

<text id="C94-2138">
<title>
A Reestimation <entity id="C94-2138.1">Algorithm</entity> For Probabilistic Ttecursive <entity id="C94-2138.2">Transition</entity> <entity id="C94-2138.3">Network</entity></title>
<abstract>
Probabilistic. Recursive <entity id="C94-2138.4">Transition</entity> <entity id="C94-2138.5">Network</entity> (I'KTN) is an elevated <entity id="C94-2138.6">version</entity> of RTN to <entity id="C94-2138.7">model</entity> and <entity id="C94-2138.8">process</entity> <entity id="C94-2138.9">languages</entity> in stochastic, <entity id="C94-2138.10">parameters</entity> . The <entity id="C94-2138.11">representation</entity> is a direct <entity id="C94-2138.12">derivation</entity> from the KTN and keeps much the spirit of <entity id="C94-2138.13">Hidden Markov Model</entity> at the same <entity id="C94-2138.14">time</entity> . We present a reestimation <entity id="C94-2138.15">algorithm</entity> for PHTN that is a <entity id="C94-2138.16">variation</entity> of Inside Outside <entity id="C94-2138.17">algorithm</entity> that <entity id="C94-2138.18">computes</entity> the values of the probabilistic <entity id="C94-2138.19">parameters</entity> from <entity id="C94-2138.20">sample</entity> <entity id="C94-2138.21">sentences</entity> (parsed or uuparsed).
</abstract>


</text>

<text id="C86-1052">
<title>
DCKR - <entity id="C86-1052.1">Knowledge Representation</entity> In Prolog And Its <entity id="C86-1052.2">Application</entity> To <entity id="C86-1052.3">Natural Language Processing</entity></title>
<abstract>
"important <entity id="C86-1052.4">tasks</entity> for <entity id="C86-1052.5">natural language processing</entity> . Basin to <entity id="C86-1052.6">semantic</entity> <entity id="C86-1052.7">processing</entity> is <entity id="C86-1052.8">descriptions</entity> of <entity id="C86-1052.9">lexical <entity id="C86-1052.10">items</entity></entity> . The most frequently used <entity id="C86-1052.11">form</entity> of <entity id="C86-1052.12">description</entity> of <entity id="C86-1052.13">lexical <entity id="C86-1052.14">items</entity></entity> is probably Frames or Objects. Therefore in what <entity id="C86-1052.15">form</entity> Frames or Objects are expressed is a key <entity id="C86-1052.16">issue</entity> for <entity id="C86-1052.17">natural language processing</entity> . A <entity id="C86-1052.18">method</entity> of the <entity id="C86-1052.19">Object</entity> <entity id="C86-1052.20">representation</entity> in Prolog <entity id="C86-1052.21">called</entity> DCKR will be introduced. It will be seen that if <entity id="C86-1052.22">part</entity> of general <entity id="C86-1052.23">knowledge</entity> and a <entity id="C86-1052.24">dictionary</entity> are described in DCKR, <entity id="C86-1052.25">part</entity> of <entity id="C86-1052.26">context-processing</entity> and the greater <entity id="C86-1052.27">part</entity> of <entity id="C86-1052.28">semantic</entity> <entity id="C86-1052.29">processing</entity> can be left  to the <entity id="C86-1052.30">functions</entity> built in Prolog. 09) se
( <entity id="C86-1052.31">animal</entity> ,age : X,_) :- bottomof(S,B), sem(B,birthYear:Y,_), X is  1986 - Y. 10) seni(face,P,S) :- hasa(eye,P,[face I S]) ; hasa(nose,P,[face IS] ) ; has a(mou th,P,[face !S]). Now the meanings of the gem, isa and hasa predicates, which are important to <entity id="C86-1052.32">descriptions</entity> in DCKR, are explained later using the DCKR <entity id="C86-1052.33">examples</entity> given above. 1. I n troduct i on Relationships    between   <entity id="C86-1052.34">knowledge</entity> represented predicate  <entity id="C86-1052.35">logic</entity> <entity id="C86-1052.36">formulas</entity> and <entity id="C86-1052.37">knowledge</entity> represented Frames    or    Siruciured <entity id="C86-1052.38">objects</entity> are clarified by [Hayes 80],     [ Nilsson 80],     CGoebel  85],[ Bowen 85], al,      but      their      <entity id="C86-1052.39">methods</entity>    <entity id="C86-1052.40">requires</entity> separately <entity id="C86-1052.41">interpreter</entity> for  their <entity id="C86-1052.42">representation</entity> . et an The authors have <entity id="C86-1052.43">developed</entity> a <entity id="C86-1052.44">knowledge representation</entity> <entity id="C86-1052.45">form</entity> <entity id="C86-1052.46">called</entity> DCKR (Definite <entity id="C86-1052.47">Clause</entity> <entity id="C86-1052.48">Knowledge Representation</entity> ) [ Koyama 85]. In DCKR, each of the sip_ts composing of a Structured <entity id="C86-1052.49">Object</entity> (hereinafter simply <entity id="C86-1052.50">called</entity> an <entity id="C86-1052.51">object</entity> ) is represented by a Horn <entity id="C86-1052.52">clause</entity> (a Prolog statement) with the ""<entity id="C86-1052.53">sen</entity> "" predicate (to be explained in <entity id="C86-1052.54">Section</entity> 2) as its head. Therefore, an <entity id="C86-1052.55">Object</entity> can be regarded as a set of Horn <entity id="C86-1052.56">clauses</entity> (slots) headed by the <entity id="C86-1052.57">sen</entity> predicate with the same first <entity id="C86-1052.58">argument</entity> . From the foregoing it follows that almost all of a <entity id="C86-1052.59">program</entity> for <entity id="C86-1052.60">performing</entity> <entity id="C86-1052.61">semantic</entity> intepretations <entity id="C86-1052.62">relative</entity> to <entity id="C86-1052.63">lexical items</entity> described in DCKR can be replaced by <entity id="C86-1052.64">functions</entity> built in Prolog. That is, most of <entity id="C86-1052.65">programming</entity> <entity id="C86-1052.66">efforts</entity> of <entity id="C86-1052.67">semantic</entity> <entity id="C86-1052.68">processing</entity> can be left to the <entity id="C86-1052.69">functions</entity> built in Pro 1og. DCKR will be described in <entity id="C86-1052.70">detail</entity> in <entity id="C86-1052.71">Section</entity> 2. <entity id="C86-1052.72">Section</entity> 3 will discuss <entity id="C86-1052.73">applications</entity> of DCKR to <entity id="C86-1052.74">semantic</entity> <entity id="C86-1052.75">processing</entity> of <entity id="C86-1052.76">natural languages</entity> . "
</abstract>


</text>

<text id="C88-1048">
<title>
Improving <entity id="C88-1048.1">Search Strategies</entity> An <entity id="C88-1048.2">Experiment</entity> In Best-First <entity id="C88-1048.3">Parsing</entity></title>
<abstract>
"Viewing the <entity id="C88-1048.4">syntactic analysis</entity> of <entity id="C88-1048.5">natural language</entity> as a <entity id="C88-1048.6">search</entity> <entity id="C88-1048.7">problem</entity> , the right <entity id="C88-1048.8">choice</entity> of <entity id="C88-1048.9">parsing</entity> <entity id="C88-1048.10">strategy</entity> plays an important <entity id="C88-1048.11">role</entity> in the <entity id="C88-1048.12">performance</entity> of <entity id="C88-1048.13">natural language</entity> <entity id="C88-1048.14">parsers</entity> . After a <entity id="C88-1048.15">motivation</entity> of the use of various heuristic <entity id="C88-1048.16">criteria</entity> , a <entity id="C88-1048.17">framework</entity> for defining and <entity id="C88-1048.18">testing</entity> ' <entity id="C88-1048.19">parsing</entity> <entity id="C88-1048.20">strategies</entity> is presented. On this <entity id="C88-1048.21">basis</entity> systematic <entity id="C88-1048.22">tests</entity> on different <entity id="C88-1048.23">parsing</entity> <entity id="C88-1048.24">strategies</entity> have been <entity id="C88-1048.25">performed</entity> , the <entity id="C88-1048.26">results</entity> of which are dicussed. Generally ;hese <entity id="C88-1048.27">tests</entity> show that a ""guided"" depth-oriented <entity id="C88-1048.28">strategy</entity> gives a considerable <entity id="C88-1048.29">reduction</entity> of <entity id="C88-1048.30">search</entity> <entity id="C88-1048.31">effort</entity> compared to the classical depth first <entity id="C88-1048.32">strategy</entity> . "
</abstract>


</text>

<text id="C88-2122">
<title><entity id="C88-2122.1">Generating</entity> Multimodal <entity id="C88-2122.2">Output-</entity> Conditions, Advantages And <entity id="C88-2122.3">Problems</entity></title>
<abstract>
"In <entity id="C88-2122.4">natural</entity> <entity id="C88-2122.5">communication</entity> <entity id="C88-2122.6">situations</entity> , multimodal <entity id="C88-2122.7">referent</entity> <entity id="C88-2122.8">specification</entity> is frequent and efficient. The linguistic <entity id="C88-2122.9">component</entity> are deictic <entity id="C88-2122.10">expressions</entity> , e.g. 'this' and 'here'. Extralinguistic <entity id="C88-2122.11">devices</entity> in <entity id="C88-2122.12">dialogs</entity> are different body movements, mainly pointing <entity id="C88-2122.13">gestures</entity> . Their functional equivalent in <entity id="C88-2122.14">texts</entity> are means like arrows and <entity id="C88-2122.15">indices</entity> . This <entity id="C88-2122.16">paper</entity> has two intentions. First, it discusses the <entity id="C88-2122.17">advantages</entity> of multimodal <entity id="C88-2122.18">reference</entity> in interhuman <entity id="C88-2122.19">communication</entity> which motivate the <entity id="C88-2122.20">integration</entity> of extralinguistic ""pointing"" <entity id="C88-2122.21">devices</entity> into NL <entity id="C88-2122.22">dialog systems</entity> . The <entity id="C88-2122.23">generation</entity> of multimodal <entity id="C88-2122.24">output</entity> poses specific <entity id="C88-2122.25">problems</entity> , which have no counterpart in the <entity id="C88-2122.26">analysis</entity> of multimodal <entity id="C88-2122.27">input</entity> . The second <entity id="C88-2122.28">part</entity> presents the <entity id="C88-2122.29">strategy</entity> for <entity id="C88-2122.30">generating</entity> multimodal <entity id="C88-2122.31">output</entity> which has been <entity id="C88-2122.32">developed</entity> within the <entity id="C88-2122.33">framework</entity> of the XTRA <entity id="C88-2122.34">system</entity> (a NL <entity id="C88-2122.35">access</entity> <entity id="C88-2122.36">system</entity> to <entity id="C88-2122.37">expert systems</entity> ). XTRA allows the <entity id="C88-2122.38">combination</entity> of <entity id="C88-2122.39">verbal</entity> <entity id="C88-2122.40">descriptions</entity> and pointing <entity id="C88-2122.41">gestures</entity> in <entity id="C88-2122.42">order</entity> to specify elements of the given visual <entity id="C88-2122.43">context</entity> , i.e. a <entity id="C88-2122.44">form</entity> <entity id="C88-2122.45">displayed</entity> on the <entity id="C88-2122.46">screen</entity> . The <entity id="C88-2122.47">component</entity> POPEL <entity id="C88-2122.48">generates</entity> referential <entity id="C88-2122.49">expressions</entity> which may be accompanied by a pointing <entity id="C88-2122.50">gesture</entity> . The appearance of these <entity id="C88-2122.51">gestures</entity> depends on several <entity id="C88-2122.52">factors</entity> , e.g. the <entity id="C88-2122.53">type</entity> of <entity id="C88-2122.54">referent</entity> (whether it is a <entity id="C88-2122.55">region</entity> or an <entity id="C88-2122.56">entry</entity> of the <entity id="C88-2122.57">form</entity> ) and its <entity id="C88-2122.58">complexity</entity> . "
</abstract>


</text>

<text id="D08-1060">
<title>
Generalizing Local and Non-Local <entity id="D08-1060.1">Word-</entity> Reordering <entity id="D08-1060.2">Patterns</entity> for <entity id="D08-1060.3">Syntax-</entity> <entity id="D08-1060.4">Based</entity> <entity id="D08-1060.5">Machine Translation</entity></title> 
<abstract><entity id="D08-1060.6">Syntactic</entity> <entity id="D08-1060.7">word</entity> reordering is essential for <entity id="D08-1060.8">translations</entity> across different grammar <entity id="D08-1060.9">structures</entity> between syntactically distant <entity id="D08-1060.10">language-pairs</entity> . In this <entity id="D08-1060.11">paper</entity> , we <entity id="D08-1060.12">propose</entity> to embed local and non-local <entity id="D08-1060.13">word</entity> reordering <entity id="D08-1060.14">decisions</entity> in a <entity id="D08-1060.15">synchronous context free grammar</entity> , and <entity id="D08-1060.16">leverages</entity> the grammar in a chart-based <entity id="D08-1060.17">decoder</entity> . Local <entity id="D08-1060.18">word-reordering</entity> is effectively encoded in Hiero-like <entity id="D08-1060.19">rules</entity> ; whereas non-local <entity id="D08-1060.20">word-reordering</entity> , which allows for long-range movements of <entity id="D08-1060.21">syntactic</entity> <entity id="D08-1060.22">chunks</entity> , is represented in <entity id="D08-1060.23">tree-based</entity> reordering <entity id="D08-1060.24">rules</entity> , which contain <entity id="D08-1060.25">variables</entity> correspond to <entity id="D08-1060.26">source-side</entity> <entity id="D08-1060.27">syntactic</entity> <entity id="D08-1060.28">constituents</entity> . We demonstrate how these <entity id="D08-1060.29">rules</entity> are learned from <entity id="D08-1060.30">parallel <entity id="D08-1060.31">corpora</entity></entity> . Our <entity id="D08-1060.32">proposed</entity> shallow <entity id="D08-1060.33">Tree-to-</entity> <entity id="D08-1060.34">String</entity> <entity id="D08-1060.35">rules</entity> show significant <entity id="D08-1060.36">improvements</entity> in <entity id="D08-1060.37">translation <entity id="D08-1060.38">quality</entity></entity> across different <entity id="D08-1060.39">test sets</entity> .
</abstract>


</text>

<text id="I05-1060">
<title><entity id="I05-1060.1">Automatic</entity> <entity id="I05-1060.2">Acquisition</entity> of <entity id="I05-1060.3">Basic</entity> Katakana <entity id="I05-1060.4">Lexicon</entity> from a Given <entity id="I05-1060.5">Corpus</entity></title> 
<abstract><entity id="I05-1060.6">Abstract</entity> .
</abstract>


</text>

<text id="I05-1063">
<title>
A Twin- <entity id="I05-1063.1">Candidate</entity> <entity id="I05-1063.2">Model</entity> of <entity id="I05-1063.3">Coreference <entity id="I05-1063.4">Resolution</entity></entity> with Non-Anaphor <entity id="I05-1063.5">Identification</entity> <entity id="I05-1063.6">Capability</entity></title> 
<abstract><entity id="I05-1063.7">Abstract</entity> .
</abstract>


</text>

<text id="I05-2039">
<title>The <entity id="I05-2039.1">Influence</entity> of <entity id="I05-2039.2">Data</entity> <entity id="I05-2039.3">Homogeneity</entity> on <entity id="I05-2039.4">NLP System</entity> <entity id="I05-2039.5">Performance</entity></title>
<abstract>
In this work we <entity id="I05-2039.6">study</entity> the <entity id="I05-2039.7">influence</entity> of <entity id="I05-2039.8">corpus</entity> <entity id="I05-2039.9">homogeneity</entity> on <entity id="I05-2039.10">corpus-based</entity> <entity id="I05-2039.11">NLP system</entity> <entity id="I05-2039.12">performance</entity> . <entity id="I05-2039.13">Experiments</entity> are <entity id="I05-2039.14">performed</entity> on both stochastic <entity id="I05-2039.15">language <entity id="I05-2039.16">models</entity></entity> and an EBMT <entity id="I05-2039.17">system</entity> <entity id="I05-2039.18">translating</entity> from <entity id="I05-2039.19">Japanese</entity> to <entity id="I05-2039.20">English</entity> with a large bicorpus, in <entity id="I05-2039.21">order</entity> to reassess the <entity id="I05-2039.22">assumption</entity> that using only homogeneous <entity id="I05-2039.23">data</entity> tends to make <entity id="I05-2039.24">system</entity> <entity id="I05-2039.25">performance</entity> go up. We describe a <entity id="I05-2039.26">method</entity> to represent <entity id="I05-2039.27">corpus</entity> <entity id="I05-2039.28">homogeneity</entity> as a <entity id="I05-2039.29">distribution</entity> of <entity id="I05-2039.30">similarity</entity> <entity id="I05-2039.31">coefficients</entity> <entity id="I05-2039.32">based</entity> on a <entity id="I05-2039.33">cross-entropic</entity> measure investigated in previous works. We show that beyond minimal <entity id="I05-2039.34">sizes</entity> of <entity id="I05-2039.35">training</entity> <entity id="I05-2039.36">data</entity> the excessive <entity id="I05-2039.37">elimination</entity> of heterogeneous <entity id="I05-2039.38">data</entity> proves prejudicial in <entity id="I05-2039.39">terms</entity> of both <entity id="I05-2039.40">perplexity</entity> and <entity id="I05-2039.41">translation quality</entity> : excessively restricting the <entity id="I05-2039.42">training</entity> <entity id="I05-2039.43">data</entity> to a particular <entity id="I05-2039.44">domain</entity> may be prejudicial in <entity id="I05-2039.45">terms</entity> of In- <entity id="I05-2039.46">Domain</entity> <entity id="I05-2039.47">system</entity> <entity id="I05-2039.48">performance</entity> , and that heterogeneous, Out-of- <entity id="I05-2039.49">Domain</entity> <entity id="I05-2039.50">data</entity> may in fact contribute to better sytem <entity id="I05-2039.51">performance</entity> .
</abstract>


</text>

<text id="I05-2046">
<title>
Using <entity id="I05-2046.1">Maximum Entropy</entity> to <entity id="I05-2046.2">Extract</entity> Biomedical <entity id="I05-2046.3">Named</entity> Entities without Dictionaries
</title>
<abstract><entity id="I05-2046.4">Current</entity> NER <entity id="I05-2046.5">approaches</entity> <entity id="I05-2046.6">include</entity> : <entity id="I05-2046.7">dictionary-based</entity> , <entity id="I05-2046.8">rule-based</entity> , or <entity id="I05-2046.9">machine learning</entity> . Since there is no consolidated nomenclature for most biomedical NEs, most NER <entity id="I05-2046.10">systems</entity> relying on limited <entity id="I05-2046.11">dictionaries</entity> or <entity id="I05-2046.12">rules</entity> do not <entity id="I05-2046.13">perform</entity> satisfactorily. In this <entity id="I05-2046.14">paper</entity> , we <entity id="I05-2046.15">apply</entity> <entity id="I05-2046.16">Maximum <entity id="I05-2046.17">Entropy</entity></entity> (ME) to <entity id="I05-2046.18">construct</entity> our NER <entity id="I05-2046.19">framework</entity> . We represent shallow <entity id="I05-2046.20">linguistic information</entity> as <entity id="I05-2046.21">linguistic features</entity> in our ME <entity id="I05-2046.22">model</entity> . On the GENIA 3.02 <entity id="I05-2046.23">corpus</entity> , our <entity id="I05-2046.24">system</entity> achieves satisfactory F-scores of 74.3% in <entity id="I05-2046.25">protein</entity> and 70.0% overall without using any <entity id="I05-2046.26">dictionary</entity> . Our <entity id="I05-2046.27">system</entity> <entity id="I05-2046.28">performs</entity> significantly better than <entity id="I05-2046.29"><entity id="I05-2046.30">dictionary-based</entity> <entity id="I05-2046.31">systems</entity></entity> . Using <entity id="I05-2046.32">partial</entity> <entity id="I05-2046.33">match</entity> <entity id="I05-2046.34">criteria</entity> , our <entity id="I05-2046.35">system</entity> achieves an F-score of 81.3%. Using appropriate <entity id="I05-2046.36">domain knowledge</entity> to modify the <entity id="I05-2046.37">boundaries</entity> , our <entity id="I05-2046.38">system</entity> has the potential to achieve an F-score of over 80%.
</abstract>


</text>

<text id="I05-3016">
<title>
Resolving Pronominal References in <entity id="I05-3016.1">Chinese</entity> with the Hobbs <entity id="I05-3016.2">Algorithm</entity></title>
<abstract>
"This <entity id="I05-3016.3">study</entity> addresses pronominal <entity id="I05-3016.4">anaphora <entity id="I05-3016.5">resolution</entity></entity> , <entity id="I05-3016.6">including</entity> zero pronouns, in <entity id="I05-3016.7">Chinese</entity> . A <entity id="I05-3016.8">syntactic</entity> , <entity id="I05-3016.9">rule-based</entity> pronoun <entity id="I05-3016.10">resolution</entity> <entity id="I05-3016.11">algorithm</entity> , the ""Hobbs <entity id="I05-3016.12">algorithm</entity> "" was run on ""<entity id="I05-3016.13">gold <entity id="I05-3016.14">standard</entity></entity> "" <entity id="I05-3016.15">hand</entity> <entity id="I05-3016.16">parses</entity> from the Penn <entity id="I05-3016.17">Chinese</entity> Treebank. While first <entity id="I05-3016.18">proposed</entity> for <entity id="I05-3016.19">English</entity> , the <entity id="I05-3016.20">algorithm</entity> counts for its <entity id="I05-3016.21">success</entity> on two <entity id="I05-3016.22">characteristics</entity> that <entity id="I05-3016.23">Chinese</entity> and <entity id="I05-3016.24">English</entity> have in <entity id="I05-3016.25">common</entity> . Both <entity id="I05-3016.26">languages</entity> are SVO, and both are fixed <entity id="I05-3016.27">word</entity> <entity id="I05-3016.28">order</entity> <entity id="I05-3016.29">languages</entity> . No changes were made to <entity id="I05-3016.30">adapt</entity> the <entity id="I05-3016.31">algorithm</entity> to <entity id="I05-3016.32">Chinese</entity> . The <entity id="I05-3016.33">accuracy</entity> of the <entity id="I05-3016.34">algorithm</entity> on overt, third-person pronouns at the <entity id="I05-3016.35">matrix</entity> <entity id="I05-3016.36">level</entity> was 77.6%, and the <entity id="I05-3016.37">accuracy</entity> for resolving <entity id="I05-3016.38">matrix-level</entity> zero pronouns was 73.3%. In <entity id="I05-3016.39">contrast</entity> , the <entity id="I05-3016.40">accuracy</entity> of the <entity id="I05-3016.41">algorithm</entity> on pronouns that appeared in subordinate <entity id="I05-3016.42">constructions</entity> was only 43.3%, <entity id="I05-3016.43">providing</entity> <entity id="I05-3016.44">support</entity> for Miltsakaki 's <entity id="I05-3016.45">two-mechanism</entity> <entity id="I05-3016.46">proposal</entity> for resolving inter- vs. "
</abstract>


</text>

<text id="I05-3034">
<title><entity id="I05-3034.1">Chinese <entity id="I05-3034.2">Word Segmentation</entity></entity> <entity id="I05-3034.3">Based</entity> On Direct <entity id="I05-3034.4">Maximum <entity id="I05-3034.5">Entropy</entity> <entity id="I05-3034.6">Model</entity></entity></title>
<abstract>
	Och, Franz Josef ; Ney , Hermann ,Discriminative <entity id="I05-3034.7">Training</entity> And <entity id="I05-3034.8">Maximum <entity id="I05-3034.9">Entropy</entity></entity> <entity id="I05-3034.10">Models</entity> For <entity id="I05-3034.11">Statistical <entity id="I05-3034.12">Machine Translation</entity></entity> ,Annual Meeting Of The <entity id="I05-3034.13">Association</entity> For <entity id="I05-3034.14">Computation</entity> al <entity id="I05-3034.15">Linguistics</entity> ,2002 *** Gao , Jianfeng ; Li , Mu; Huang , Changning,Improved <entity id="I05-3034.16">Source-</entity> <entity id="I05-3034.17">Channel</entity> <entity id="I05-3034.18">Models</entity> For <entity id="I05-3034.19">Chinese <entity id="I05-3034.20">Word Segmentation</entity></entity> ,Annual Meeting Of The <entity id="I05-3034.21">Association</entity> For <entity id="I05-3034.22">Computation</entity> al <entity id="I05-3034.23">Linguistics</entity> ,2003</abstract>


</text>

<text id="E03-1088">
<title>
Linguistic <entity id="E03-1088.1">Variation</entity> And <entity id="E03-1088.2">Computation</entity> (Invited Talk)
</title>
<abstract><entity id="E03-1088.3">Language</entity> variationists <entity id="E03-1088.4">study</entity> how <entity id="E03-1088.5">languages</entity> vary along geographical or social lines or along lines of age and <entity id="E03-1088.6">gender</entity> . Variationist <entity id="E03-1088.7">data</entity> is available and <entity id="E03-1088.8">challenging</entity> , in particular for dialectology, the <entity id="E03-1088.9">study</entity> of geographical <entity id="E03-1088.10">variation</entity> , which will be the <entity id="E03-1088.11">focus</entity> of this <entity id="E03-1088.12">paper</entity> , although we present <entity id="E03-1088.13">approaches</entity> we expect to <entity id="E03-1088.14">transfer</entity> smoothly to the <entity id="E03-1088.15">study</entity> of <entity id="E03-1088.16">variation</entity> correlating with other extralinguistic <entity id="E03-1088.17">variables</entity> . <entity id="E03-1088.18">Techniques</entity> from <entity id="E03-1088.19">computational linguistics</entity> on the one <entity id="E03-1088.20">hand</entity> , and <entity id="E03-1088.21">standard</entity> <entity id="E03-1088.22">statistical</entity> <entity id="E03-1088.23">data</entity> <entity id="E03-1088.24">reduction</entity> <entity id="E03-1088.25">techniques</entity> on the other, not only shed light on this classic linguistic <entity id="E03-1088.26">problem</entity> , but they also suggest avenues for exploring the <entity id="E03-1088.27">question</entity> at more <entity id="E03-1088.28">abstract</entity> <entity id="E03-1088.29">levels</entity> , and perhaps for seeking the determinants of <entity id="E03-1088.30">variation</entity> .
</abstract>


</text>

<text id="C96-2164">
<title>
A <entity id="C96-2164.1">Method</entity> For Abstracting <entity id="C96-2164.2">Newspaper</entity> Articles By Using <entity id="C96-2164.3">Surface</entity> Clues
</title>
<abstract>
This <entity id="C96-2164.4">paper</entity> describes a <entity id="C96-2164.5">system</entity> which automatically creates an <entity id="C96-2164.6">abstract</entity> of a <entity id="C96-2164.7">newspaper</entity> article by selecting important <entity id="C96-2164.8">sentences</entity> of a given <entity id="C96-2164.9">text</entity> . To determine the <entity id="C96-2164.10">importance</entity> of a <entity id="C96-2164.11">sentence</entity> , several superficial <entity id="C96-2164.12">features</entity> are considered, and <entity id="C96-2164.13">weights</entity> for <entity id="C96-2164.14">features</entity> are determined by <entity id="C96-2164.15">multiple-regression</entity> <entity id="C96-2164.16">analysis</entity> of a <entity id="C96-2164.17">hand</entity> <entity id="C96-2164.18">processed</entity> <entity id="C96-2164.19">corpus</entity> .
</abstract>


</text>

<text id="C00-1036">
<title>
XML And Multilingual <entity id="C00-1036.1">Document</entity> Authoring: Convergent Trends
</title>
<abstract>
Typical <entity id="C00-1036.2">approaches</entity> to XML authoring view a XML <entity id="C00-1036.3">document</entity> as a <entity id="C00-1036.4">mixture</entity> of <entity id="C00-1036.5">structure</entity> (the <entity id="C00-1036.6">tags</entity> ) and <entity id="C00-1036.7">surface</entity> ( <entity id="C00-1036.8">text</entity> between the <entity id="C00-1036.9">tags</entity> ). We advocate a radical <entity id="C00-1036.10">approach</entity> where the <entity id="C00-1036.11">surface</entity> disappears from the XML <entity id="C00-1036.12">document</entity> altogether to be handled exclusively by rendering <entity id="C00-1036.13">mechanisms</entity> . This move is <entity id="C00-1036.14">based</entity> on the view that the author's <entity id="C00-1036.15">choices</entity> when authoring XML <entity id="C00-1036.16">documents</entity> are best seen as <entity id="C00-1036.17">language-neutral</entity> <entity id="C00-1036.18">semantic</entity> <entity id="C00-1036.19">decisions</entity> , that the <entity id="C00-1036.20">structure</entity> can then be viewed as interlingual <entity id="C00-1036.21">content</entity> , and that the textual <entity id="C00-1036.22">output</entity> should be derived from this <entity id="C00-1036.23">content</entity> by <entity id="C00-1036.24">language-specific</entity> <entity id="C00-1036.25">realization</entity> <entity id="C00-1036.26">mechanisms</entity> , thus assimilating XML authoring to Multilingual <entity id="C00-1036.27">Document</entity> Authoring. However, <entity id="C00-1036.28">standard</entity> XML <entity id="C00-1036.29">tools</entity> have important <entity id="C00-1036.30">limitations</entity> when used for such a <entity id="C00-1036.31">purpose</entity> : (1) they are weak at propagating <entity id="C00-1036.32">semantic</entity> <entity id="C00-1036.33">dependencies</entity> between different <entity id="C00-1036.34">parts</entity> of the <entity id="C00-1036.35">structure</entity> , and, (2) <entity id="C00-1036.36">current</entity> XML rendering <entity id="C00-1036.37">tools</entity> are ill-suited for handling the grammatical <entity id="C00-1036.38">combination</entity> of textual <entity id="C00-1036.39">units</entity> . We present two related <entity id="C00-1036.40">proposals</entity> for overcoming these <entity id="C00-1036.41">limitations</entity> : one (GF) originating in the tradition of mathematical proof editors and constructive <entity id="C00-1036.42">type</entity> <entity id="C00-1036.43">theory</entity> , the other (IG), a specialization of Definite <entity id="C00-1036.44">Clause</entity> Grammars strongly inspired by GF.
</abstract>


</text>

<text id="C02-1041">
<title><entity id="C02-1041.1">Automatic</entity> <entity id="C02-1041.2">Semantic</entity> Grouping In A <entity id="C02-1041.3">Spoken Language</entity> <entity id="C02-1041.4">User Interface</entity> Toolkit
</title>
<abstract>
With the rapid growth of real <entity id="C02-1041.5">application</entity> <entity id="C02-1041.6">domains</entity> for <entity id="C02-1041.7">NLP systems</entity> , there is a genuine demand for a general toolkit from which programmers with no <entity id="C02-1041.8">linguistic knowledge</entity> can build specific <entity id="C02-1041.9">NLP systems</entity> . Such a toolkit should <entity id="C02-1041.10">provide</entity> an <entity id="C02-1041.11">interface</entity> to accept <entity id="C02-1041.12">sample</entity> <entity id="C02-1041.13">sentences</entity> and convert them into <entity id="C02-1041.14">semantic <entity id="C02-1041.15">representations</entity></entity> so as to allow programmers to <entity id="C02-1041.16">map</entity> them to <entity id="C02-1041.17">domain</entity> <entity id="C02-1041.18">actions</entity> . In <entity id="C02-1041.19">order</entity> to reduce the workload of managing a large <entity id="C02-1041.20">number</entity> of <entity id="C02-1041.21">semantic</entity> <entity id="C02-1041.22">forms</entity> individually, the toolkit will <entity id="C02-1041.23">perform</entity> what we <entity id="C02-1041.24">call</entity> <entity id="C02-1041.25">semantic</entity> grouping to organize the <entity id="C02-1041.26">forms</entity> into meaningful groups. In this <entity id="C02-1041.27">paper</entity> , we present three <entity id="C02-1041.28">semantic</entity> grouping <entity id="C02-1041.29">methods</entity> : <entity id="C02-1041.30">similarity-based</entity> , <entity id="C02-1041.31">verb-based</entity> and <entity id="C02-1041.32">category-based</entity> grouping, and their <entity id="C02-1041.33">implementation</entity> in the SLUI toolkit. We also discuss the pros and <entity id="C02-1041.34">cons</entity> of each <entity id="C02-1041.35">method</entity> and how they can be utilized according to the different <entity id="C02-1041.36">domain</entity> needs.
</abstract>


</text>

<text id="E89-1008">
<title>
Paradigmatic <entity id="E89-1008.1">Morphology</entity></title>
<abstract>
We present a <entity id="E89-1008.2">notation</entity> for the declarative statement of morphological <entity id="E89-1008.3">relationships</entity> and <entity id="E89-1008.4">lexical</entity> <entity id="E89-1008.5">rules</entity> , <entity id="E89-1008.6">based</entity> on the traditional <entity id="E89-1008.7">notion</entity> of <entity id="E89-1008.8">Word</entity> and <entity id="E89-1008.9">Paradigm</entity> Elsewhere Condition, <entity id="E89-1008.10">string</entity> <entity id="E89-1008.11">equations</entity></abstract>


</text>

<text id="E99-1022">
<title>
Selective Magic HPSG <entity id="E99-1022.1">Parsing</entity></title>
<abstract>
We <entity id="E99-1022.2">propose</entity> a <entity id="E99-1022.3">parser</entity> for <entity id="E99-1022.4">constraint-logic</entity> grammars <entity id="E99-1022.5">implementing</entity> HPSG that combines the <entity id="E99-1022.6">advantages</entity> of dynamic bottom-up and <entity id="E99-1022.7">advanced</entity> top-down <entity id="E99-1022.8">control</entity> . The <entity id="E99-1022.9">parser</entity> allows the <entity id="E99-1022.10">user</entity> to <entity id="E99-1022.11">apply</entity> magic <entity id="E99-1022.12">compilation</entity> to specific <entity id="E99-1022.13">constraints</entity> in a grammar which as a <entity id="E99-1022.14">result</entity> can be <entity id="E99-1022.15">processed</entity> dynamically in a bottom-up and <entity id="E99-1022.16">goal-directed</entity> <entity id="E99-1022.17">fashion</entity> . State of the art top-down <entity id="E99-1022.18">processing</entity> <entity id="E99-1022.19">techniques</entity> are used to <entity id="E99-1022.20">deal</entity> with the remaining <entity id="E99-1022.21">constraints</entity> . We discuss various <entity id="E99-1022.22">aspects</entity> <entity id="E99-1022.23">concerning</entity> the <entity id="E99-1022.24">implementation</entity> of the <entity id="E99-1022.25">parser</entity> as <entity id="E99-1022.26">part</entity> of a grammar <entity id="E99-1022.27">development</entity> <entity id="E99-1022.28">system</entity> .
</abstract>


</text>

<text id="E99-1023">
<title>
Representing <entity id="E99-1023.1">Text</entity> Chunks
</title>
<abstract>
"Dividing <entity id="E99-1023.2">sentences</entity> in <entity id="E99-1023.3">chunks</entity> of <entity id="E99-1023.4">words</entity> is a useful preprocessing <entity id="E99-1023.5">step</entity> for <entity id="E99-1023.6">parsing</entity> , <entity id="E99-1023.7">information extraction</entity> and <entity id="E99-1023.8">information retrieval</entity> . ( Ramshaw and  Marcus, 1995 ) have introduced a ""convenient""<entity id="E99-1023.9">data</entity> <entity id="E99-1023.10">representation</entity> for <entity id="E99-1023.11">chunking</entity> by converting it to a <entity id="E99-1023.12">tagging</entity> <entity id="E99-1023.13">task</entity> . In this <entity id="E99-1023.14">paper</entity> we will examine seven different <entity id="E99-1023.15">data</entity> <entity id="E99-1023.16">representations</entity> for the <entity id="E99-1023.17">problem</entity> of recognizing <entity id="E99-1023.18">noun phrase</entity> <entity id="E99-1023.19">chunks</entity> . We will show that the the <entity id="E99-1023.20">data</entity> <entity id="E99-1023.21">representation</entity> <entity id="E99-1023.22">choice</entity> has a minor <entity id="E99-1023.23">influence</entity> on <entity id="E99-1023.24">chunking</entity> <entity id="E99-1023.25">performance</entity> . However, equipped with the most suitable <entity id="E99-1023.26">data</entity> <entity id="E99-1023.27">representation</entity> , our <entity id="E99-1023.28">memory-based</entity> <entity id="E99-1023.29">learning</entity> chunker was able to <entity id="E99-1023.30">improve</entity> the best published <entity id="E99-1023.31">chunking</entity> <entity id="E99-1023.32">results</entity> for a <entity id="E99-1023.33">standard</entity> <entity id="E99-1023.34">data</entity> set."
</abstract>


</text>

<text id="E99-1024">
<title><entity id="E99-1024.1">Detection</entity> Of <entity id="E99-1024.2">Japanese</entity> Homophone Errors By A <entity id="E99-1024.3">Decision</entity> <entity id="E99-1024.4">List</entity> Including A Written <entity id="E99-1024.5">Word</entity> As A <entity id="E99-1024.6">Default</entity> <entity id="E99-1024.7">Evidence</entity></title>
<abstract>
In this <entity id="E99-1024.8">paper</entity> , we <entity id="E99-1024.9">propose</entity> a practical <entity id="E99-1024.10">method</entity> to detect <entity id="E99-1024.11">Japanese</entity> homophone <entity id="E99-1024.12">errors</entity> in <entity id="E99-1024.13">Japanese</entity> <entity id="E99-1024.14">texts</entity> . It is very important to detect homophone <entity id="E99-1024.15">errors</entity> in <entity id="E99-1024.16">Japanese</entity> <entity id="E99-1024.17">revision</entity> <entity id="E99-1024.18">systems</entity> because <entity id="E99-1024.19">Japanese</entity> <entity id="E99-1024.20">texts</entity> suffer from homophone <entity id="E99-1024.21">errors</entity> frequently. In <entity id="E99-1024.22">order</entity> to detect homophone <entity id="E99-1024.23">errors</entity> , we have only to <entity id="E99-1024.24">solve</entity> the homophone <entity id="E99-1024.25">problem</entity> . We can use the <entity id="E99-1024.26">decision</entity> <entity id="E99-1024.27">list</entity> to do it because the homophone <entity id="E99-1024.28">problem</entity> is equivalent to the <entity id="E99-1024.29">word sense disambiguation</entity> <entity id="E99-1024.30">problem</entity> . However, the homophone <entity id="E99-1024.31">problem</entity> is different from the <entity id="E99-1024.32">word sense disambiguation</entity> <entity id="E99-1024.33">problem</entity> because the former can use the written <entity id="E99-1024.34">word</entity> but the latter cannot. In this <entity id="E99-1024.35">paper</entity> , we incorporate the written <entity id="E99-1024.36">word</entity> into the original <entity id="E99-1024.37">decision</entity> <entity id="E99-1024.38">list</entity> by obtaining the identifying <entity id="E99-1024.39">strength</entity> of the written <entity id="E99-1024.40">word</entity> . The <entity id="E99-1024.41">improved</entity> <entity id="E99-1024.42">decision</entity> <entity id="E99-1024.43">list</entity> can raise the F-measure of <entity id="E99-1024.44">error</entity> <entity id="E99-1024.45">detection</entity> .
</abstract>


</text>

<text id="E99-1025">
<title>
New <entity id="E99-1025.1">Models</entity> For Improving Supertag <entity id="E99-1025.2">Disambiguation</entity></title>
<abstract>
In previous work, supertag <entity id="E99-1025.3">disambiguation</entity> has been presented as a <entity id="E99-1025.4">robust</entity> <entity id="E99-1025.5">partial</entity> <entity id="E99-1025.6">parsing</entity> <entity id="E99-1025.7">technique</entity> . In this <entity id="E99-1025.8">paper</entity> we present two <entity id="E99-1025.9">approaches</entity> : contextual <entity id="E99-1025.10">models</entity> , which exploit a <entity id="E99-1025.11">variety</entity> of <entity id="E99-1025.12">features</entity> in <entity id="E99-1025.13">order</entity> to <entity id="E99-1025.14">improve</entity> supertag <entity id="E99-1025.15">performance</entity> , and <entity id="E99-1025.16">class-based models</entity> , which assign sets of supertags to <entity id="E99-1025.17">words</entity> in <entity id="E99-1025.18">order</entity> to substantially <entity id="E99-1025.19">improve</entity> <entity id="E99-1025.20">accuracy</entity> with only a slight <entity id="E99-1025.21">increase</entity> in <entity id="E99-1025.22">ambiguity</entity> .
</abstract>


</text>

<text id="E99-1026">
<title><entity id="E99-1026.1">Japanese</entity> <entity id="E99-1026.2">Dependency Structure</entity> <entity id="E99-1026.3">Analysis</entity> <entity id="E99-1026.4">Based</entity> On <entity id="E99-1026.5">Maximum Entropy</entity> <entity id="E99-1026.6">Models</entity></title>
<abstract>
This <entity id="E99-1026.7">paper</entity> describes a <entity id="E99-1026.8">dependency structure</entity> <entity id="E99-1026.9">analysis</entity> of <entity id="E99-1026.10">Japanese</entity> <entity id="E99-1026.11">sentences</entity> <entity id="E99-1026.12">based</entity> on the <entity id="E99-1026.13">maximum entropy models</entity> . Our <entity id="E99-1026.14">model</entity> is created by learning the <entity id="E99-1026.15">weights</entity> of some <entity id="E99-1026.16">features</entity> from a <entity id="E99-1026.17">training corpus</entity> to predict the <entity id="E99-1026.18">dependency</entity> between bunsetsus or phrasal <entity id="E99-1026.19">units</entity> . The <entity id="E99-1026.20">dependency</entity> <entity id="E99-1026.21">accuracy</entity> of our <entity id="E99-1026.22">system</entity> is 87.2% using the Kyoto <entity id="E99-1026.23">University</entity> <entity id="E99-1026.24">corpus</entity> . We discuss the <entity id="E99-1026.25">contribution</entity> of each <entity id="E99-1026.26">feature set</entity> and the <entity id="E99-1026.27">relationship</entity> between the <entity id="E99-1026.28">number</entity> of <entity id="E99-1026.29">training</entity> <entity id="E99-1026.30">data</entity> and the <entity id="E99-1026.31">accuracy</entity> .
</abstract>


</text>

<text id="P06-1043">
<title>
Reranking And Self- <entity id="P06-1043.1">Training</entity> For <entity id="P06-1043.2">Parser</entity> <entity id="P06-1043.3">Adaptation</entity></title>
<abstract>
"<entity id="P06-1043.4">Statistical</entity> " <entity id="P06-1043.5">parsers</entity> <entity id="P06-1043.6">trained</entity> and <entity id="P06-1043.7">tested</entity> on the Penn <entity id="P06-1043.8">Wall Street Journal</entity> (wsj) treebank have shown vast <entity id="P06-1043.9">improvements</entity> over the last 10 years. Much of this <entity id="P06-1043.10">improvement</entity> , however, is <entity id="P06-1043.11">based</entity> upon an ever-increasing <entity id="P06-1043.12">number</entity> of <entity id="P06-1043.13">features</entity> to be <entity id="P06-1043.14">trained</entity> on (typically) the wsj treebank <entity id="P06-1043.15">data</entity> . This has led to <entity id="P06-1043.16">concern</entity> that such <entity id="P06-1043.17">parsers</entity> may be too finely tuned to this <entity id="P06-1043.18">corpus</entity> at the expense of <entity id="P06-1043.19">portability</entity> to other <entity id="P06-1043.20">genres</entity> . Such worries have merit. The <entity id="P06-1043.21">standard</entity> ""Charniak <entity id="P06-1043.22">parser</entity> "" <entity id="P06-1043.23">checks</entity> in at a labeled <entity id="P06-1043.24">precision-recall</entity> f-measure of 89.7% on the Penn WSJ <entity id="P06-1043.25">test set</entity> , but only 82.9% on the <entity id="P06-1043.26">test set</entity> from the Brown treebank <entity id="P06-1043.27">corpus</entity> . This <entity id="P06-1043.28">paper</entity> should allay these fears. In particular, we show that the reranking <entity id="P06-1043.29">parser</entity> described in Charniak and Johnson (2005) <entity id="P06-1043.30">improves</entity> <entity id="P06-1043.31">performance</entity> of the <entity id="P06-1043.32">parser</entity> on Brown to 85.2%. Furthermore, use of the <entity id="P06-1043.33">self-training</entity> <entity id="P06-1043.34">techniques</entity> described in ( McClosky et al., 2006 ) raise this to 87.8% (an <entity id="P06-1043.35">error</entity> <entity id="P06-1043.36">reduction</entity> of 28%) again without any use of labeled Brown <entity id="P06-1043.37">data</entity> . This is remarkable since <entity id="P06-1043.38">training</entity> the <entity id="P06-1043.39">parser</entity> and reranker on labeled Brown <entity id="P06-1043.40">data</entity> achieves only 88.4%. "
</abstract>


</text>

<text id="P06-1044">
<title><entity id="P06-1044.1">Automatic</entity> <entity id="P06-1044.2">Classification</entity> Of Verbs In Biomedical <entity id="P06-1044.3">Texts</entity></title> 
<abstract><entity id="P06-1044.4">Lexical</entity> <entity id="P06-1044.5">classes</entity> , when tailored to the <entity id="P06-1044.6">application</entity> and <entity id="P06-1044.7">domain</entity> in <entity id="P06-1044.8">question</entity> , can <entity id="P06-1044.9">provide</entity> an effective means to <entity id="P06-1044.10">deal</entity> with a <entity id="P06-1044.11">number</entity> of <entity id="P06-1044.12">natural language processing</entity> (nlp) <entity id="P06-1044.13">tasks</entity> . While <entity id="P06-1044.14">manual</entity> <entity id="P06-1044.15">construction</entity> of such <entity id="P06-1044.16">classes</entity> is difficult, recent <entity id="P06-1044.17">research</entity> shows that it is possible to automatically induce <entity id="P06-1044.18">verb <entity id="P06-1044.19">classes</entity></entity> from <entity id="P06-1044.20">cross-domain</entity> <entity id="P06-1044.21">corpora</entity> with promising <entity id="P06-1044.22">accuracy</entity> . We <entity id="P06-1044.23">report</entity> a novel <entity id="P06-1044.24">experiment</entity> where similar <entity id="P06-1044.25">technology</entity> is <entity id="P06-1044.26">applied</entity> to the important, <entity id="P06-1044.27">challenging</entity> <entity id="P06-1044.28">domain</entity> of biomedicine. We show that the <entity id="P06-1044.29">resulting</entity> <entity id="P06-1044.30">classification</entity> , acquired from a <entity id="P06-1044.31">corpus</entity> of biomedical <entity id="P06-1044.32">journal</entity> articles, is highly accurate and strongly <entity id="P06-1044.33">domain-specific</entity> . It can be used to aid bio-nlp directly or as useful material for investigating the <entity id="P06-1044.34">syntax</entity> and <entity id="P06-1044.35">semantics</entity> of <entity id="P06-1044.36">verbs</entity> in biomedical <entity id="P06-1044.37">texts</entity> .
</abstract>


</text>

<text id="P06-1082">
<title><entity id="P06-1082.1">Word Alignment</entity> In <entity id="P06-1082.2">English-</entity> Hindi <entity id="P06-1082.3">Parallel Corpus</entity> Using Recency- <entity id="P06-1082.4">Vector</entity> <entity id="P06-1082.5">Approach</entity> : Some <entity id="P06-1082.6">Studies</entity></title> 
<abstract><entity id="P06-1082.7">Word alignment</entity> using <entity id="P06-1082.8">recency-vector</entity> <entity id="P06-1082.9">based</entity> <entity id="P06-1082.10">approach</entity> has recently become popular. One major <entity id="P06-1082.11">advantage</entity> of these <entity id="P06-1082.12">techniques</entity> is that unlike other <entity id="P06-1082.13">approaches</entity> they <entity id="P06-1082.14">perform</entity> well even if the <entity id="P06-1082.15">size</entity> of the <entity id="P06-1082.16">parallel corpora</entity> is small. This makes these <entity id="P06-1082.17">algorithms</entity> worth-studying for <entity id="P06-1082.18">languages</entity> where <entity id="P06-1082.19">resources</entity> are scarce. In this work we <entity id="P06-1082.20">studied</entity> the <entity id="P06-1082.21">performance</entity> of two very popular <entity id="P06-1082.22">recency-vector</entity> <entity id="P06-1082.23">based</entity> <entity id="P06-1082.24">approaches</entity> , <entity id="P06-1082.25">proposed</entity> in ( Fung and  McKeown, 1994 ) and ( Somers, 1998 ), respectively, for <entity id="P06-1082.26">word <entity id="P06-1082.27">alignment</entity></entity> in <entity id="P06-1082.28">English</entity> - Hindi <entity id="P06-1082.29">parallel corpus</entity> . But <entity id="P06-1082.30">performance</entity> of the above <entity id="P06-1082.31">algorithms</entity> was not found to be satisfactory. However, subsequent <entity id="P06-1082.32">addition</entity> of some new <entity id="P06-1082.33">constraints</entity> <entity id="P06-1082.34">improved</entity> the <entity id="P06-1082.35">performance</entity> of the <entity id="P06-1082.36">recency-vector</entity> <entity id="P06-1082.37">based</entity> <entity id="P06-1082.38">alignment</entity> <entity id="P06-1082.39">technique</entity> significantly for the said <entity id="P06-1082.40">corpus</entity> . The present <entity id="P06-1082.41">paper</entity> discusses the new <entity id="P06-1082.42">version</entity> of the <entity id="P06-1082.43">algorithm</entity> and its <entity id="P06-1082.44">performance</entity> in <entity id="P06-1082.45">detail</entity> .
</abstract>


</text>

<text id="P06-1097">
<title>
Semi-Supervised <entity id="P06-1097.1">Training</entity> For <entity id="P06-1097.2">Statistical</entity> <entity id="P06-1097.3">Word Alignment</entity></title>
<abstract>
We introduce a semi-supervised <entity id="P06-1097.4">approach</entity> to <entity id="P06-1097.5">training</entity> for <entity id="P06-1097.6">statistical <entity id="P06-1097.7">machine translation</entity></entity> that alternates the traditional <entity id="P06-1097.8">Expectation</entity> <entity id="P06-1097.9">Maximization</entity> <entity id="P06-1097.10">step</entity> that is <entity id="P06-1097.11">applied</entity> on a large <entity id="P06-1097.12">training <entity id="P06-1097.13">corpus</entity></entity> with a discriminative <entity id="P06-1097.14">step</entity> aimed at <entity id="P06-1097.15">increasing</entity> <entity id="P06-1097.16">word-alignment</entity> <entity id="P06-1097.17">quality</entity> on a small, manually <entity id="P06-1097.18">word-aligned</entity> <entity id="P06-1097.19">sub-corpus</entity> . We show that our <entity id="P06-1097.20">algorithm</entity> leads not only to <entity id="P06-1097.21">improved</entity> <entity id="P06-1097.22">alignments</entity> but also to <entity id="P06-1097.23">machine translation</entity> <entity id="P06-1097.24">outputs</entity> of higher <entity id="P06-1097.25">quality</entity> .
</abstract>


</text>

<text id="P06-1099">
<title>
You Can't Beat <entity id="P06-1099.1">Frequency</entity> (Unless You Use <entity id="P06-1099.2">Linguistic Knowledge</entity> ) - A Qualitative <entity id="P06-1099.3">Evaluation</entity> Of <entity id="P06-1099.4">Association</entity> Measures For <entity id="P06-1099.5">Collocation</entity> And <entity id="P06-1099.6">Term</entity> <entity id="P06-1099.7">Extraction</entity></title>
<abstract>
In the past years, a <entity id="P06-1099.8">number</entity> of <entity id="P06-1099.9">lexical</entity> <entity id="P06-1099.10">association</entity> measures have been <entity id="P06-1099.11">studied</entity> to <entity id="P06-1099.12">help</entity> <entity id="P06-1099.13">extract</entity> new scientific <entity id="P06-1099.14">terminology</entity> or <entity id="P06-1099.15">general-language</entity> <entity id="P06-1099.16">collocations</entity> . The implicit <entity id="P06-1099.17">assumption</entity> of this <entity id="P06-1099.18">research</entity> was that newly <entity id="P06-1099.19">designed</entity> <entity id="P06-1099.20">term</entity> measures involving more sophisticated <entity id="P06-1099.21">statistical</entity> <entity id="P06-1099.22">criteria</entity> would outperform <entity id="P06-1099.23">simple</entity> counts of cooccurrence <entity id="P06-1099.24">frequencies</entity> . We here explicitly <entity id="P06-1099.25">test</entity> this <entity id="P06-1099.26">assumption</entity> . By way of four qualitative <entity id="P06-1099.27">criteria</entity> , we show that purely <entity id="P06-1099.28">statistics-based</entity> measures reveal virtually no <entity id="P06-1099.29">difference</entity> compared with <entity id="P06-1099.30">frequency</entity> of <entity id="P06-1099.31">occurrence</entity> counts, while linguistically more informed <entity id="P06-1099.32">metrics</entity> do reveal such a marked <entity id="P06-1099.33">difference</entity> .
</abstract>


</text>

<text id="P06-2002">
<title>
A Rote <entity id="P06-2002.1">Extractor</entity> With Edit <entity id="P06-2002.2">Distance-</entity> <entity id="P06-2002.3">Based</entity> Generalisation And Multi- <entity id="P06-2002.4">Corpora</entity> <entity id="P06-2002.5">Precision</entity> <entity id="P06-2002.6">Calculation</entity></title>
<abstract>In this <entity id="P06-2002.7">paper</entity> , we describe a rote <entity id="P06-2002.8">extractor</entity> that learns <entity id="P06-2002.9">patterns</entity> for finding <entity id="P06-2002.10">semantic</entity> <entity id="P06-2002.11">relationships</entity> in unrestricted <entity id="P06-2002.12">text</entity> , with new <entity id="P06-2002.13">procedures</entity> for <entity id="P06-2002.14">pattern</entity> <entity id="P06-2002.15">generalization</entity> and scoring. These <entity id="P06-2002.16">include</entity> the use of <entity id="P06-2002.17">part-of-speech</entity> <entity id="P06-2002.18">tags</entity> to guide the <entity id="P06-2002.19">generalization</entity> , <entity id="P06-2002.20">Named</entity> <entity id="P06-2002.21">Entity</entity> <entity id="P06-2002.22">categories</entity> inside the <entity id="P06-2002.23">patterns</entity> , an <entity id="P06-2002.24">edit-distance-based</entity> <entity id="P06-2002.25">pattern</entity> <entity id="P06-2002.26">generalization</entity> <entity id="P06-2002.27">algorithm</entity> , and a <entity id="P06-2002.28">pattern</entity> <entity id="P06-2002.29">accuracy</entity> <entity id="P06-2002.30">calculation</entity> <entity id="P06-2002.31">procedure</entity> <entity id="P06-2002.32">based</entity> on <entity id="P06-2002.33">evaluating</entity> the <entity id="P06-2002.34">patterns</entity> on several <entity id="P06-2002.35">test</entity> <entity id="P06-2002.36">corpora</entity> . In an <entity id="P06-2002.37">evaluation</entity> with 14 <entity id="P06-2002.38">entities</entity> , the <entity id="P06-2002.39">system</entity> attains a <entity id="P06-2002.40">precision</entity> higher than 50% for half of the <entity id="P06-2002.41">relationships</entity> considered.
</abstract>


</text>

<text id="P06-2013">
<title>
An Empirical <entity id="P06-2013.1">Study</entity> Of <entity id="P06-2013.2">Chinese</entity> Chunking
</title>
<abstract>
In this <entity id="P06-2013.3">paper</entity> , we describe an empirical <entity id="P06-2013.4">study</entity> of <entity id="P06-2013.5">Chinese</entity> <entity id="P06-2013.6">chunking</entity> on a <entity id="P06-2013.7">corpus</entity> , which is <entity id="P06-2013.8">extracted</entity> from UPENN <entity id="P06-2013.9">Chinese</entity> Treebank-4 (CTB4). First, we compare the <entity id="P06-2013.10">performance</entity> of the state-of-the-art <entity id="P06-2013.11">machine</entity> learning <entity id="P06-2013.12">models</entity> . Then we <entity id="P06-2013.13">propose</entity> two <entity id="P06-2013.14">approaches</entity> in <entity id="P06-2013.15">order</entity> to <entity id="P06-2013.16">improve</entity> the <entity id="P06-2013.17">performance</entity> of <entity id="P06-2013.18">Chinese</entity> <entity id="P06-2013.19">chunking</entity> . 1) We <entity id="P06-2013.20">propose</entity> an <entity id="P06-2013.21">approach</entity> to resolve the special <entity id="P06-2013.22">problems</entity> of <entity id="P06-2013.23">Chinese</entity> <entity id="P06-2013.24">chunking</entity> . This <entity id="P06-2013.25">approach</entity> extends the <entity id="P06-2013.26">chunk</entity> <entity id="P06-2013.27">tags</entity> for every <entity id="P06-2013.28">problem</entity> by a <entity id="P06-2013.29">tag-extension</entity> <entity id="P06-2013.30">function</entity> . 2) We <entity id="P06-2013.31">propose</entity> two novel voting <entity id="P06-2013.32">methods</entity> <entity id="P06-2013.33">based</entity> on the <entity id="P06-2013.34">characteristics</entity> of <entity id="P06-2013.35">chunking</entity> <entity id="P06-2013.36">task</entity> . Compared with traditional voting <entity id="P06-2013.37">methods</entity> , the <entity id="P06-2013.38">proposed</entity> voting <entity id="P06-2013.39">methods</entity> consider long <entity id="P06-2013.40">distance</entity> <entity id="P06-2013.41">information</entity> . The <entity id="P06-2013.42">experimental</entity> <entity id="P06-2013.43">results</entity> show that the SVMs <entity id="P06-2013.44">model</entity> outperforms the other <entity id="P06-2013.45">models</entity> and that our <entity id="P06-2013.46">proposed</entity> <entity id="P06-2013.47">approaches</entity> can <entity id="P06-2013.48">improve</entity> <entity id="P06-2013.49">performance</entity> significantly.
</abstract>


</text>

<text id="P06-2014">
<title>
Soft <entity id="P06-2014.1">Syntactic</entity> Constraints For <entity id="P06-2014.2">Word <entity id="P06-2014.3">Alignment</entity></entity> Through Discriminative <entity id="P06-2014.4">Training</entity></title> 
<abstract><entity id="P06-2014.5">Word alignment</entity> <entity id="P06-2014.6">methods</entity> can <entity id="P06-2014.7">gain</entity> valuable guidance by ensuring that their <entity id="P06-2014.8">alignments</entity> maintain <entity id="P06-2014.9">cohesion</entity> with <entity id="P06-2014.10">respect</entity> to the <entity id="P06-2014.11">phrases</entity> specified by a monolingual <entity id="P06-2014.12">dependency tree</entity> . However, this hard <entity id="P06-2014.13">constraint</entity> can also <entity id="P06-2014.14">rule</entity> out correct <entity id="P06-2014.15">alignments</entity> , and its <entity id="P06-2014.16">utility</entity> decreases as <entity id="P06-2014.17">alignment <entity id="P06-2014.18">models</entity></entity> become more <entity id="P06-2014.19">complex</entity> . We use a publicly available <entity id="P06-2014.20">structured</entity> <entity id="P06-2014.21">output</entity> SVM to create a max-margin <entity id="P06-2014.22">syntactic</entity> aligner with a soft <entity id="P06-2014.23">cohesion</entity> <entity id="P06-2014.24">constraint</entity> . The <entity id="P06-2014.25">resulting</entity> aligner is the first, to our <entity id="P06-2014.26">knowledge</entity> , to use a discriminative <entity id="P06-2014.27">learning <entity id="P06-2014.28">method</entity></entity> to <entity id="P06-2014.29">train</entity> an ITG bitext <entity id="P06-2014.30">parser</entity> .
</abstract>


</text>

<text id="P06-2023">
<title>
A Bio-Inspired <entity id="P06-2023.1">Approach</entity> For Multi- <entity id="P06-2023.2">Word</entity> <entity id="P06-2023.3">Expression</entity> <entity id="P06-2023.4">Extraction</entity></title>
<abstract>This <entity id="P06-2023.5">paper</entity> <entity id="P06-2023.6">proposes</entity> a new <entity id="P06-2023.7">approach</entity> for <entity id="P06-2023.8">Multi-word</entity> <entity id="P06-2023.9">Expression</entity> (MWE) <entity id="P06-2023.10">extraction</entity> on the <entity id="P06-2023.11">motivation</entity> of <entity id="P06-2023.12">gene</entity> <entity id="P06-2023.13">sequence</entity> <entity id="P06-2023.14">alignment</entity> because textual <entity id="P06-2023.15">sequence</entity> is similar to <entity id="P06-2023.16">gene</entity> <entity id="P06-2023.17">sequence</entity> in <entity id="P06-2023.18">pattern</entity> <entity id="P06-2023.19">analysis</entity> . <entity id="P06-2023.20">Theory</entity> of Longest <entity id="P06-2023.21">Common</entity> <entity id="P06-2023.22">Subsequence</entity> (LCS) originates from <entity id="P06-2023.23">computer science</entity> and has been established as affine <entity id="P06-2023.24">gap</entity> <entity id="P06-2023.25">model</entity> in Bioinformatics. We <entity id="P06-2023.26">perform</entity> this developed LCS <entity id="P06-2023.27">technique</entity> combined with linguistic <entity id="P06-2023.28">criteria</entity> in MWE <entity id="P06-2023.29">extraction</entity> . In <entity id="P06-2023.30">comparison</entity> with traditional <entity id="P06-2023.31">n-gram</entity> <entity id="P06-2023.32">method</entity> , which is the major <entity id="P06-2023.33">technique</entity> for MWE <entity id="P06-2023.34">extraction</entity> , LCS <entity id="P06-2023.35">approach</entity> is <entity id="P06-2023.36">applied</entity> with great <entity id="P06-2023.37">efficiency</entity> and <entity id="P06-2023.38">performance</entity> guarantee. <entity id="P06-2023.39">Experimental</entity> <entity id="P06-2023.40">results</entity> show that <entity id="P06-2023.41">LCS-based <entity id="P06-2023.42">approach</entity></entity> achieves better <entity id="P06-2023.43">results</entity> than <entity id="P06-2023.44">n-gram</entity> .
</abstract>


</text>

<text id="P06-2047">
<title>
Graph Branch <entity id="P06-2047.1">Algorithm</entity> : An Optimum <entity id="P06-2047.2">Tree</entity> <entity id="P06-2047.3">Search</entity> <entity id="P06-2047.4">Method</entity> For Scored <entity id="P06-2047.5">Dependency Graph</entity> With <entity id="P06-2047.6">Arc</entity> Co- <entity id="P06-2047.7">Occurrence</entity> Constraints
</title>
<abstract>
"Various <entity id="P06-2047.8">kinds</entity> of scored <entity id="P06-2047.9">dependency graphs</entity> are <entity id="P06-2047.10">proposed</entity> as packed shared <entity id="P06-2047.11">data</entity> <entity id="P06-2047.12">structures</entity> in <entity id="P06-2047.13">combination</entity> with optimum <entity id="P06-2047.14">dependency tree</entity> <entity id="P06-2047.15">search algorithms</entity> . This <entity id="P06-2047.16">paper</entity> classifies the scored <entity id="P06-2047.17">dependency graphs</entity> and discusses the specific <entity id="P06-2047.18">features</entity> of the ""<entity id="P06-2047.19">Dependency</entity> <entity id="P06-2047.20">Forest</entity> "" (DF) which is the packed shared <entity id="P06-2047.21">data</entity> <entity id="P06-2047.22">structure</entity> adopted in the ""<entity id="P06-2047.23">Preference</entity> <entity id="P06-2047.24">Dependency Grammar</entity> "" (PDG), and <entity id="P06-2047.25">proposes</entity> the ""Graph Branch <entity id="P06-2047.26">Algorithm</entity> "" for <entity id="P06-2047.27">computing</entity> the optimum <entity id="P06-2047.28">dependency tree</entity> from a DF. This <entity id="P06-2047.29">paper</entity> also <entity id="P06-2047.30">reports</entity> the <entity id="P06-2047.31">experiment</entity> showing the <entity id="P06-2047.32">computational</entity> <entity id="P06-2047.33">amount</entity> and <entity id="P06-2047.34">behavior</entity> of the graph branch <entity id="P06-2047.35">algorithm</entity> . "
</abstract>


</text>

<text id="P06-2091">
<title>
Translating HPSG-Style Outputs Of A <entity id="P06-2091.1">Robust</entity> <entity id="P06-2091.2">Parser</entity> Into Typed Dynamic <entity id="P06-2091.3">Logic</entity></title>
<abstract>
The present <entity id="P06-2091.4">paper</entity> <entity id="P06-2091.5">proposes</entity> a <entity id="P06-2091.6">method</entity> by which to <entity id="P06-2091.7">translate</entity> <entity id="P06-2091.8">outputs</entity> of a <entity id="P06-2091.9">robust</entity> HPSG <entity id="P06-2091.10">parser</entity> into <entity id="P06-2091.11">semantic <entity id="P06-2091.12">representations</entity></entity> of Typed Dynamic <entity id="P06-2091.13">Logic</entity> (TDL), a dynamic plural <entity id="P06-2091.14">semantics</entity> defined in <entity id="P06-2091.15">typed</entity> lambda <entity id="P06-2091.16">calculus</entity> . With its <entity id="P06-2091.17">higher-order</entity> <entity id="P06-2091.18">representations</entity> of <entity id="P06-2091.19">contexts</entity> , TDL analyzes and describes the inherently inter-sentential <entity id="P06-2091.20">nature</entity> of <entity id="P06-2091.21">quantification</entity> and anaphora in a strictly lexicalized and compositional <entity id="P06-2091.22">manner</entity> . The present <entity id="P06-2091.23">study</entity> shows that the <entity id="P06-2091.24">proposed</entity> <entity id="P06-2091.25">translation</entity> <entity id="P06-2091.26">method</entity> successfully combines <entity id="P06-2091.27">robustness</entity> and descriptive <entity id="P06-2091.28">adequacy</entity> of contemporary <entity id="P06-2091.29">semantics</entity> . The present <entity id="P06-2091.30">implementation</entity> achieves high <entity id="P06-2091.31">coverage</entity> , approximately 90%, for the real <entity id="P06-2091.32">text</entity> of the <entity id="P06-2091.33">Penn Treebank</entity> <entity id="P06-2091.34">corpus</entity> .
</abstract>


</text>

<text id="P06-2096">
<title>
Adding <entity id="P06-2096.1">Syntax</entity> To <entity id="P06-2096.2">Dynamic Programming</entity> For Aligning Comparable <entity id="P06-2096.3">Texts</entity> For The <entity id="P06-2096.4">Generation</entity> Of Paraphrases
</title>
<abstract>
Multiple <entity id="P06-2096.5">sequence</entity> <entity id="P06-2096.6">alignment</entity> <entity id="P06-2096.7">techniques</entity> have recently <entity id="P06-2096.8">gained</entity> <entity id="P06-2096.9">popularity</entity> in the <entity id="P06-2096.10">Natural Language</entity> <entity id="P06-2096.11">community</entity> , especially for <entity id="P06-2096.12">tasks</entity> such as <entity id="P06-2096.13">machine <entity id="P06-2096.14">translation</entity></entity> , <entity id="P06-2096.15">text generation</entity> , and paraphrase <entity id="P06-2096.16">identification</entity> . Prior work falls into two <entity id="P06-2096.17">categories</entity> , depending on the <entity id="P06-2096.18">type</entity> of <entity id="P06-2096.19">input</entity> used: (a) <entity id="P06-2096.20">parallel corpora</entity> (e.g., multiple <entity id="P06-2096.21">translations</entity> of the same <entity id="P06-2096.22">text</entity> ) or (b) comparable <entity id="P06-2096.23">texts</entity> (non-parallel but on the same <entity id="P06-2096.24">topic</entity> ). So far, only <entity id="P06-2096.25">techniques</entity> <entity id="P06-2096.26">based</entity> on <entity id="P06-2096.27">parallel <entity id="P06-2096.28">texts</entity></entity> have successfully used <entity id="P06-2096.29">syntactic <entity id="P06-2096.30">information</entity></entity> to guide <entity id="P06-2096.31">alignments</entity> . In this <entity id="P06-2096.32">paper</entity> , we describe an <entity id="P06-2096.33">algorithm</entity> for incorporating <entity id="P06-2096.34">syntactic features</entity> in the <entity id="P06-2096.35">alignment</entity> <entity id="P06-2096.36">process</entity> for <entity id="P06-2096.37">non-parallel texts</entity> with the <entity id="P06-2096.38">goal</entity> of <entity id="P06-2096.39">generating</entity> novel paraphrases of existing <entity id="P06-2096.40">texts</entity> . Our <entity id="P06-2096.41">method</entity> uses <entity id="P06-2096.42">dynamic <entity id="P06-2096.43">programming</entity></entity> with <entity id="P06-2096.44">alignment</entity> <entity id="P06-2096.45">decision</entity> <entity id="P06-2096.46">based</entity> on the local <entity id="P06-2096.47">syntactic</entity> <entity id="P06-2096.48">similarity</entity> between two <entity id="P06-2096.49">sentences</entity> . Our <entity id="P06-2096.50">results</entity> show that <entity id="P06-2096.51">syntactic</entity> <entity id="P06-2096.52">alignment</entity> outrivals <entity id="P06-2096.53">syntax-free</entity> <entity id="P06-2096.54">methods</entity> by 20% in both grammatically and fidelity when computed over the novel <entity id="P06-2096.55">sentences</entity> <entity id="P06-2096.56">generated</entity> by <entity id="P06-2096.57">alignment-induced</entity> finite state automata.
</abstract>


</text>

<text id="P06-3015">
<title>
Clavius: Bi-Directional <entity id="P06-3015.1">Parsing</entity> For Generic Multimodal <entity id="P06-3015.2">Interaction</entity></title>
<abstract>
We introduce a new multi-threaded <entity id="P06-3015.3">parsing</entity> <entity id="P06-3015.4">algorithm</entity> on <entity id="P06-3015.5">unification</entity> grammars <entity id="P06-3015.6">designed</entity> specifically for multimodal <entity id="P06-3015.7">interaction</entity> and noisy <entity id="P06-3015.8">environments</entity> . By lifting some traditional <entity id="P06-3015.9">constraints</entity> , namely those related to the <entity id="P06-3015.10">ordering</entity> of <entity id="P06-3015.11">constituents</entity> , we overcome several <entity id="P06-3015.12">difficulties</entity> of other <entity id="P06-3015.13">systems</entity> in this <entity id="P06-3015.14">domain</entity> . We also present several <entity id="P06-3015.15">criteria</entity> used in this <entity id="P06-3015.16">model</entity> to constrain the <entity id="P06-3015.17">search</entity> <entity id="P06-3015.18">process</entity> using dynamically loadable scoring <entity id="P06-3015.19">functions</entity> . Some early <entity id="P06-3015.20">analyses</entity> of our <entity id="P06-3015.21">implementation</entity> are discussed.
</abstract>


</text>

<text id="P06-4002">
<title>
Is It Correct? - Towards Web- <entity id="P06-4002.1">Based</entity> <entity id="P06-4002.2">Evaluation</entity> Of <entity id="P06-4002.3">Automatic</entity> <entity id="P06-4002.4">Natural Language</entity> <entity id="P06-4002.5">Phrase</entity> <entity id="P06-4002.6">Generation</entity></title>
<abstract>
This <entity id="P06-4002.7">paper</entity> describes a novel <entity id="P06-4002.8">approach</entity> for the <entity id="P06-4002.9">automatic</entity> <entity id="P06-4002.10">generation</entity> and <entity id="P06-4002.11">evaluation</entity> of a trivial <entity id="P06-4002.12">dialogue</entity> <entity id="P06-4002.13">phrases</entity></abstract>


</text>

<text id="C80-1015">
<title>
A <entity id="C80-1015.1">Model</entity> Of <entity id="C80-1015.2">Natural Language Processing</entity> Of <entity id="C80-1015.3">Time</entity> - <entity id="C80-1015.4">Related</entity> Expressions
</title>


</text>

<abstract></abstract>

<text id="C80-1063">
<title>
A <entity id="C80-1063.1">Machine <entity id="C80-1063.2">Translation System</entity></entity> From <entity id="C80-1063.3">Japanese</entity> Into <entity id="C80-1063.4">English</entity> - Another <entity id="C80-1063.5">Perspective</entity> Of MT <entity id="C80-1063.6">Systems</entity></title>
<abstract>
A <entity id="C80-1063.7">machine <entity id="C80-1063.8">translation system</entity></entity> from <entity id="C80-1063.9">Japanese</entity> into <entity id="C80-1063.10">English</entity> is described.    The <entity id="C80-1063.11">system</entity> aims at <entity id="C80-1063.12">translation</entity> of <entity id="C80-1063.13">computer</entity> <entity id="C80-1063.14">manuals</entity> , and basically follows to the <entity id="C80-1063.15">transfer</entity> <entity id="C80-1063.16">approach</entity> .    The <entity id="C80-1063.17">design</entity> <entity id="C80-1063.18">principles</entity> of the <entity id="C80-1063.19">system</entity> are discussed in <entity id="C80-1063.20">detail</entity> , together with the overall <entity id="C80-1063.21">constructions</entity> of the <entity id="C80-1063.22">system</entity> . Especially, the <entity id="C80-1063.23">effectiveness</entity> of <entity id="C80-1063.24">lexicon-based</entity> <entity id="C80-1063.25">procedures</entity> , i.e. <entity id="C80-1063.26">lexicon-based</entity> <entity id="C80-1063.27">analysis</entity> , <entity id="C80-1063.28">transfer</entity> , and <entity id="C80-1063.29">synthesis</entity> , is emphasized.    Most of the linguistic <entity id="C80-1063.30">phenomena</entity> are treated by using <entity id="C80-1063.31">lexical</entity> <entity id="C80-1063.32">descriptions</entity> and <entity id="C80-1063.33">lexical</entity> <entity id="C80-1063.34">rules</entity> , instead of by general <entity id="C80-1063.35">syntactic</entity> <entity id="C80-1063.36">rules</entity> .    Because <entity id="C80-1063.37">Japanese</entity> and <entity id="C80-1063.38">English</entity> belong to quite different <entity id="C80-1063.39">language</entity> families, much more <entity id="C80-1063.40">structural</entity> <entity id="C80-1063.41">transfers</entity> are necessary than in other <entity id="C80-1063.42">MT systems</entity> among European <entity id="C80-1063.43">languages</entity> .    Special cares have been paid for <entity id="C80-1063.44">designing</entity> the <entity id="C80-1063.45">transfer</entity> <entity id="C80-1063.46">component</entity> .    Some <entity id="C80-1063.47">translation results</entity> are also given to illustrate the <entity id="C80-1063.48">current</entity> <entity id="C80-1063.49">abilities</entity> of the <entity id="C80-1063.50">system</entity> .
</abstract>


</text>

<text id="C82-1016">
<title>
Referential Nets With Attributes
</title>
<abstract>
One of the essential <entity id="C82-1016.1">problems</entity> in <entity id="C82-1016.2">natural language</entity> production and <entity id="C82-1016.3">understanding</entity> is the <entity id="C82-1016.4">problem</entity> of <entity id="C82-1016.5">processing</entity> referential <entity id="C82-1016.6">relations</entity>. In this <entity id="C82-1016.7">paper</entity> I describe a <entity id="C82-1016.8">model</entity> for representing and <entity id="C82-1016.9">processing</entity> referential <entity id="C82-1016.10">relations</entity> : referential nets with attributes. Both <entity id="C82-1016.11">processes</entity> (analyzing and <entity id="C82-1016.12">generating</entity> referential <entity id="C82-1016.13">expressions</entity> ) are controlled by attributes. There are two <entity id="C82-1016.14">types</entity> of attributes, on one <entity id="C82-1016.15">hand</entity> , the ones to the internal substitutes of the <entity id="C82-1016.16">objects</entity> spoken about, on the other <entity id="C82-1016.17">hand</entity> , the ones to the <entity id="C82-1016.18">descriptions</entity> of these <entity id="C82-1016.19">objects</entity> .
</abstract>


</text>

<text id="C82-1021">
<title>
A Multilayered <entity id="C82-1021.1">Approach</entity> To The Handling Of <entity id="C82-1021.2">Word</entity> <entity id="C82-1021.3">Formation</entity></title>
<abstract>
The <entity id="C82-1021.4">treatment</entity> of <entity id="C82-1021.5">word</entity> <entity id="C82-1021.6">formations</entity> has until recently been a neglected <entity id="C82-1021.7">topic</entity> in <entity id="C82-1021.8">natural  language</entity> AI <entity id="C82-1021.9">research</entity> . This <entity id="C82-1021.10">paper</entity> <entity id="C82-1021.11">proposes</entity> a multilayered <entity id="C82-1021.12">approach</entity> to <entity id="C82-1021.13">word</entity> <entity id="C82-1021.14">formation</entity> which treats derivatives and compounds on several different <entity id="C82-1021.15">levels</entity> of <entity id="C82-1021.16">processing</entity> within a <entity id="C82-1021.17">natural <entity id="C82-1021.18">language</entity></entity> <entity id="C82-1021.19">dialogue <entity id="C82-1021.20">system</entity></entity> . <entity id="C82-1021.21">Analysis</entity> and <entity id="C82-1021.22">generation</entity> <entity id="C82-1021.23">strategies</entity> being <entity id="C82-1021.24">developed</entity> for the <entity id="C82-1021.25">dialogue <entity id="C82-1021.26">system</entity></entity> HAM-ANS are described. <entity id="C82-1021.27">Identification</entity> of <entity id="C82-1021.28">word</entity> <entity id="C82-1021.29">format</entity> <entity id="C82-1021.30">ions</entity> , <entity id="C82-1021.31">semantic interpretation</entity> , and <entity id="C82-1021.32">evaluation</entity> in the <entity id="C82-1021.33">context</entity> of a <entity id="C82-1021.34">dialogue</entity> are the <entity id="C82-1021.35">main</entity> <entity id="C82-1021.36">levels</entity> of <entity id="C82-1021.37">analysis</entity> on which the <entity id="C82-1021.38">system</entity> successively attempts to infer the implicit <entity id="C82-1021.39">relations</entity> between <entity id="C82-1021.40">word</entity> <entity id="C82-1021.41">formation</entity> <entity id="C82-1021.42">components</entity> . <entity id="C82-1021.43">Generation</entity> of <entity id="C82-1021.44">word</entity> <entity id="C82-1021.45">formations</entity> is viewed as a <entity id="C82-1021.46">process</entity> comparable to the <entity id="C82-1021.47">generation</entity> of elliptical <entity id="C82-1021.48">utterances</entity> .
</abstract>


</text>

<text id="C82-1031">
<title>
The Anatomy Of A Systemic <entity id="C82-1031.1">Choice</entity></title> 
<abstract><entity id="C82-1031.2">Choice</entity> is one of the most prominent organizing <entity id="C82-1031.3">concepts</entity> in systemic <entity id="C82-1031.4">linguistics</entity> . <entity id="C82-1031.5">Languages</entity> are described in <entity id="C82-1031.6">terms</entity> of the <entity id="C82-1031.7">choices</entity> available to the speaker and the <entity id="C82-1031.8">relationships</entity> of those <entity id="C82-1031.9">choices</entity> to each other and to the <entity id="C82-1031.10">language</entity> produced. This <entity id="C82-1031.11">paper</entity> addresses the <entity id="C82-1031.12">problems</entity> of
</abstract>


</text>

<text id="C82-1059">
<title><entity id="C82-1059.1">Parsing</entity> German
</title>
<abstract>
The first <entity id="C82-1059.2">part</entity> of this <entity id="C82-1059.3">paper</entity> is dedicated to an <entity id="C82-1059.4">overview</entity> of the <entity id="C82-1059.5">parser</entity> of the <entity id="C82-1059.6">system</entity> VIE-LANG (Viennese <entity id="C82-1059.7">Language Understanding System</entity> ). The <entity id="C82-1059.8">parser</entity> is a <entity id="C82-1059.9">production <entity id="C82-1059.10">system</entity></entity> which uses an interleaved <entity id="C82-1059.11">method</entity> that combines <entity id="C82-1059.12">syntax</entity> and <entity id="C82-1059.13">semantics</entity> . It <entity id="C82-1059.14">parses</entity> directly into the internal <entity id="C82-1059.15">representation</entity> of the <entity id="C82-1059.16">system</entity> , without producing an. intermediate <entity id="C82-1059.17">syntactic structure</entity> . The last <entity id="C82-1059.18">part</entity> discusses the <entity id="C82-1059.19">relationship</entity> between some special <entity id="C82-1059.20">features</entity> of the German <entity id="C82-1059.21">language</entity> , and <entity id="C82-1059.22">properties</entity> of the <entity id="C82-1059.23">parser</entity> that originate in the <entity id="C82-1059.24">language</entity> .
</abstract>


</text>

<text id="C86-1088">
<title>
Definite <entity id="C86-1088.1">Noun</entity> Phrases And The <entity id="C86-1088.2">Semantics</entity> Of <entity id="C86-1088.3">Discourse</entity></title> 
<abstract><entity id="C86-1088.4">Discourse Representation Theory</entity> (DRT), <entity id="C86-1088.5">developed</entity> by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change <entity id="C86-1088.6">Semantics</entity> ( Heim 1982 ) and <entity id="C86-1088.7">Situation</entity> <entity id="C86-1088.8">Semantics</entity> (Barwise/ Perry 1983 ), to a group of theoretical <entity id="C86-1088.9">approaches</entity> which in the early Eighties introduced a dynamic, <entity id="C86-1088.10">context-oriented</entity> <entity id="C86-1088.11">perspective</entity> into the <entity id="C86-1088.12">semantics</entity> of <entity id="C86-1088.13">natural <entity id="C86-1088.14">language</entity></entity> . This recent <entity id="C86-1088.15">development</entity> in theoretical <entity id="C86-1088.16">semantics</entity> indicates a <entity id="C86-1088.17">shift</entity> of interest towards <entity id="C86-1088.18">topics</entity> that have been familiar in <entity id="C86-1088.19">natural language processing</entity> <entity id="C86-1088.20">research</entity> for the last decade: among others, the <entity id="C86-1088.21">interpretation</entity> of new <entity id="C86-1088.22">utterances</entity> with <entity id="C86-1088.23">respect</entity> to a given <entity id="C86-1088.24">context</entity> , and <entity id="C86-1088.25">integration</entity> of the <entity id="C86-1088.26">utterance</entity> <entity id="C86-1088.27">information</entity> into that <entity id="C86-1088.28">context</entity> ; the <entity id="C86-1088.29">step-by-step</entity> <entity id="C86-1088.30">construction</entity> of <entity id="C86-1088.31">representations</entity> for larger pieces of <entity id="C86-1088.32">discourse</entity> ; the <entity id="C86-1088.33">investigation</entity> of <entity id="C86-1088.34">text</entity> <entity id="C86-1088.35">coherence</entity> <entity id="C86-1088.36">phenomena</entity> ; and the <entity id="C86-1088.37">description</entity> of referential <entity id="C86-1088.38">processes</entity> . The <entity id="C86-1088.39">core</entity> of DRT (and File Change <entity id="C86-1088.40">Semantics</entity> ) is the <entity id="C86-1088.41">treatment</entity> of indefinite <entity id="C86-1088.42">noun <entity id="C86-1088.43">phrases</entity></entity> as <entity id="C86-1088.44">reference</entity> establishing <entity id="C86-1088.45">terms</entity> (as opposed to their <entity id="C86-1088.46">standard</entity> truth-conditional <entity id="C86-1088.47">quantifier</entity> <entity id="C86-1088.48">analysis</entity> , but in accordance with the treatmant of indefinites in NLP <entity id="C86-1088.49">research</entity> ) and definite <entity id="C86-1088.50">noun phrases</entity> (pronouns as well as full NPs) as anaphoric <entity id="C86-1088.51">expressions</entity> . It is one of the theoretically most appealing <entity id="C86-1088.52">features</entity> of these <entity id="C86-1088.53">theories</entity> that they <entity id="C86-1088.54">provide</entity> <entity id="C86-1088.55">simple</entity> unified accounts for all indefinites, and for all definites, respectively. This theoretical <entity id="C86-1088.56">simplicity</entity> stands however in sharp <entity id="C86-1088.57">contrast</entity> to the <entity id="C86-1088.58">complexity</entity> of the <entity id="C86-1088.59">process</entity> of etablishing <entity id="C86-1088.60">reference</entity> observed in NLP <entity id="C86-1088.61">research</entity> , and the <entity id="C86-1088.62">variety</entity> of <entity id="C86-1088.63">phenomena</entity> and linguistic <entity id="C86-1088.64">levels</entity> involved. On the one <entity id="C86-1088.65">hand</entity> , this <entity id="C86-1088.66">contrast</entity> is quite <entity id="C86-1088.67">natural</entity> : As a semantically motivated <entity id="C86-1088.68">theory</entity> , DRT should not be expected to incorporate every <entity id="C86-1088.69">detail</entity> of <entity id="C86-1088.70">inferencing</entity> necessary to come up with an <entity id="C86-1088.71">interpretation</entity> for a specific <entity id="C86-1088.72">utterance</entity> in a given <entity id="C86-1088.73">context</entity> ; it can better be thought of as an <entity id="C86-1088.74">interface</entity> relating theoretical, truth-conditional <entity id="C86-1088.75">semantics</entity> and the genuinely pragmatic work of <entity id="C86-1088.76">text</entity> <entity id="C86-1088.77">understanding</entity> . On the other <entity id="C86-1088.78">hand</entity> , if DRT is seriously intended to bridge the <entity id="C86-1088.79">gap</entity> between theoretical <entity id="C86-1088.80">linguistics</entity> and the NLP <entity id="C86-1088.81">approach</entity> , it should take into <entity id="C86-1088.82">consideration</entity> as many factual <entity id="C86-1088.83">restrictions</entity> on NP <entity id="C86-1088.84">reference</entity> , and <entity id="C86-1088.85">distinctions</entity> among subtypes of referential <entity id="C86-1088.86">expressions</entity> , as is possible in a systematic and descriptive way. Several <entity id="C86-1088.87">extensions</entity> of the <entity id="C86-1088.88">standard</entity> <entity id="C86-1088.89">system</entity> are at work, e.g. for the <entity id="C86-1088.90">treatment</entity> of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the <entity id="C86-1088.91">analysis</entity> of (singular) definite <entity id="C86-1088.92">noun phrases</entity> , once the <entity id="C86-1088.93">basic</entity> <entity id="C86-1088.94">concepts</entity> had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished <entity id="C86-1088.95">fragment</entity> . In this talk I will first give a short <entity id="C86-1088.96">overview</entity> of the <entity id="C86-1088.97">basic</entity> DRT <entity id="C86-1088.98">system</entity> , and sketch Kamp 's <entity id="C86-1088.99">proposal</entity> for the <entity id="C86-1088.100">treatment</entity> of definite <entity id="C86-1088.101">noun <entity id="C86-1088.102">phrases</entity></entity> . Then I will indicate how the <entity id="C86-1088.103">basic</entity> <entity id="C86-1088.104">reference</entity> establishing <entity id="C86-1088.105">function</entity> and the '<entity id="C86-1088.106">side-effects</entity> 'of different <entity id="C86-1088.107">types</entity> of definite NPs can be described in more <entity id="C86-1088.108">detail</entity> . In doing this, I will refer to the work about anaphora done in the NLP <entity id="C86-1088.109">area</entity> (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their <entity id="C86-1088.110">assumptions</entity> into the DRT <entity id="C86-1088.111">framework</entity> , and critically commenting on some others.
</abstract>


</text>

<text id="C86-1091">
<title>
Towards The <entity id="C86-1091.1">Automatic</entity> <entity id="C86-1091.2">Acquisition</entity> Of <entity id="C86-1091.3">Lexical</entity> <entity id="C86-1091.4">Data</entity></title>
<abstract>
"Creating a <entity id="C86-1091.5">knowledge <entity id="C86-1091.6">base</entity></entity> has always been a bottleneck in the <entity id="C86-1091.7">implementation</entity> of AI <entity id="C86-1091.8">systems</entity> . This is also true for <entity id="C86-1091.9">Natural Language Understanding</entity> (NEU) <entity id="C86-1091.10">systems</entity> , particularly for <entity id="C86-1091.11">data-driven</entity> ones. While a perfect <entity id="C86-1091.12">system</entity> for <entity id="C86-1091.13">automatic</entity> <entity id="C86-1091.14">acquisition</entity> of all sorts of <entity id="C86-1091.15">knowledge</entity> is still feir from being realized, <entity id="C86-1091.16">partial</entity> <entity id="C86-1091.17">solutions</entity> are possible. This holds especially for <entity id="C86-1091.18">lexical</entity> <entity id="C86-1091.19">data</entity> . Nevertheless, the <entity id="C86-1091.20">task</entity> is not trivial, in particular when <entity id="C86-1091.21">dealing</entity> with <entity id="C86-1091.22">languages</entity> rich in inflectional Horms like German. Our: <entity id="C86-1091.23">system</entity> is to be used by persons with no specific <entity id="C86-1091.24">linguistic knowledge</entity> , thus linguistic expertise has been put into the <entity id="C86-1091.25">system</entity> to ascertain correct <entity id="C86-1091.26">classification</entity> of <entity id="C86-1091.27">words</entity>. <entity id="C86-1091.28">Classification</entity> is done by means of a small <entity id="C86-1091.29">rule</entity> <entity id="C86-1091.30">based</entity> <entity id="C86-1091.31">system</entity> with <entity id="C86-1091.32">Lexical  knowledge</entity> and <entity id="C86-1091.33">language-specific</entity> heuristics. The key idea is the <entity id="C86-1091.34">identification</entity> of three sorts of <entity id="C86-1091.35">knowledge</entity> which are <entity id="C86-1091.36">processed</entity> distinctly and the <entity id="C86-1091.37">optimal</entity> use of <entity id="C86-1091.38">knowledge</entity> already contained in the existing <entity id="C86-1091.39">Lexicon</entity> . 1
 <entity id="C86-1091.40">Introduction</entity> in this <entity id="C86-1091.41">paper</entity> we introduce a <entity id="C86-1091.42">system</entity> for the <entity id="C86-1091.43">semi-automatic</entity> enlargement of a morphological 1 ex <entity id="C86-1091.44">icon</entity> . If <entity id="C86-1091.45">forms</entity> <entity id="C86-1091.46">part</entity> of VIE-BANG, a German <entity id="C86-1091.47">Language</entity> <entity id="C86-1091.48">dialogue system</entity> ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta <entity id="C86-1091.49">system</entity> as we 11 : i ts <entity id="C86-1091.50">knowledge base</entity> is to be enlarged, and its facilities are used L:o <entity id="C86-1091.51">support</entity> that <entity id="C86-1091.52">process</entity> : the parsor serves to analyze the i nput to the acquisj ti on <entity id="C86-1091.53">system</entity> , the <entity id="C86-1091.54">generator</entity>  i. s used  to <entity id="C86-1091.55">provide</entity> exampl es. Ln <entity id="C86-1091.56">contrast</entity> to <entity id="C86-1091.57">English</entity> the <entity id="C86-1091.58">morphological <entity id="C86-1091.59">analysis</entity></entity> of German <entity id="C86-1091.60">words</entity>  is no trivial <entity id="C86-1091.61">task</entity> ,  <entity id="C86-1091.62">due</entity> to two causes: - First, there is a rich inflectional <entity id="C86-1091.63">system</entity> , consisting of about 60 different endings (where most endings have various different <entity id="C86-1091.64">interpretations</entity> ), some <entity id="C86-1091.65">prefixes</entity> ('go-PPP, "
</abstract>


</text>

<text id="C88-1041">
<title>The PSI/PHI <entity id="C88-1041.1">Architecture</entity> For Prosodic <entity id="C88-1041.2">Parsing</entity></title>
<abstract>
In is in as a a
</abstract>


</text>

<text id="C88-2134">
<title><entity id="C88-2134.1">Optimization</entity> <entity id="C88-2134.2">Algorithms</entity> Of Deciphering As The Elements Of A <entity id="C88-2134.3">Linguistic Theory</entity></title>
<abstract>
This <entity id="C88-2134.4">paper</entity> presents an <entity id="C88-2134.5">outline</entity> of the <entity id="C88-2134.6">linguistic theory</entity> which may be identified with the partially <entity id="C88-2134.7">ordered</entity> set of <entity id="C88-2134.8">optimization</entity> <entity id="C88-2134.9">algorithms</entity> of deciphering. An <entity id="C88-2134.10">algorithm</entity> of deciphering is the operational <entity id="C88-2134.11">definition</entity> of a given linguistic <entity id="C88-2134.12">phenomenon</entity> which han the following three <entity id="C88-2134.13">components</entity> : a set of admissible <entity id="C88-2134.14">solutions</entity> , an <entity id="C88-2134.15">objective function</entity> and a <entity id="C88-2134.16">procedure</entity> which finds out the ministurn or the maximum of the <entity id="C88-2134.17">objective function</entity> . The <entity id="C88-2134.18">paper</entity> contains the <entity id="C88-2134.19">description</entity> of the four <entity id="C88-2134.20">algorithms</entity> of the <entity id="C88-2134.21">proposed</entity> <entity id="C88-2134.22">type</entity> : 1. The <entity id="C88-2134.23">algorithm</entity> which classifies the letters into <entity id="C88-2134.24">vowels</entity> and <entity id="C88-2134.25">consonants</entity> . 2. The <entity id="C88-2134.26">algorithm</entity> which identifies the morphemes in the <entity id="C88-2134.27">text</entity> without the <entity id="C88-2134.28">boundaries</entity> between <entity id="C88-2134.29">words</entity> . 3. The <entity id="C88-2134.30">algorithm</entity> whioh finds out the <entity id="C88-2134.31">dependency <entity id="C88-2134.32">tree</entity></entity> of a <entity id="C88-2134.33">sentence</entity> .
</abstract>


</text>

<text id="C90-2046">
<title>
Tenets For An Interlingual <entity id="C90-2046.1">Representation</entity> Of Definite NPs
</title>
<abstract>
The <entity id="C90-2046.2">main</entity> <entity id="C90-2046.3">goal</entity> of this <entity id="C90-2046.4">paper</entity> (as in Keenan and  Stavi 1986 ) is to characterize the possible <entity id="C90-2046.5">determiner</entity> denotations in <entity id="C90-2046.6">order</entity> to <entity id="C90-2046.7">develop</entity> a <entity id="C90-2046.8">computational</entity> <entity id="C90-2046.9">approach</entity> that makes explicit use of this <entity id="C90-2046.10">information</entity> . To cope with the <entity id="C90-2046.11">constraints</entity> that <entity id="C90-2046.12">languages</entity> impose when <entity id="C90-2046.13">generating</entity> <entity id="C90-2046.14">determiners</entity> , a <entity id="C90-2046.15">computational model</entity> has to follow the laws that <entity id="C90-2046.16">map</entity> d
finiteness to <entity id="C90-2046.17">structures</entity> and <entity id="C90-2046.18">strings</entity> and viceversa. In the following <entity id="C90-2046.19">proposal</entity> I distantiate from K. B
hlers Deixis <entity id="C90-2046.20">Theory</entity> and Weinrichs (76) <entity id="C90-2046.21">proposal</entity> where indefinites suggest subsequent <entity id="C90-2046.22">information</entity> , while definite point out facts from the previous <entity id="C90-2046.23">information</entity> . This very general position is insufficient if we want to formalize NP-definiteness. The <entity id="C90-2046.24">semantics</entity> of NP defmiteness must be captured adequately in <entity id="C90-2046.25">computational</entity> <entity id="C90-2046.26">frameworks</entity> for such <entity id="C90-2046.27">tasks</entity> as answering quantified NL-- <entity id="C90-2046.28">questions</entity> , or in a <entity id="C90-2046.29">MT system</entity> to convert NPs from one <entity id="C90-2046.30">language</entity> into another. In the first <entity id="C90-2046.31">part</entity> of this <entity id="C90-2046.32">paper</entity> I draw a <entity id="C90-2046.33">typology</entity> of defmiteness ; later I reflect on the defmiteness of NPs in an <entity id="C90-2046.34">IL-representation</entity> . The major <entity id="C90-2046.35">result</entity> is given by the <entity id="C90-2046.36">determiner</entity> <entity id="C90-2046.37">generators</entity> . Defmiteness should be <entity id="C90-2046.38">evaluated</entity> in a Q-A <entity id="C90-2046.39">system</entity> and in MT. The extensive <entity id="C90-2046.40">functionality</entity>    of   defmiteness    is first elaborated in the <entity id="C90-2046.41">parsing</entity> and <entity id="C90-2046.42">results</entity> in an <entity id="C90-2046.43">IL-representation</entity> ; finally the <entity id="C90-2046.44">determiner</entity> <entity id="C90-2046.45">generators</entity> create correct morphological <entity id="C90-2046.46">determiners</entity> and right <entity id="C90-2046.47">determiner</entity> <entity id="C90-2046.48">structures</entity> .
</abstract>


</text>

<text id="C90-2057">
<title><entity id="C90-2057.1">Lexical</entity> Gaps And Idioms In <entity id="C90-2057.2">Machine Translation</entity></title>
<abstract>
This <entity id="C90-2057.3">paper</entity> describes the <entity id="C90-2057.4">treatment</entity> of <entity id="C90-2057.5">lexical</entity> <entity id="C90-2057.6">gaps</entity> , <entity id="C90-2057.7">collocation</entity> <entity id="C90-2057.8">information</entity> and <entity id="C90-2057.9">idioms</entity> in the <entity id="C90-2057.10">English</entity> to Portuguese <entity id="C90-2057.11">machine <entity id="C90-2057.12">translation system</entity></entity> PORTUGA. The <entity id="C90-2057.13">perspective</entity> is strictly bilingual, in the <entity id="C90-2057.14">sense</entity> that all <entity id="C90-2057.15">problems</entity> referenced above are considered to belong to the <entity id="C90-2057.16">transfer</entity> <entity id="C90-2057.17">phase</entity> , and not, as in other <entity id="C90-2057.18">systems</entity> , to <entity id="C90-2057.19">analysis</entity> or <entity id="C90-2057.20">generation</entity> . The <entity id="C90-2057.21">solution</entity> presented invokes a <entity id="C90-2057.22">parser</entity> for the <entity id="C90-2057.23">target language</entity> (Portuguese) that <entity id="C90-2057.24">analyses</entity> , producing the corresponding graph <entity id="C90-2057.25">structure</entity> , the <entity id="C90-2057.26">multiword <entity id="C90-2057.27">expression</entity></entity> selected as the <entity id="C90-2057.28">result</entity> of <entity id="C90-2057.29">lexical</entity> <entity id="C90-2057.30">transfer</entity> . This <entity id="C90-2057.31">process</entity> seems to bring considerable <entity id="C90-2057.32">advantage</entity> in what <entity id="C90-2057.33">readability</entity> and ease of bilingual <entity id="C90-2057.34">dictionary</entity> <entity id="C90-2057.35">development</entity> is <entity id="C90-2057.36">concerned</entity> , and to furnish maximal <entity id="C90-2057.37">flexibility</entity> together with minimal <entity id="C90-2057.38">storage</entity> <entity id="C90-2057.39">requirements</entity> . Finally, it also <entity id="C90-2057.40">provides</entity> complete <entity id="C90-2057.41">independence</entity> between <entity id="C90-2057.42">dictionary</entity> and grammar <entity id="C90-2057.43">formalisms</entity> .
</abstract>


</text>

<text id="C90-3079">
<title>
Intelligent Handling Of Weather Forecasts
</title>
<abstract>
Some typical <entity id="C90-3079.1">cases</entity> of intelligent handling of weather forecasts such as <entity id="C90-3079.2">translation</entity> , <entity id="C90-3079.3">visualization</entity> , etc. are decomposed into two subprocesses <entity id="C90-3079.4">analysis</entity> and <entity id="C90-3079.5">synthesis</entity> . Specific <entity id="C90-3079.6">techniques</entity> are presented for <entity id="C90-3079.7">analysis</entity> and <entity id="C90-3079.8">synthesis</entity> of weather forecast <entity id="C90-3079.9">texts</entity> as well as for <entity id="C90-3079.10">generation</entity> of weather <entity id="C90-3079.11">maps</entity> . These <entity id="C90-3079.12">techniques</entity> <entity id="C90-3079.13">deal</entity> with the weather forecasts at different <entity id="C90-3079.14">levels</entity> <entity id="C90-3079.15">syntactic</entity> , <entity id="C90-3079.16">discourse</entity> and <entity id="C90-3079.17">semantic</entity> . They are <entity id="C90-3079.18">based</entity> on a conceptual <entity id="C90-3079.19">model</entity> underlying weather forecasts as well as on formal <entity id="C90-3079.20">descriptions</entity> of the means of <entity id="C90-3079.21">expression</entity> used in particular <entity id="C90-3079.22">natural</entity> and cartographic sublanguages.
</abstract>


</text>

<text id="C92-1037">
<title>
Genetic NPs And Habitual VPs
</title>
<abstract>
We <entity id="C92-1037.1">propose</entity> a <entity id="C92-1037.2">simple</entity> , intuitively satisfying <entity id="C92-1037.3">treatment</entity> of the <entity id="C92-1037.4">semantics</entity> of bare plural NPs . This <entity id="C92-1037.5">treatment</entity> avoids the use of nonstandard <entity id="C92-1037.6">logics</entity> , and avoids the need for systematic <entity id="C92-1037.7">ambiguity</entity> of <entity id="C92-1037.8">verb</entity> <entity id="C92-1037.9">semantics</entity> .
</abstract>


</text>

<text id="C92-1049">
<title>
Using Linguistic, World, And Contextual <entity id="C92-1049.1">Knowledge</entity> In A Plan <entity id="C92-1049.2">Recognition</entity> <entity id="C92-1049.3">Model</entity> Of <entity id="C92-1049.4">Dialogue</entity></title>
<abstract>This <entity id="C92-1049.5">paper</entity> presents a <entity id="C92-1049.6">plan-based <entity id="C92-1049.7">model</entity></entity> of <entity id="C92-1049.8">dialogue</entity> that combines world, linguistic, and contextual <entity id="C92-1049.9">knowledge</entity> in <entity id="C92-1049.10">order</entity> to recognize <entity id="C92-1049.11">complex</entity> communicative <entity id="C92-1049.12">actions</entity> such as expressing doubt. <entity id="C92-1049.13">Linguistic knowledge</entity> suggests certain <entity id="C92-1049.14">discourse</entity> acts, a speaker's beliefs, and the <entity id="C92-1049.15">strength</entity> of those beliefs; contextual <entity id="C92-1049.16">knowledge</entity> suggests the most coherent continuation of the <entity id="C92-1049.17">dialogue</entity> ; and <entity id="C92-1049.18">world knowledge</entity> <entity id="C92-1049.19">provides</entity> <entity id="C92-1049.20">evidence</entity> that the <entity id="C92-1049.21">applicability</entity> conditions hold for those <entity id="C92-1049.22">discourse</entity> acts that capture the <entity id="C92-1049.23">relationship</entity> of the <entity id="C92-1049.24">current</entity> <entity id="C92-1049.25">utterance</entity> to the <entity id="C92-1049.26">discourse</entity> as a whole.
</abstract>


</text>

<text id="C92-1052">
<title>
Temporal <entity id="C92-1052.1">Structure</entity> Of <entity id="C92-1052.2">Discourse</entity></title>
<abstract>
In this <entity id="C92-1052.3">paper</entity> <entity id="C92-1052.4">discourse</entity> <entity id="C92-1052.5">segments</entity> are defined and a <entity id="C92-1052.6">method</entity> for <entity id="C92-1052.7">discourse</entity> segmentation primarily <entity id="C92-1052.8">based</entity> on abduction of <entity id="C92-1052.9">temporal relations</entity> between <entity id="C92-1052.10">segments</entity> is <entity id="C92-1052.11">proposed</entity> . This <entity id="C92-1052.12">method</entity> is precise and computationally feasible and is <entity id="C92-1052.13">supported</entity> by previous work in the <entity id="C92-1052.14">area</entity> of temporal <entity id="C92-1052.15">anaphora <entity id="C92-1052.16">resolution</entity></entity> .
</abstract>


</text>

<text id="C92-1053">
<title>
Organizing <entity id="C92-1053.1">Dialogue</entity> From An Incoherent <entity id="C92-1053.2">Stream</entity> Of Goals
</title>
<abstract>
their <entity id="C92-1053.3">reasoning</entity> for <entity id="C92-1053.4">structure</entity> their <entity id="C92-1053.5">dialogues</entity> . Instead, <entity id="C92-1053.6">computer-generated</entity> <entity id="C92-1053.7">conversation</entity> must rely on some other <entity id="C92-1053.8">mechanism</entity> for its organisation. In this <entity id="C92-1053.9">paper</entity> , we discuss such <entity id="C92-1053.10">mechanism</entity> . We describe <entity id="C92-1053.11">provides</entity> a guide for <entity id="C92-1053.12">conversation</entity> . The <entity id="C92-1053.13">template</entity> is built from <entity id="C92-1053.14">schemata</entity> representing <entity id="C92-1053.15">discourse</entity> <entity id="C92-1053.16">convention</entity> . As <entity id="C92-1053.17">goals</entity> arrive from the <entity id="C92-1053.18">problem</entity> <entity id="C92-1053.19">solver</entity> they are added to the <entity id="C92-1053.20">template</entity> . Because accepted <entity id="C92-1053.21">discourse structures</entity> are used to connect a new <entity id="C92-1053.22">goal</entity> to the existing <entity id="C92-1053.23">template</entity> , <entity id="C92-1053.24">goals</entity> are organised into sub-groups that follow conventional, coherent <entity id="C92-1053.25">patterns</entity> of <entity id="C92-1053.26">discourse</entity> . We present JUDIS , an <entity id="C92-1053.27">interface</entity> to <entity id="C92-1053.28">distributed</entity> <entity id="C92-1053.29">problem</entity> <entity id="C92-1053.30">solver</entity> that uses this <entity id="C92-1053.31">approach</entity> to organise <entity id="C92-1053.32">dialogues</entity> from incoherent of <entity id="C92-1053.33">goals</entity> .
</abstract>


</text>

<text id="C92-2108">
<title>
Preventing False Temporal Implicatures: Interactive <entity id="C92-2108.1">Defaults</entity> For <entity id="C92-2108.2">Text Generation</entity></title>
<abstract>
Given the causal and <entity id="C92-2108.3">temporal <entity id="C92-2108.4">relations</entity></entity> between <entity id="C92-2108.5">events</entity> in a <entity id="C92-2108.6">knowledge base</entity> , what are the ways they can be described in <entity id="C92-2108.7">text</entity> ? Elsewhere, we have argued that during <entity id="C92-2108.8">interpretation</entity> , the reader-hearer // must infer certain temporal <entity id="C92-2108.9">information</entity> from <entity id="C92-2108.10">knowledge</entity> about the world, <entity id="C92-2108.11">language</entity> use and pragmatics. It is generally agreed that <entity id="C92-2108.12">processes</entity> of Gricean implicature <entity id="C92-2108.13">help</entity> determine the <entity id="C92-2108.14">interpretation</entity> of <entity id="C92-2108.15">text</entity> in <entity id="C92-2108.16">context</entity> . But without a <entity id="C92-2108.17">notion</entity> of logical <entity id="C92-2108.18">con</entity> se quo ii ce to underwrite them, the <entity id="C92-2108.19">inferences</entity>
-often defeasible in <entity id="C92-2108.20">nature</entity> - will appear arbitrary, and unprincipled. Hence, we have explored the <entity id="C92-2108.21">requirements</entity> on a formal <entity id="C92-2108.22">model</entity> of temporal implicature, and <entity id="C92-2108.23">outlined</entity> one possible nonmonotonic <entity id="C92-2108.24">framework</entity> for <entity id="C92-2108.25">discourse</entity> <entity id="C92-2108.26">interpretation</entity> (Lascarides &amp;; Asher [1991], Lascarides     Oberlander [1992a]). Here, we argue that if the writer-speaker 22077.
</abstract>


</text>

<text id="C92-2110">
<title><entity id="C92-2110.1">Design</entity> <entity id="C92-2110.2">Tool</entity> <entity id="C92-2110.3">Combining</entity> <entity id="C92-2110.4">Keyword</entity> <entity id="C92-2110.5">Analyzer</entity> And <entity id="C92-2110.6">Case-</entity> <entity id="C92-2110.7">Based</entity> <entity id="C92-2110.8">Parser</entity> For <entity id="C92-2110.9">Developing</entity> <entity id="C92-2110.10">Natural Language</entity> <entity id="C92-2110.11">Database</entity> Interfaces
</title>
<abstract>
We have <entity id="C92-2110.12">designed</entity> and experimentally <entity id="C92-2110.13">implemented</entity> a <entity id="C92-2110.14">tool</entity> for <entity id="C92-2110.15">developing</entity> a <entity id="C92-2110.16">natural <entity id="C92-2110.17">language systems</entity></entity> that can accept extra-grammatical <entity id="C92-2110.18">expressions</entity> , <entity id="C92-2110.19">keyword</entity> <entity id="C92-2110.20">sequences</entity> , and linguistic <entity id="C92-2110.21">fragments</entity> , as well as ordinary <entity id="C92-2110.22">natural language</entity> <entity id="C92-2110.23">queries</entity> . The key to this <entity id="C92-2110.24">tool</entity> 's <entity id="C92-2110.25">efficiency</entity> is its effective use of a <entity id="C92-2110.26">simple</entity> <entity id="C92-2110.27">keyword</entity> <entity id="C92-2110.28">analyzer</entity> in <entity id="C92-2110.29">combination</entity> with a conventional <entity id="C92-2110.30">case-based</entity> <entity id="C92-2110.31">parser</entity> . The <entity id="C92-2110.32">keyword</entity> <entity id="C92-2110.33">analyzer</entity> <entity id="C92-2110.34">performs</entity> a <entity id="C92-2110.35">majority</entity> of those <entity id="C92-2110.36">queries</entity> which are <entity id="C92-2110.37">simple</entity> <entity id="C92-2110.38">data</entity> <entity id="C92-2110.39">retrievals</entity> . Since it uses only <entity id="C92-2110.40">keywords</entity> in any <entity id="C92-2110.41">query</entity> , this <entity id="C92-2110.42">analyzer</entity> is <entity id="C92-2110.43">robust</entity> with regard to extra-grammatical <entity id="C92-2110.44">expressions</entity> . Since little <entity id="C92-2110.45">labor</entity> is <entity id="C92-2110.46">required</entity> of the <entity id="C92-2110.47">application</entity> designer in using the <entity id="C92-2110.48">keyword</entity> <entity id="C92-2110.49">analyzer</entity> <entity id="C92-2110.50">portion</entity> of the <entity id="C92-2110.51">tool</entity> , and since the <entity id="C92-2110.52">case-based</entity> <entity id="C92-2110.53">parser</entity> <entity id="C92-2110.54">processes</entity> only those <entity id="C92-2110.55">queries</entity> which the <entity id="C92-2110.56">keyword</entity> <entity id="C92-2110.57">analyzer</entity> fails to interpret, total <entity id="C92-2110.58">labor</entity> <entity id="C92-2110.59">required</entity> of the designer is less than that for a <entity id="C92-2110.60">tool</entity> which employs a conventional <entity id="C92-2110.61">case-based</entity> <entity id="C92-2110.62">parser</entity> alone.
</abstract>


</text>

<text id="C92-2119">
<title>
A <entity id="C92-2119.1">Robust</entity> <entity id="C92-2119.2">Approach</entity> For Handling Oral Dialogues
</title>
<abstract>
Present <entity id="C92-2119.3">limits</entity> of <entity id="C92-2119.4">speech <entity id="C92-2119.5">recognition</entity></entity> and <entity id="C92-2119.6">understanding</entity> in the <entity id="C92-2119.7">context</entity> of free spoken <entity id="C92-2119.8">language</entity> (altlwugh with a limited <entity id="C92-2119.9">vocabulary</entity> ) have perverse <entity id="C92-2119.10">effects</entity> on the flow of the <entity id="C92-2119.11">dialogue</entity> with a <entity id="C92-2119.12">system</entity> . Typically a non <entity id="C92-2119.13">robust</entity> <entity id="C92-2119.14">dialogue</entity> <entity id="C92-2119.15">manager</entity> will fail to face with these <entity id="C92-2119.16">limits</entity> and <entity id="C92-2119.17">conversations</entity> will often be a failure. This <entity id="C92-2119.18">paper</entity> presents some <entity id="C92-2119.19">possibilities</entity> of a <entity id="C92-2119.20">structural</entity> <entity id="C92-2119.21">approach</entity> for handling <entity id="C92-2119.22">communication</entity> failures in <entity id="C92-2119.23">task-oriented</entity> oral <entity id="C92-2119.24">dialogues</entity> . Several <entity id="C92-2119.25">types</entity> of <entity id="C92-2119.26">communication</entity> failures are presented and explained. They must be <entity id="C92-2119.27">dealt</entity> with by the <entity id="C92-2119.28">dialogue</entity> <entity id="C92-2119.29">manager</entity> if we strike to have a <entity id="C92-2119.30">robust</entity> <entity id="C92-2119.31">system</entity> . The exposed <entity id="C92-2119.32">strategies</entity> for handling these failures are <entity id="C92-2119.33">based</entity> on a <entity id="C92-2119.34">structural</entity> <entity id="C92-2119.35">approach</entity> of the <entity id="C92-2119.36">conversation</entity> and are <entity id="C92-2119.37">implemented</entity> in the SUNDIAL <entity id="C92-2119.38">system</entity> . We first <entity id="C92-2119.39">recall</entity> some <entity id="C92-2119.40">aspects</entity> of the <entity id="C92-2119.41">model</entity> and then describe the <entity id="C92-2119.42">strategies</entity> for preventing and <entity id="C92-2119.43">repairing</entity> <entity id="C92-2119.44">communication</entity> failure in oral <entity id="C92-2119.45">conversations</entity> with a <entity id="C92-2119.46">system</entity> .
</abstract>


</text>

<text id="C92-3131">
<title>
Causal <entity id="C92-3131.1">Ambiguity</entity> In <entity id="C92-3131.2">Natural Language</entity> : Conceptual <entity id="C92-3131.3">Representation</entity> Of'parce Que/because' And 'puisque/since'
</title>
<abstract>
This <entity id="C92-3131.4">research</entity> <entity id="C92-3131.5">deals</entity> with the <entity id="C92-3131.6">representation</entity> of causal <entity id="C92-3131.7">relations</entity> found in <entity id="C92-3131.8">texts</entity> written in <entity id="C92-3131.9">natural language</entity> , in <entity id="C92-3131.10">order</entity> for KALIPSOS [1], an <entity id="C92-3131.11">NL-understanding</entity> and <entity id="C92-3131.12">question-answering system</entity> , to encode causal <entity id="C92-3131.13">information</entity> in conceptual graphs so as to handle causal <entity id="C92-3131.14">information</entity> and <entity id="C92-3131.15">reasoning</entity> . <entity id="C92-3131.16">Natural <entity id="C92-3131.17">languages</entity></entity> such as French or <entity id="C92-3131.18">English</entity> have many ways to express a causal <entity id="C92-3131.19">relation</entity> . It can be <entity id="C92-3131.20">syntactic</entity> (parce que/because) {provoquer/to produce), (Je me suis cass
e la jambe el je n'ai pas pu venir/1 broke my leg and I couldn't come), parce que/because puisque/since parce que/because puisque/since
</abstract>


</text>

<text id="C92-3147">
<title>
B-SURE: A Believed <entity id="C92-3147.1">Situation</entity> And Uncertain- <entity id="C92-3147.2">Action</entity> <entity id="C92-3147.3">Representation</entity> <entity id="C92-3147.4">Environment</entity></title>
<abstract>
Tliis <entity id="C92-3147.5">paper</entity> presents a <entity id="C92-3147.6">system</entity> that is capable of representing <entity id="C92-3147.7">situations</entity> , states, and nondeterniinistic <entity id="C92-3147.8">nonmonotonic-outcome</entity> <entity id="C92-3147.9">actions</entity> occurring in multiple possible worlds. The <entity id="C92-3147.10">system</entity> <entity id="C92-3147.11">supports</entity> explicit <entity id="C92-3147.12">representations</entity> of <entity id="C92-3147.13">actions</entity> and <entity id="C92-3147.14">situations</entity> used in intentional <entity id="C92-3147.15">action</entity> <entity id="C92-3147.16">theory</entity> and <entity id="C92-3147.17">situation</entity> <entity id="C92-3147.18">theory</entity> , l
oth <entity id="C92-3147.19">types</entity> and <entity id="C92-3147.20">instances</entity> are <entity id="C92-3147.21">supported</entity> . Situations and states before and after nonmonotonic <entity id="C92-3147.22">actions</entity> can be represented simultaneously. Agents have free will as to whether to choose to <entity id="C92-3147.23">perform</entity> an <entity id="C92-3147.24">action</entity> or not. Situations mid <entity id="C92-3147.25">actions</entity> can have expected values, allowing the <entity id="C92-3147.26">system</entity> to <entity id="C92-3147.27">support</entity> <entity id="C92-3147.28">decision-making</entity> and <entity id="C92-3147.29">decision-based</entity> plan <entity id="C92-3147.30">inferencing</entity> . The <entity id="C92-3147.31">system</entity> can <entity id="C92-3147.32">perform</entity> global <entity id="C92-3147.33">reasoning</entity> simultaneously across multiple possible worlds, without being forced to extend each world explicitly. The <entity id="C92-3147.34">resulting</entity> <entity id="C92-3147.35">system</entity> is useful for Biich <entity id="C92-3147.36">natural language</entity> <entity id="C92-3147.37">tasks</entity> as plan <entity id="C92-3147.38">recognition</entity> , intentions <entity id="C92-3147.39">modeling</entity> , and parallel <entity id="C92-3147.40">task</entity> scheduling.
</abstract>


</text>

<text id="C94-1038">
<title>
An <entity id="C94-1038.1">Architecture</entity> For A Universal <entity id="C94-1038.2">Lexicon</entity> A <entity id="C94-1038.3">Case <entity id="C94-1038.4">Study</entity></entity> On Shared <entity id="C94-1038.5">Syntactic <entity id="C94-1038.6">Information</entity></entity> In <entity id="C94-1038.7">Japanese</entity> , Hindi, <entity id="C94-1038.8">Bengali</entity> , <entity id="C94-1038.9">Greek</entity> , And <entity id="C94-1038.10">English</entity></title> 
<abstract>Pustejovsky , James ,The Generative <entity id="C94-1038.11">Lexicon</entity> , <entity id="C94-1038.12">Computation</entity> al <entity id="C94-1038.13">Linguistics</entity> ,1991</abstract>


</text>

<text id="C94-1047">
<title><entity id="C94-1047.1">Logic</entity> <entity id="C94-1047.2">Compression</entity> Of Dictionaries For Multilingual Spelling Checkers
</title>
<abstract>
"To <entity id="C94-1047.3">provide</entity> practical spelling checkers on <entity id="C94-1047.4">micro-computers</entity> , good <entity id="C94-1047.5">compression</entity> <entity id="C94-1047.6">algorithms</entity> are essential. <entity id="C94-1047.7">Current</entity> <entity id="C94-1047.8">techniques</entity> used to compress <entity id="C94-1047.9">lexicons</entity> for indo-Europcan <entity id="C94-1047.10">languages</entity> <entity id="C94-1047.11">provide</entity> efficient spelling checker. <entity id="C94-1047.12">Applying</entity> the same <entity id="C94-1047.13">methods</entity> to <entity id="C94-1047.14">languages</entity> which have a different morphological <entity id="C94-1047.15">system</entity> (Arabic, Turkish,...) gives insufficient <entity id="C94-1047.16">results</entity> . To get better <entity id="C94-1047.17">results</entity> , we <entity id="C94-1047.18">apply</entity> other ""logical"" <entity id="C94-1047.19">compression</entity> <entity id="C94-1047.20">mechanisms</entity> <entity id="C94-1047.21">based</entity> on the <entity id="C94-1047.22">structure</entity> of the <entity id="C94-1047.23">language</entity> itself. <entity id="C94-1047.24">Experiments</entity> with multilingual <entity id="C94-1047.25">dictionaries</entity> show a significant <entity id="C94-1047.26">reduction</entity> <entity id="C94-1047.27">rate</entity> attributable to our <entity id="C94-1047.28">logic</entity> <entity id="C94-1047.29">compression</entity> alone and even better <entity id="C94-1047.30">results</entity> when using our <entity id="C94-1047.31">method</entity> in <entity id="C94-1047.32">conjunction</entity> with existing <entity id="C94-1047.33">methods</entity> . KEY <entity id="C94-1047.34">WORDS</entity> : "
</abstract>


</text>

<text id="C94-1103">
<title>
CLAWS4: The <entity id="C94-1103.1">Tagging</entity> Of The British National <entity id="C94-1103.2">Corpus</entity></title>
<abstract>
The <entity id="C94-1103.3">main</entity> <entity id="C94-1103.4">purpose</entity> of this <entity id="C94-1103.5">paper</entity> is to describe the CLAWS4 <entity id="C94-1103.6">general-purpose</entity> grammatical tagger, used for the <entity id="C94-1103.7">tagging</entity> of the 100- <entity id="C94-1103.8">million-word</entity> British National <entity id="C94-1103.9">Corpus</entity> , of which c.70 million <entity id="C94-1103.10">words</entity> have been <entity id="C94-1103.11">tagged</entity> at the <entity id="C94-1103.12">time</entity> of writing (April  1994 ).tagsets <entity id="C94-1103.13">input</entity> <entity id="C94-1103.14">formats</entity> . <entity id="C94-1103.15">output</entity> <entity id="C94-1103.16">formats</entity> :
</abstract>


</text>

<text id="C94-2169">
<title><entity id="C94-2169.1">Thesaurus-</entity> <entity id="C94-2169.2">Based</entity> Efficient <entity id="C94-2169.3">Example</entity> <entity id="C94-2169.4">Retrieval</entity> By <entity id="C94-2169.5">Generating</entity> <entity id="C94-2169.6">Retrieval</entity> Queries From Similarities
</title>
<abstract>
In <entity id="C94-2169.7">example-based</entity> NLP, the <entity id="C94-2169.8">problem</entity> of <entity id="C94-2169.9">computational</entity> <entity id="C94-2169.10">cost</entity> of <entity id="C94-2169.11">example</entity> <entity id="C94-2169.12">retrieval</entity> is severe , since the <entity id="C94-2169.13">retrieval</entity> <entity id="C94-2169.14">time</entity> <entity id="C94-2169.15">increases</entity> in proportion to the <entity id="C94-2169.16">number</entity> of <entity id="C94-2169.17">examples</entity> in the <entity id="C94-2169.18">database</entity> . This <entity id="C94-2169.19">paper</entity> <entity id="C94-2169.20">proposes</entity> a novel <entity id="C94-2169.21">example</entity> <entity id="C94-2169.22">retrieval</entity> <entity id="C94-2169.23">method</entity> for avoiding full <entity id="C94-2169.24">retrieval</entity> of <entity id="C94-2169.25">examples</entity> . The <entity id="C94-2169.26">proposed</entity> <entity id="C94-2169.27">method</entity> has the following three <entity id="C94-2169.28">features</entity> , 1) it <entity id="C94-2169.29">generates</entity></abstract>


</text>

<text id="C96-1001">
<title>
Discovering The Sounds Of <entity id="C96-1001.1">Discourse Structure</entity> Extended <entity id="C96-1001.2">Abstract</entity></title>
<abstract>
It is widely accepted that <entity id="C96-1001.3">discourses</entity> are composed of <entity id="C96-1001.4">segments</entity> and that the <entity id="C96-1001.5">recognition</entity> of <entity id="C96-1001.6">segment</entity> <entity id="C96-1001.7">boundaries</entity> is essential to a <entity id="C96-1001.8">determination</entity> of <entity id="C96-1001.9">discourse</entity> meaning ( Grosz and  Sidner, 1986 ). Written <entity id="C96-1001.10">language</entity> has orthographic <entity id="C96-1001.11">cues</entity> such as <entity id="C96-1001.12">section</entity> headings, <entity id="C96-1001.13">paragraph</entity> <entity id="C96-1001.14">boundaries</entity> , and <entity id="C96-1001.15">punctuation</entity> which can assist in identifying <entity id="C96-1001.16">discourse structure</entity> . In spoken <entity id="C96-1001.17">language</entity> , into-national <entity id="C96-1001.18">variation</entity> <entity id="C96-1001.19">provides</entity> essential <entity id="C96-1001.20">information</entity> about <entity id="C96-1001.21">discourse structure</entity> . For <entity id="C96-1001.22">instance</entity> , it may be used to mark <entity id="C96-1001.23">structural</entity> <entity id="C96-1001.24">features</entity> of <entity id="C96-1001.25">discourse</entity> at the global <entity id="C96-1001.26">level</entity> , such as <entity id="C96-1001.27">segment</entity> <entity id="C96-1001.28">boundaries</entity> . <entity id="C96-1001.29">Intonation</entity> also <entity id="C96-1001.30">provides</entity> more local <entity id="C96-1001.31">information</entity> about <entity id="C96-1001.32">relations</entity> among <entity id="C96-1001.33">utterances</entity> within a <entity id="C96-1001.34">segment</entity> , for <entity id="C96-1001.35">example</entity> indicating whether <entity id="C96-1001.36">phrases</entity> are parenthetical. It can also <entity id="C96-1001.37">help</entity> distinguish between different <entity id="C96-1001.38">interpretations</entity> of <entity id="C96-1001.39">phrases</entity> that can <entity id="C96-1001.40">function</entity> either as <entity id="C96-1001.41">cue</entity> <entity id="C96-1001.42">phrases</entity> that indicate <entity id="C96-1001.43">discourse</entity> <entity id="C96-1001.44">segment</entity> <entity id="C96-1001.45">boundaries</entity> or sentcntially to convey <entity id="C96-1001.46">domain</entity> <entity id="C96-1001.47">information</entity> . Finally, <entity id="C96-1001.48">variations</entity> in intonational <entity id="C96-1001.49">prominence</entity> may be used to convey <entity id="C96-1001.50">information</entity> about the <entity id="C96-1001.51">discourse</entity> <entity id="C96-1001.52">status</entity> of <entity id="C96-1001.53">entities</entity> referred to by definite <entity id="C96-1001.54">noun phrases</entity> and pronouns. An <entity id="C96-1001.55">understanding</entity> of intonational <entity id="C96-1001.56">variation</entity> and the ways in which it carries <entity id="C96-1001.57">information</entity> about <entity id="C96-1001.58">discourse</entity> <entity id="C96-1001.59">characteristics</entity> of spoken <entity id="C96-1001.60">language</entity> is important for <entity id="C96-1001.61">computer-based</entity> <entity id="C96-1001.62">interpretation</entity> and <entity id="C96-1001.63">generation</entity> of <entity id="C96-1001.64">speech</entity> . From the <entity id="C96-1001.65">interpretation</entity> <entity id="C96-1001.66">perspective</entity> , this <entity id="C96-1001.67">understanding</entity> may <entity id="C96-1001.68">provide</entity> new <entity id="C96-1001.69">techniques</entity> for identifying <entity id="C96-1001.70">discourse <entity id="C96-1001.71">structure</entity></entity> . From the <entity id="C96-1001.72">generation</entity> <entity id="C96-1001.73">perspective</entity> , it would lead to more <entity id="C96-1001.74">natural</entity> synthetic <entity id="C96-1001.75">speech</entity> , making it possible to produce <entity id="C96-1001.76">computer</entity> <entity id="C96-1001.77">speech</entity> that is easier for people to understand and less susceptible to misinterpretation. Three major <entity id="C96-1001.78">challenges</entity> have faced <entity id="C96-1001.79">researchers</entity> attempting to discover the <entity id="C96-1001.80">relationship</entity> between intonational <entity id="C96-1001.81">features</entity> and the <entity id="C96-1001.82">structure</entity> of spoken <entity id="C96-1001.83">discourse</entity> . First, the <entity id="C96-1001.84">collection</entity> of <entity id="C96-1001.85">corpora</entity> of <entity id="C96-1001.86">spontaneous <entity id="C96-1001.87">speech</entity></entity> has <entity id="C96-1001.88">required</entity> the <entity id="C96-1001.89">development</entity> of * The <entity id="C96-1001.90">research</entity> described in this <entity id="C96-1001.91">presentation</entity> was <entity id="C96-1001.92">supported</entity> by the National <entity id="C96-1001.93">Science</entity> Foundation, <entity id="C96-1001.94">Grant</entity> IRI 94-04756. The <entity id="C96-1001.95">research</entity> has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn <entity id="C96-1001.96">provided</entity> invaluable technical assistance. new <entity id="C96-1001.97">experimental</entity> <entity id="C96-1001.98">methodologies</entity> . Whereas it is straightforward to have the same <entity id="C96-1001.99">text</entity> read by many speakers, it is much more difficult to obtain similar <entity id="C96-1001.100">samples</entity> of <entity id="C96-1001.101">spontaneous <entity id="C96-1001.102">speech</entity></entity> from multiple speakers. Second, <entity id="C96-1001.103">techniques</entity> must be <entity id="C96-1001.104">developed</entity> to obtain reliable segmentations and labelings of the <entity id="C96-1001.105">corpora</entity>. Because <entity id="C96-1001.106">discourse structure</entity> is rooted in <entity id="C96-1001.107">semantics</entity> rather than <entity id="C96-1001.108">syntax</entity> , this has proved more difficult than <entity id="C96-1001.109">tagging</entity> <entity id="C96-1001.110">corpora</entity> for <entity id="C96-1001.111">sentence structure</entity> . Third, measures of <entity id="C96-1001.112">agreement</entity> among segmentations must be <entity id="C96-1001.113">designed</entity> . In this <entity id="C96-1001.114">area</entity> too, the <entity id="C96-1001.115">semantic</entity> <entity id="C96-1001.116">nature</entity> of <entity id="C96-1001.117">discourse structure</entity> leads to a more <entity id="C96-1001.118">complex</entity> <entity id="C96-1001.119">problem</entity> than comparing <entity id="C96-1001.120">sentence</entity> <entity id="C96-1001.121">parse</entity> <entity id="C96-1001.122">structures</entity> . This talk will begin with a <entity id="C96-1001.123">summary</entity> of <entity id="C96-1001.124">pilot</entity> <entity id="C96-1001.125">studies</entity> that demonstrated reliable <entity id="C96-1001.126">correlations</entity> of <entity id="C96-1001.127">discourse structure</entity> and intonational <entity id="C96-1001.128">features</entity> ( Grosz and  Hirschberg, 1992 ; Hirschbcrg and  Grosz, 1992 ; Hirschbcrg and  Grosz, 1994 ). It will then <entity id="C96-1001.129">focus</entity> on a new <entity id="C96-1001.130">corpus</entity> of <entity id="C96-1001.131">direction-giving</entity> monologues, the Boston Directions <entity id="C96-1001.132">Corpus</entity> ( Nakatani et al., 1995a ; Hirschberg and  Nakatani, 1996 ). I will describe the <entity id="C96-1001.133">methodology</entity> we <entity id="C96-1001.134">developed</entity> to elicit fluent spontaneous <entity id="C96-1001.135">direction-giving</entity> monologues ranging over a <entity id="C96-1001.136">spectrum</entity> of planning <entity id="C96-1001.137">complexity</entity> . Next I will describe the <entity id="C96-1001.138">development</entity> of annotation <entity id="C96-1001.139">instructions</entity> used to <entity id="C96-1001.140">train</entity> labelers to <entity id="C96-1001.141">segment</entity> spoken <entity id="C96-1001.142">discourses</entity> ( Nakatani et ah, 1995b) and will discuss <entity id="C96-1001.143">agreement</entity> among segmentations on the Boston Directions <entity id="C96-1001.144">Corpus</entity> obtained using these <entity id="C96-1001.145">instructions</entity> . Then I will describe <entity id="C96-1001.146">results</entity> of our <entity id="C96-1001.147">analyses</entity> of the <entity id="C96-1001.148">correlation</entity> between <entity id="C96-1001.149">discourse structure</entity> and intonational <entity id="C96-1001.150">features</entity> . Finally, I will present a <entity id="C96-1001.151">list</entity> of <entity id="C96-1001.152">challenges</entity> for future <entity id="C96-1001.153">research</entity> in this <entity id="C96-1001.154">area</entity> .
</abstract>


</text>

<text id="C96-1010">
<title><entity id="C96-1010.1">Parsing</entity> <entity id="C96-1010.2">Spoken <entity id="C96-1010.3">Language</entity></entity> Without <entity id="C96-1010.4">Syntax</entity></title> 
<abstract><entity id="C96-1010.5">Parsing</entity> <entity id="C96-1010.6">spontaneous <entity id="C96-1010.7">speech</entity></entity> is a difficult <entity id="C96-1010.8">task</entity> because of the ungrammatical <entity id="C96-1010.9">nature</entity> of most spoken <entity id="C96-1010.10">utterances</entity> . To overpass this <entity id="C96-1010.11">problem</entity> , we <entity id="C96-1010.12">propose</entity> in this <entity id="C96-1010.13">paper</entity> to handle the spoken <entity id="C96-1010.14">language</entity> without considering <entity id="C96-1010.15">syntax</entity> . We describe thus a microsemantic <entity id="C96-1010.16">parser</entity> which is uniquely <entity id="C96-1010.17">based</entity> on an associative <entity id="C96-1010.18">network</entity> of <entity id="C96-1010.19">semantic</entity> priming. <entity id="C96-1010.20">Experimental</entity> <entity id="C96-1010.21">results</entity> on <entity id="C96-1010.22">spontaneous speech</entity> show that this <entity id="C96-1010.23">parser</entity> stands for a <entity id="C96-1010.24">robust</entity> <entity id="C96-1010.25">alternative</entity> to <entity id="C96-1010.26">standard</entity> ones.
</abstract>


</text>

<text id="C86-1014">
<title><entity id="C86-1014.1">Processing</entity> <entity id="C86-1014.2">Word</entity> <entity id="C86-1014.3">Order</entity> <entity id="C86-1014.4">Variation</entity> Within A Modified ID/LP <entity id="C86-1014.5">Framework</entity></title>
<abstract>
"From a ""well represented <entity id="C86-1014.6">sample</entity> of world <entity id="C86-1014.7">languages</entity> Steele (1978) shows that about "
</abstract>


</text>

<abstract></abstract>

<text id="C86-1074">
<title>
A Compositional <entity id="C86-1074.1">Approach</entity> To The <entity id="C86-1074.2">Translation</entity> Of Temporal Expressions In The ROSETTA <entity id="C86-1074.3">System</entity></title>
<abstract>
This <entity id="C86-1074.4">paper</entity> discusses the <entity id="C86-1074.5">translation</entity> of temporal <entity id="C86-1074.6">expressions</entity> , in the <entity id="C86-1074.7">framework</entity> of the <entity id="C86-1074.8">machine translation system</entity> Rosetta. The <entity id="C86-1074.9">translation</entity> <entity id="C86-1074.10">method</entity> of Rosetta, the 'isomorphic grammar <entity id="C86-1074.11">method</entity> ', is <entity id="C86-1074.12">based</entity> on Montague 's Compositionality <entity id="C86-1074.13">Principle</entity> . It is shown that a compositional <entity id="C86-1074.14">approach</entity> leads to a transparent account of the <entity id="C86-1074.15">complex</entity> <entity id="C86-1074.16">aspects</entity> of <entity id="C86-1074.17">time</entity> in <entity id="C86-1074.18">natural language</entity> and can be used for the <entity id="C86-1074.19">translation</entity> of temporal <entity id="C86-1074.20">expressions</entity> .
</abstract>


</text>

<text id="C86-1078">
<title>
Pragmatics In <entity id="C86-1078.1">Machine Translation</entity></title>
<abstract>
TEXAN is a <entity id="C86-1078.2">system</entity> of transferi-oriented <entity id="C86-1078.3">text <entity id="C86-1078.4">analysis</entity></entity> . Its linguistic <entity id="C86-1078.5">concept</entity> is <entity id="C86-1078.6">based</entity> on a communicative <entity id="C86-1078.7">approach</entity> within the <entity id="C86-1078.8">framework</entity> of <entity id="C86-1078.9">speech act</entity> <entity id="C86-1078.10">theory</entity> . In this view <entity id="C86-1078.11">texts</entity> are considered to be the <entity id="C86-1078.12">result</entity> of linguistic <entity id="C86-1078.13">actions</entity> . It is assumed that they <entity id="C86-1078.14">control</entity> the <entity id="C86-1078.15">selection</entity> of <entity id="C86-1078.16">translation</entity> equivalents. The <entity id="C86-1078.17">transition</entity> of this <entity id="C86-1078.18">concept</entity> of linguistic <entity id="C86-1078.19">actions</entity> ( <entity id="C86-1078.20">text</entity> acts) to the <entity id="C86-1078.21">model</entity> of <entity id="C86-1078.22">computer</entity> <entity id="C86-1078.23">analysis</entity> is <entity id="C86-1078.24">performed</entity> by a <entity id="C86-1078.25">context-free</entity> il locution grammar <entity id="C86-1078.26">processing</entity> <entity id="C86-1078.27">categories</entity> of <entity id="C86-1078.28">actions</entity> and a propositional <entity id="C86-1078.29">structure</entity> of states of affairs. The grammar which is related to a <entity id="C86-1078.30">text</entity> <entity id="C86-1078.31">lexicon</entity> <entity id="C86-1078.32">provides</entity> the connection of these <entity id="C86-1078.33">categories</entity> and the linguistic <entity id="C86-1078.34">surface</entity> <entity id="C86-1078.35">units</entity> of a single <entity id="C86-1078.36">language</entity> .
</abstract>


</text>

<text id="C86-1133">
<title>
From <entity id="C86-1133.1">Structure</entity> To <entity id="C86-1133.2">Process</entity> <entity id="C86-1133.3">Computer-</entity> Assisted Teaching Of Various <entity id="C86-1133.4">Strategies</entity> For <entity id="C86-1133.5">Generating</entity> Pronoun Constructions In French
</title>
<abstract>
This <entity id="C86-1133.6">paper</entity> describes an <entity id="C86-1133.7">implemented</entity> tutoring <entity id="C86-1133.8">system</entity> (2), <entity id="C86-1133.9">designed</entity> to <entity id="C86-1133.10">help</entity> students to <entity id="C86-1133.11">generate</entity> clitic-constructions in French. While showing various ways of converting a given meaning <entity id="C86-1133.12">structure</entity> into its corresponding <entity id="C86-1133.13">surface</entity> <entity id="C86-1133.14">expression</entity> , the <entity id="C86-1133.15">system</entity> <entity id="C86-1133.16">helps</entity> not only to discover what
</abstract>


</text>

<text id="C86-1139">
<title>
Divided And <entity id="C86-1139.1">Valency-</entity> Oriented <entity id="C86-1139.2">Parsing</entity> In <entity id="C86-1139.3">Speech</entity> Undstanding
</title>
<abstract>
A <entity id="C86-1139.4">parsing</entity> <entity id="C86-1139.5">scheme</entity> for spoken <entity id="C86-1139.6">utterances</entity> is <entity id="C86-1139.7">proposed</entity> that deviates from traditional 'one go' left to right <entity id="C86-1139.8">sentence</entity> <entity id="C86-1139.9">parsing</entity> in that it d
vides the <entity id="C86-1139.10">parsing</entity> <entity id="C86-1139.11">process</entity> first into two aeperate parallel <entity id="C86-1139.12">processes</entity> . <entity id="C86-1139.13">Verbal</entity> <entity id="C86-1139.14">constituents</entity> and nominal <entity id="C86-1139.15">phrases</entity> ( <entity id="C86-1139.16">including</entity> prepositonal <entity id="C86-1139.17">phrases</entity> ) are treated seperately and only brought together in an <entity id="C86-1139.18">utterance</entity> <entity id="C86-1139.19">parser</entity> . This allows especially the <entity id="C86-1139.20">utterance</entity> <entity id="C86-1139.21">parser</entity> to draw on <entity id="C86-1139.22">valency</entity> <entity id="C86-1139.23">information</entity> right from beginning when amalgamating the nominal <entity id="C86-1139.24">constituents</entity> to the <entity id="C86-1139.25">verbal</entity> <entity id="C86-1139.26">core</entity> by means of binary <entity id="C86-1139.27">sentence</entity> <entity id="C86-1139.28">rules</entity> . The <entity id="C86-1139.29">paper</entity> also discusses <entity id="C86-1139.30">problems</entity> of representing the <entity id="C86-1139.31">valency</entity> <entity id="C86-1139.32">information</entity> in <entity id="C86-1139.33">case-frames</entity> arising in a spoken <entity id="C86-1139.34">language</entity> <entity id="C86-1139.35">environment</entity> .
</abstract>


</text>

<text id="C86-1144">
<title><entity id="C86-1144.1">Computational</entity> <entity id="C86-1144.2">Phonology</entity> : Merged, Not Mixed
</title>
<abstract><entity id="C86-1144.3">Research</entity> into <entity id="C86-1144.4">text-to-speech</entity> <entity id="C86-1144.5">systems</entity> has become a rather important <entity id="C86-1144.6">topic</entity> in the <entity id="C86-1144.7">areas</entity> of <entity id="C86-1144.8">linguistics</entity> and phonetics. Particularly for <entity id="C86-1144.9">English</entity> , several <entity id="C86-1144.10">text-to-speech</entity> <entity id="C86-1144.11">systems</entity> have been established (cf. for <entity id="C86-1144.12">example</entity> Hertz (1982), Klatt (1976)). For Dutch, <entity id="C86-1144.13">text-to-speech</entity> <entity id="C86-1144.14">systems</entity> are being <entity id="C86-1144.15">developed</entity> at the <entity id="C86-1144.16">University</entity> of Nijmegen (cf. Wester (1984)) and at the <entity id="C86-1144.17">Universities</entity> of Utrecht and Leyden and the Institute of Perception <entity id="C86-1144.18">Research</entity> (IPO) Eindhoven as well. In this <entity id="C86-1144.19">paper</entity> we will be <entity id="C86-1144.20">concerned</entity> with the <entity id="C86-1144.21">grapheme-to-phoneme</entity> <entity id="C86-1144.22">conversion</entity> <entity id="C86-1144.23">component</entity> as <entity id="C86-1144.24">part</entity> of the Dutch <entity id="C86-1144.25">text-to-speech</entity> <entity id="C86-1144.26">system</entity> which is being <entity id="C86-1144.27">developed</entity> in Utrecht, Leyden and Eindhoven. One of our primary interests is that the <entity id="C86-1144.28">grapheme-to-phoneme</entity> <entity id="C86-1144.29">system</entity> not only has to <entity id="C86-1144.30">generate</entity> the <entity id="C86-1144.31">input</entity> for <entity id="C86-1144.32">speech <entity id="C86-1144.33">synthesis</entity></entity> , either in allophone or diphone <entity id="C86-1144.34">form</entity> , but that it had to be used for other <entity id="C86-1144.35">purposes</entity> as well. Thus, the <entity id="C86-1144.36">system</entity> has to satisfy the <entity id="C86-1144.37">following</entity> demands: - its <entity id="C86-1144.38">output</entity> must <entity id="C86-1144.39">form</entity> a proper and flexible <entity id="C86-1144.40">input</entity> for diphone as well as allophone <entity id="C86-1144.41">synthesis</entity> ; - it must be possible to easily <entity id="C86-1144.42">generate</entity> phonematized <entity id="C86-1144.43">lists</entity> on the <entity id="C86-1144.44">basis</entity> of orthographic <entity id="C86-1144.45">input</entity> ; - it must be possible to automatically obtain <entity id="C86-1144.46">information</entity> regarding the <entity id="C86-1144.47">relation</entity> between graphemes and <entity id="C86-1144.48">phonemes</entity> in <entity id="C86-1144.49">texts</entity> ; - the <entity id="C86-1144.50">system</entity> has to be <entity id="C86-1144.51">user-friendly</entity> , so that it can be addressed by <entity id="C86-1144.52">linguists</entity> without <entity id="C86-1144.53">computer</entity> <entity id="C86-1144.54">training</entity> (for <entity id="C86-1144.55">example</entity> to <entity id="C86-1144.56">test</entity> their phonological <entity id="C86-1144.57">rules</entity> ). In our view, there are two <entity id="C86-1144.58">aspects</entity> to a <entity id="C86-1144.59">grapheme-to-phoneme</entity> <entity id="C86-1144.60">conversion</entity> <entity id="C86-1144.61">system</entity> : a linguistic and a <entity id="C86-1144.62">computational</entity> one. The <entity id="C86-1144.63">linguist</entity> , in fact, <entity id="C86-1144.64">provides</entity> the grammar necessary for the <entity id="C86-1144.65">conversion</entity> and the engineer <entity id="C86-1144.66">implements</entity> this grammar into a <entity id="C86-1144.67">computer</entity> <entity id="C86-1144.68">system</entity> . Thus, <entity id="C86-1144.69">knowledge</entity> about <entity id="C86-1144.70">spelling</entity> and <entity id="C86-1144.71">linguistics</entity> are separated
</abstract>


</text>

<text id="C88-1037">
<title>
Expressing <entity id="C88-1037.1">Quantifier</entity> <entity id="C88-1037.2">Scope</entity> In French <entity id="C88-1037.3">Generation</entity></title>
<abstract>
In this <entity id="C88-1037.4">paper</entity> we <entity id="C88-1037.5">propose</entity> a new <entity id="C88-1037.6">method</entity> to express <entity id="C88-1037.7">quantification</entity> and especially <entity id="C88-1037.8">quantifier</entity> <entity id="C88-1037.9">scope</entity> in French <entity id="C88-1037.10">generation</entity> . Our <entity id="C88-1037.11">approach</entity> is <entity id="C88-1037.12">based</entity> on two points: the <entity id="C88-1037.13">identification</entity> of the <entity id="C88-1037.14">sentence</entity> <entity id="C88-1037.15">components</entity> between which <entity id="C88-1037.16">quantifier</entity> <entity id="C88-1037.17">scope</entity> can indeed be expressed and a <entity id="C88-1037.18">mechanism</entity> to reinforce the <entity id="C88-1037.19">expression</entity> of <entity id="C88-1037.20">quantifier</entity> <entity id="C88-1037.21">scope</entity> . This <entity id="C88-1037.22">approach</entity> is being integrated in a written French <entity id="C88-1037.23">generator</entity> , <entity id="C88-1037.24">called</entity> Herm
s, which will become the <entity id="C88-1037.25">generator</entity> of a portable <entity id="C88-1037.26">natural <entity id="C88-1037.27">language</entity> <entity id="C88-1037.28">interface</entity></entity> .
</abstract>


</text>

<text id="C88-1061">
<title><entity id="C88-1061.1">Constituent</entity> <entity id="C88-1061.2">Coordination</entity> In <entity id="C88-1061.3">Lexical-</entity> Functional Grammar
</title>
<abstract>
"<entity id="C88-1061.4">Abstract</entity> : This <entity id="C88-1061.5">paper</entity> <entity id="C88-1061.6">outlines</entity> a <entity id="C88-1061.7">theory</entity> of <entity id="C88-1061.8">constituent</entity> <entity id="C88-1061.9">coordination</entity> for <entity id="C88-1061.10">Lexical-</entity> Functional Grammar. On this <entity id="C88-1061.11">theory</entity> LFG's flat, unstructured sets <entity id="C88-1061.12">arc</entity> used as the functional <entity id="C88-1061.13">representation</entity> of coordinate <entity id="C88-1061.14">constructions</entity> . <entity id="C88-1061.15">Function-application</entity> is extended to sets by treating a sot formally as the <entity id="C88-1061.16">generalization</entity> of its functional elements. This causes <entity id="C88-1061.17">properties</entity> attributed externally to a coordinate <entity id="C88-1061.18">structure</entity> to be uniformly <entity id="C88-1061.19">distributed</entity> across its elements, without <entity id="C88-1061.20">requiring</entity> additional grammatical <entity id="C88-1061.21">specifications</entity> . <entity id="C88-1061.22">Introduction</entity> A proper <entity id="C88-1061.23">treatment</entity> of <entity id="C88-1061.24">coordination</entity> has long been an elusive <entity id="C88-1061.25">goal</entity> of both theoretical and <entity id="C88-1061.26">computational</entity> <entity id="C88-1061.27">approaches</entity> to <entity id="C88-1061.28">language</entity> . The original transformational <entity id="C88-1061.29">formulation</entity> in <entity id="C88-1061.30">terms</entity> of the Coordinate <entity id="C88-1061.31">Reduction</entity> <entity id="C88-1061.32">rule</entity> (e.g. / Dougherty 1970 /) was quickly shown to have many theoretical and empirical inadequacies, and only recently have <entity id="C88-1061.33">linguistic theories</entity> (e g, GPSG / Gazdar et al. 1985 /, <entity id="C88-1061.34">Categorial grammar</entity> (e.g. / Steedman 1985 /) made substantial <entity id="C88-1061.35">progress</entity> on characterizing the <entity id="C88-1061.36">complex</entity> <entity id="C88-1061.37">restrictions</entity> on coordinate <entity id="C88-1061.38">constructions</entity> and also on their <entity id="C88-1061.39">semantic <entity id="C88-1061.40">interpretations</entity></entity> . <entity id="C88-1061.41">Coordination</entity> has also presented descriptive <entity id="C88-1061.42">problems</entity> for <entity id="C88-1061.43">computational</entity> <entity id="C88-1061.44">approaches</entity>. Typically these have been <entity id="C88-1061.45">solved</entity> by special <entity id="C88-1061.46">devices</entity> that are added to the <entity id="C88-1061.47">parsing</entity> <entity id="C88-1061.48">algorithms</entity> to analyze coordinate <entity id="C88-1061.49">constructions</entity> that cannot easily be characterized in explicit <entity id="C88-1061.50">rules</entity> of grammar. The best known <entity id="C88-1061.51">examples</entity> of this <entity id="C88-1061.52">kind</entity> of <entity id="C88-1061.53">approach</entity> are SYSCONJ / Woods 1973 /, ESP / Sager 1981 /, and MSG / Dahl and  McCord 1983 /. <entity id="C88-1061.54">Coordination</entity> <entity id="C88-1061.55">phenomena</entity> are usually divided into two <entity id="C88-1061.56">classes</entity> , the so-called <entity id="C88-1061.57">constituent</entity> <entity id="C88-1061.58">coordinations</entity> where the coordinated elements look like otherwise well-motivated phrasal <entity id="C88-1061.59">constituents</entity> II), and nonconstituent <entity id="C88-1061.60">coordination</entity> where the coordinated elements look like <entity id="C88-1061.61">fragments</entity> of phrasal <entity id="C88-1061.62">constituents</entity> (2). (1) (a)  A girl saw Mary and ran to Iiill. (Coordinated <entity id="C88-1061.63">verb</entity> <entity id="C88-1061.64">phrases</entity> ) (b)  A girl saw and hoard Mary . (Coordinated <entity id="C88-1061.65">verbs</entity> ) (2) Iiill went to Chicago on Wednesday and New York on Thursday. Of course, what is or is not a well-motivated <entity id="C88-1061.66">constituent</entity> depends on the <entity id="C88-1061.67">details</entity> of the particular grammatical <entity id="C88-1061.68">theory</entity> . Constituents in transformationally-oriented <entity id="C88-1061.69">theories</entity> , for <entity id="C88-1061.70">example</entity> , are <entity id="C88-1061.71">units</entity> that simplify the feeding <entity id="C88-1061.72">relations</entity> of transformational <entity id="C88-1061.73">rules</entity> , whereas ""<entity id="C88-1061.74">constituents</entity> "" in <entity id="C88-1061.75">categorial grammars</entity> merely reflect the <entity id="C88-1061.76">order</entity> of binary <entity id="C88-1061.77">combinations</entity> and have no other special <entity id="C88-1061.78">motivation</entity> . In <entity id="C88-1061.79">lexical-functional</entity> grammar, <entity id="C88-1061.80">surface</entity> <entity id="C88-1061.81">constituents</entity> are taken to be the <entity id="C88-1061.82">units</entity> of phonological <entity id="C88-1061.83">interpretation</entity> . These may differ markedly from the <entity id="C88-1061.84">units</entity> of functional or <entity id="C88-1061.85">semantic interpretation</entity> , as shown in the <entity id="C88-1061.86">analysis</entity> of Dutch <entity id="C88-1061.87">cross</entity> serial <entity id="C88-1061.88">dependencies</entity> given by/ Bresnan et al. 1982 /. N'onconstituent <entity id="C88-1061.89">coordination</entity> , of course, presents a wide <entity id="C88-1061.90">variety</entity> of <entity id="C88-1061.91">complex</entity> and difficult descriptive <entity id="C88-1061.92">problems</entity> , but <entity id="C88-1061.93">constituent</entity> <entity id="C88-1061.94">coordination</entity> also raises important linguistic <entity id="C88-1061.95">issues</entity> . It is the latter that we <entity id="C88-1061.96">focus</entity> on in this brief <entity id="C88-1061.97">paper</entity> . To a first <entity id="C88-1061.98">approximation</entity> , <entity id="C88-1061.99">constituent</entity> <entity id="C88-1061.100">coordinations</entity> can be analyzed as the <entity id="C88-1061.101">result</entity> of taking two independent <entity id="C88-1061.102">clauses</entity> and factoring out their <entity id="C88-1061.103">common</entity> subparts. The <entity id="C88-1061.104">verb</entity> <entity id="C88-1061.105">coordination</entity> in (lb) is thus related to the fuller <entity id="C88-1061.106">sentence</entity> <entity id="C88-1061.107">coordination</entity> in (3). This <entity id="C88-1061.108">intuition</entity> , which was the <entity id="C88-1061.109">basis</entity> of the Coordinate <entity id="C88-1061.110">Reduction</entity> <entity id="C88-1061.111">Transformation</entity> , accounts for more <entity id="C88-1061.112">complex</entity> <entity id="C88-1061.113">patterns</entity> of acceptability such as (4) illustrates. The <entity id="C88-1061.114">coordination</entity> in (4c) is acceptable because both (4a) and (4b) are, while (4e) is bad because of the independent subcategorization violation in (4d). (3) A girl saw Mary and a girl heard Mary . (4) (a) A gir l dedicated a pie to Bill . (b) A girl gave a pie to Bill . (c) A girl dedicated and gave a pie to Bill . (d) *A girl ate a pie to Bill . (e) *A girl dedicated and ate a pie to Bill . This first <entity id="C88-1061.115">approximation</entity> is frought with <entity id="C88-1061.116">difficulties</entity> . It ensures that <entity id="C88-1061.117">constituents</entity> of like <entity id="C88-1061.118">categories</entity> can be conjoined only if they share some finer <entity id="C88-1061.119">details</entity> of <entity id="C88-1061.120">specification</entity> , but there are more subtle conditions that it does not cover. For <entity id="C88-1061.121">example</entity> , even though (5a) and (5b) are both independently grammatical, the <entity id="C88-1061.122">coordination</entity> in (5c) is unacceptable: (5) (a) The girl promised John to go. (b) The girl persuaded John to go. (c) ""The girl promised and persuaded John to go, (Hint: Who is going"
</abstract>


</text>

<text id="C88-2089">
<title>
An Integrated <entity id="C88-2089.1">Model</entity> For The <entity id="C88-2089.2">Treatment</entity> Of <entity id="C88-2089.3">Time</entity> In <entity id="C88-2089.4">MT-Systems</entity></title>
<abstract>
One of the ways to achieve a good <entity id="C88-2089.5">translation</entity> of <entity id="C88-2089.6">verbal</entity> foras is the morphosyntactic <entity id="C88-2089.7">approach</entity> , which consists in a <entity id="C88-2089.8">function</entity> <entity id="C88-2089.9">pairing</entity> the different morphological tenses that occur in a given <entity id="C88-2089.10">language</entity> with the tenses of the other <entity id="C88-2089.11">language</entity> . Complicated <entity id="C88-2089.12">rules</entity> must be established to calculate the right <entity id="C88-2089.13">pair</entity> for an <entity id="C88-2089.14">expression</entity> , because of the <entity id="C88-2089.15">amount</entity> of discrepancies that different <entity id="C88-2089.16">languages</entity> show with <entity id="C88-2089.17">respect</entity> to each other. 
he way wa have chosen to <entity id="C88-2089.18">deal</entity> with this <entity id="C88-2089.19">problem</entity> is, conversely, the <entity id="C88-2089.20">projection</entity> of the different values coming from <entity id="C88-2089.21">verbs</entity> ( <entity id="C88-2089.22">type</entity> , processivity, morftense, morfaspect, sioodrequirement), from <entity id="C88-2089.23">adverbs</entity> , <entity id="C88-2089.24">prepositional phrases</entity> and temporal M.?s (deixis, <entity id="C88-2089.25">aspect</entity> , <entity id="C88-2089.26">iteration</entity> ), and from subordinate <entity id="C88-2089.27">conjunctions</entity> ( <entity id="C88-2089.28">aspect</entity> , moodrequirement). All this <entity id="C88-2089.29">information</entity> permits to obtain a final value for <entity id="C88-2089.30">aspect</entity> and tense for the whole <entity id="C88-2089.31">sentence</entity> , which later on is percolated, not only to the <entity id="C88-2089.32">verb</entity> <entity id="C88-2089.33">node</entity> , but also to the the rest of elements conveying <entity id="C88-2089.34">information</entity> . Our <entity id="C88-2089.35">proposal</entity> relies on the fact that tense/ <entity id="C88-2089.36">aspect</entity> <entity id="C88-2089.37">calculation</entity> is relevant not only for a good <entity id="C88-2089.38">translation</entity> of <entity id="C88-2089.39">verbs</entity> , but also for a good <entity id="C88-2089.40">translation</entity> of <entity id="C88-2089.41">adverbs</entity> , PPs, temporal Ni-n and <entity id="C88-2089.42">conjunctions</entity> , as we have intended to demonstrate in this <entity id="C88-2089.43">paper</entity> . I. <entity id="C88-2089.44">Introduction</entity> Thin article <entity id="C88-2089.45">deals</entity> with a <entity id="C88-2089.46">methodology</entity> to achieve the right <entity id="C88-2089.47">translation</entity> of temporal <entity id="C88-2089.48">expressions</entity> by giving account of the temporal <entity id="C88-2089.49">reference</entity> and <entity id="C88-2089.50">temporal relations</entity> in/ between <entity id="C88-2089.51">sentences</entity> . The <entity id="C88-2089.52">task</entity> to accomplish is to <entity id="C88-2089.53">translate</entity> <entity id="C88-2089.54">syntactic</entity> marks into <entity id="C88-2089.55">semantic</entity> values that decide/ reflect the aspectunl value of the <entity id="C88-2089.56">sentences</entity> . For our <entity id="C88-2089.57">treatment</entity> of <entity id="C88-2089.58">time</entity> and <entity id="C88-2089.59">aspect</entity> we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking <entity id="C88-2089.60">status</entity> and <entity id="C88-2089.61">events</entity> as primitives and <entity id="C88-2089.62">relations</entity> of precedence und overlapping between them. The <entity id="C88-2089.63">ordering</entity> <entity id="C88-2089.64">relation</entity> between <entity id="C88-2089.65">events</entity> is crucial for deciding about the <entity id="C88-2089.66">aspect</entity> of the <entity id="C88-2089.67">sentences</entity> involved. The present <entity id="C88-2089.68">proposal</entity> presumes an <entity id="C88-2089.69">analysis</entity> and a <entity id="C88-2089.70">generation</entity> <entity id="C88-2089.71">component</entity> that deliver a set of S- <entity id="C88-2089.72">trees</entity> whose leaves correspond to <entity id="C88-2089.73">words</entity> . The pre-terminals have morphosyntatitic and <entity id="C88-2089.74">relational</entity> <entity id="C88-2089.75">information</entity> . As usual, <entity id="C88-2089.76">features</entity> am percolated and <entity id="C88-2089.77">nodes</entity> get <entity id="C88-2089.78">features</entity> assigned . She tine/ aspectual <entity id="C88-2089.79">problem</entity> ia <entity id="C88-2089.80">dealt</entity> with under the perspectiv
 of MT with the aim of sketching a <entity id="C88-2089.81">system</entity> that can be <entity id="C88-2089.82">implemented</entity> independently of the particular <entity id="C88-2089.83">formalisms</entity> of different <entity id="C88-2089.84">MT-systems</entity> . To <entity id="C88-2089.85">outline</entity> a general <entity id="C88-2089.86">model</entity> for the <entity id="C88-2089.87">time</entity> / <entity id="C88-2089.88">aspect</entity> <entity id="C88-2089.89">calculation</entity> in MT we subsume a <entity id="C88-2089.90">system</entity> with PSG <entity id="C88-2089.91">rules</entity> that obtain some <entity id="C88-2089.92">sentence structure</entity> with no regard to a specific grammar <entity id="C88-2089.93">type</entity> ; it could be an augmented PSG, as in METAL, or some <entity id="C88-2089.94">kind</entity> of deep <entity id="C88-2089.95">syntactic structure</entity> , as it is the <entity id="C88-2089.96">case</entity> in Eurotra. The <entity id="C88-2089.97">problem</entity> is the well known fact that <entity id="C88-2089.98">translations</entity> of temporal <entity id="C88-2089.99">expressions</entity> in Ni does not involve a <entity id="C88-2089.100">simple</entity> <entity id="C88-2089.101">mapping</entity> of tenses and adverbials. We could just compare Spanish, rich in <entity id="C88-2089.102">aspect</entity> and tenses vs. German or <entity id="C88-2089.103">English</entity> . That is, a MT <entity id="C88-2089.104">dealing</entity> with Germanic and Romance <entity id="C88-2089.105">languages</entity> is <entity id="C88-2089.106">concerned</entity> with different <entity id="C88-2089.107">parameters</entity> for each <entity id="C88-2089.108">language</entity> ; the whole <entity id="C88-2089.109">practice</entity> in <entity id="C88-2089.110">MT systems</entity> is to <entity id="C88-2089.111">translate</entity> morphological tenses, and syntactical values into <entity id="C88-2089.112">reference</entity> <entity id="C88-2089.113">times</entity> that <entity id="C88-2089.114">include</entity> <entity id="C88-2089.115">events</entity> or slates
</abstract>


</text>

<text id="D08-1053">
<title>
Improved <entity id="D08-1053.1">Sentence</entity> <entity id="D08-1053.2">Alignment</entity> on Parallel Web Pages Using a Stochastic <entity id="D08-1053.3">Tree</entity> <entity id="D08-1053.4">Alignment Model</entity></title>
<abstract>
"Parallel <entity id="D08-1053.5">web <entity id="D08-1053.6">pages</entity></entity> are important <entity id="D08-1053.7">source</entity> of <entity id="D08-1053.8">training</entity> <entity id="D08-1053.9">data</entity> for <entity id="D08-1053.10">statistical machine translation</entity> . In this <entity id="D08-1053.11">paper</entity> , we present a new <entity id="D08-1053.12">approach</entity> to <entity id="D08-1053.13">sentence</entity> <entity id="D08-1053.14">alignment</entity> on parallel <entity id="D08-1053.15">web <entity id="D08-1053.16">pages</entity></entity> . Parallel <entity id="D08-1053.17">web pages</entity> tend to have parallel <entity id="D08-1053.18">structures</entity> , and the <entity id="D08-1053.19">structural</entity> <entity id="D08-1053.20">correspondence</entity> can be indicative <entity id="D08-1053.21">information</entity> for identifying parallel <entity id="D08-1053.22">sentences</entity> . In our <entity id="D08-1053.23">approach</entity> , the <entity id="D08-1053.24">web <entity id="D08-1053.25">page</entity></entity> is represented as a <entity id="D08-1053.26">tree</entity> , and a stochastic <entity id="D08-1053.27">tree</entity> <entity id="D08-1053.28">alignment model</entity> is used to exploit the <entity id="D08-1053.29">structural</entity> <entity id="D08-1053.30">correspondence</entity> for <entity id="D08-1053.31">sentence</entity> <entity id="D08-1053.32">alignment</entity> . <entity id="D08-1053.33">Experiments</entity> show that this <entity id="D08-1053.34">method</entity> significantly enhances <entity id="D08-1053.35">alignment</entity> <entity id="D08-1053.36">accuracy</entity> and <entity id="D08-1053.37">robustness</entity> for parallel <entity id="D08-1053.38">web pages</entity> which are much more diverse and noisy than <entity id="D08-1053.39">standard</entity> <entity id="D08-1053.40">parallel corpora</entity> such as ""Hansard"". With <entity id="D08-1053.41">improved</entity> <entity id="D08-1053.42">sentence</entity> <entity id="D08-1053.43">alignment</entity> <entity id="D08-1053.44">performance</entity> , web mining <entity id="D08-1053.45">systems</entity> are able to acquire parallel <entity id="D08-1053.46">sentences</entity> of higher <entity id="D08-1053.47">quality</entity> from the web. "
</abstract>


</text>

<text id="D08-1055">
<title>
A <entity id="D08-1055.1">Japanese</entity> <entity id="D08-1055.2">Predicate Argument Structure</entity> <entity id="D08-1055.3">Analysis</entity> using <entity id="D08-1055.4">Decision</entity> Lists
</title>
<abstract>
This <entity id="D08-1055.5">paper</entity> describes a new <entity id="D08-1055.6">automatic</entity> <entity id="D08-1055.7">method</entity> for <entity id="D08-1055.8">Japanese</entity> <entity id="D08-1055.9">predicate <entity id="D08-1055.10">argument structure</entity></entity> <entity id="D08-1055.11">analysis</entity> . The <entity id="D08-1055.12">method</entity> learns relevant <entity id="D08-1055.13">features</entity> to assign <entity id="D08-1055.14">case</entity> <entity id="D08-1055.15">roles</entity> to the <entity id="D08-1055.16">argument</entity> of the <entity id="D08-1055.17">target</entity> predicate using the <entity id="D08-1055.18">features</entity> of the <entity id="D08-1055.19">words</entity> located closest to the <entity id="D08-1055.20">target</entity> predicate under various <entity id="D08-1055.21">constraints</entity> such as <entity id="D08-1055.22">dependency</entity> <entity id="D08-1055.23">types</entity> , <entity id="D08-1055.24">words</entity> , <entity id="D08-1055.25">semantic</entity> <entity id="D08-1055.26">categories</entity> , <entity id="D08-1055.27">parts of speech</entity> , functional <entity id="D08-1055.28">words</entity> and predicate voices. We <entity id="D08-1055.29">constructed</entity> <entity id="D08-1055.30">decision</entity> <entity id="D08-1055.31">lists</entity> in which these <entity id="D08-1055.32">features</entity> were sorted by their learned <entity id="D08-1055.33">weights</entity> . Using our <entity id="D08-1055.34">method</entity> , we integrated the <entity id="D08-1055.35">tasks</entity> of <entity id="D08-1055.36">semantic role</entity> labeling and zero-pronoun <entity id="D08-1055.37">identification</entity> , and achieved a 17% <entity id="D08-1055.38">improvement</entity> compared with a baseline <entity id="D08-1055.39">method</entity> in a <entity id="D08-1055.40">sentence level</entity> <entity id="D08-1055.41">performance</entity> <entity id="D08-1055.42">analysis</entity> .
</abstract>


</text>

<text id="D08-1068">
<title>
Joint Unsupervised <entity id="D08-1068.1">Coreference Resolution</entity> with Markov <entity id="D08-1068.2">Logic</entity></title> 
<abstract><entity id="D08-1068.3">Machine</entity> learning <entity id="D08-1068.4">approaches</entity> to <entity id="D08-1068.5">coreference resolution</entity> are typically supervised, and <entity id="D08-1068.6">require</entity> expensive labeled <entity id="D08-1068.7">data</entity> . Some unsuper-vised <entity id="D08-1068.8">approaches</entity> have been <entity id="D08-1068.9">proposed</entity> (e.g., Haghighi and Klein (2007)), but they are less accurate. In this <entity id="D08-1068.10">paper</entity> , we present the first un-supervised <entity id="D08-1068.11">approach</entity> that is competitive with supervised ones. This is made possible by <entity id="D08-1068.12">performing</entity> joint <entity id="D08-1068.13">inference</entity> across <entity id="D08-1068.14">mentions</entity> , in <entity id="D08-1068.15">contrast</entity> to the pairwise <entity id="D08-1068.16">classification</entity> typically used in supervised <entity id="D08-1068.17">methods</entity> , and by using Markov <entity id="D08-1068.18">logic</entity> as a <entity id="D08-1068.19">representation language</entity> , which enables us to easily express <entity id="D08-1068.20">relations</entity> like apposition and predicate nominals. On MUC and <entity id="D08-1068.21">ACE</entity> datasets, our <entity id="D08-1068.22">model</entity> outperforms Haghigi and Klein 's one using only a fraction of the <entity id="D08-1068.23">training</entity> data, and often <entity id="D08-1068.24">matches</entity> or exceeds the <entity id="D08-1068.25">accuracy</entity> of state-of-the-art supervised <entity id="D08-1068.26">models</entity> .
</abstract>


</text>

<text id="E99-1043">
<title>
The GENIA <entity id="E99-1043.1">Project</entity> : <entity id="E99-1043.2">Corpus-</entity> <entity id="E99-1043.3">Based</entity> <entity id="E99-1043.4">Knowledge</entity> <entity id="E99-1043.5">Acquisition</entity> And <entity id="E99-1043.6">Information Extraction</entity> From Genome <entity id="E99-1043.7">Research</entity> <entity id="E99-1043.8">Papers</entity></title>
<abstract>
We present an <entity id="E99-1043.9">outline</entity> of the genome <entity id="E99-1043.10">information</entity> <entity id="E99-1043.11">acquisition</entity> (GENIA) <entity id="E99-1043.12">project</entity> for automatically <entity id="E99-1043.13">extracting</entity> biochemical <entity id="E99-1043.14">information</entity> from <entity id="E99-1043.15">journal</entity> <entity id="E99-1043.16">papers</entity> and <entity id="E99-1043.17">abstracts</entity> . GENIA will be available over the Internet and is <entity id="E99-1043.18">designed</entity> to aid in <entity id="E99-1043.19">information extraction</entity> , <entity id="E99-1043.20">retrieval</entity> and visualisation and to <entity id="E99-1043.21">help</entity> reduce <entity id="E99-1043.22">information</entity> overload on <entity id="E99-1043.23">researchers</entity> . The vast <entity id="E99-1043.24">repository</entity> of <entity id="E99-1043.25">papers</entity> available online in <entity id="E99-1043.26">databases</entity> such as MEDLINE is a <entity id="E99-1043.27">natural</entity> <entity id="E99-1043.28">environment</entity> in which to <entity id="E99-1043.29">develop</entity> <entity id="E99-1043.30">language</entity> <entity id="E99-1043.31">engineering</entity> <entity id="E99-1043.32">methods</entity> and <entity id="E99-1043.33">tools</entity> and is an opportunity to show how <entity id="E99-1043.34">language</entity> <entity id="E99-1043.35">engineering</entity> can play a key <entity id="E99-1043.36">role</entity> on the Internet.
</abstract>


</text>

<text id="E03-1086">
<title>
Interactive <entity id="E03-1086.1">Word Alignment</entity> For <entity id="E03-1086.2">Language</entity> <entity id="E03-1086.3">Engineering</entity></title>
<abstract>
In this <entity id="E03-1086.4">paper</entity> we <entity id="E03-1086.5">report</entity> ongoing work on <entity id="E03-1086.6">developing</entity> an interactive <entity id="E03-1086.7">word <entity id="E03-1086.8">alignment</entity></entity> <entity id="E03-1086.9">environment</entity> that will assist a <entity id="E03-1086.10">user</entity> to quickly produce accurate <entity id="E03-1086.11">full-coverage</entity> <entity id="E03-1086.12">word alignment</entity> in bitexts for different <entity id="E03-1086.13">language</entity> <entity id="E03-1086.14">engineering</entity> <entity id="E03-1086.15">tasks</entity> , such as MT <entity id="E03-1086.16">lexicons</entity> and <entity id="E03-1086.17">gold <entity id="E03-1086.18">standards</entity></entity> for <entity id="E03-1086.19">evaluation</entity> . The <entity id="E03-1086.20">system</entity> uses a graphical <entity id="E03-1086.21">interface</entity> , static and dynamic <entity id="E03-1086.22">resources</entity> as well as <entity id="E03-1086.23">machine</entity> learning <entity id="E03-1086.24">techniques</entity> . We also sketch how the <entity id="E03-1086.25">system</entity> is being integrated with an <entity id="E03-1086.26">automatic</entity> <entity id="E03-1086.27">word</entity> aligner.
</abstract>


</text>

<text id="E06-1032">
<title>
Re- <entity id="E06-1032.1">Evaluation</entity> The <entity id="E06-1032.2">Role</entity> Of Bleu In <entity id="E06-1032.3">Machine Translation</entity> <entity id="E06-1032.4">Research</entity></title>
<abstract>
We argue that the <entity id="E06-1032.5">machine translation</entity> <entity id="E06-1032.6">community</entity> is overly reliant on the Bleu machine <entity id="E06-1032.7">translation</entity> <entity id="E06-1032.8">evaluation <entity id="E06-1032.9">metric</entity></entity> . We show that an <entity id="E06-1032.10">improved</entity> Bleu score is neither necessary nor sufficient for achieving an actual <entity id="E06-1032.11">improvement</entity> in <entity id="E06-1032.12">translation <entity id="E06-1032.13">quality</entity></entity> , and give two significant counterexamples to Bleu 's <entity id="E06-1032.14">correlation</entity> with human <entity id="E06-1032.15">judgments</entity> of <entity id="E06-1032.16">quality</entity> . This offers new potential for <entity id="E06-1032.17">research</entity> which was previously deemed unpromising by an <entity id="E06-1032.18">inability</entity> to <entity id="E06-1032.19">improve</entity> upon Bleu scores.
</abstract>


</text>

<text id="E06-1043">
<title>
Automatically Constructing A <entity id="E06-1043.1">Lexicon</entity> Of <entity id="E06-1043.2">Verb</entity> <entity id="E06-1043.3">Phrase</entity> Idiomatic Combinations
</title>
<abstract>
We investigate the <entity id="E06-1043.4">lexical</entity> and <entity id="E06-1043.5">syntactic</entity> <entity id="E06-1043.6">flexibility</entity> of a <entity id="E06-1043.7">class</entity> of idiomatic <entity id="E06-1043.8">expressions</entity> . We <entity id="E06-1043.9">develop</entity> measures that draw on such linguistic <entity id="E06-1043.10">properties</entity> , and demonstrate that these <entity id="E06-1043.11">statistical</entity> , <entity id="E06-1043.12">corpus-based</entity> measures can be successfully used for distinguishing idiomatic <entity id="E06-1043.13">combinations</entity> from non-idiomatic ones. We also <entity id="E06-1043.14">propose</entity> a means for automatically determining which <entity id="E06-1043.15">syntactic</entity> <entity id="E06-1043.16">forms</entity> a particular <entity id="E06-1043.17">idiom</entity> can appear in, and hence should be <entity id="E06-1043.18">included</entity> in its <entity id="E06-1043.19">lexical</entity> <entity id="E06-1043.20">representation</entity> .
</abstract>


</text>

<abstract></abstract>

<text id="C96-2197">
<title>
An Education And <entity id="C96-2197.1">Research</entity> <entity id="C96-2197.2">Tool</entity> For <entity id="C96-2197.3">Computational</entity> <entity id="C96-2197.4">Semantics</entity></title>
<abstract>
This <entity id="C96-2197.5">paper</entity> describes an interactive graphical <entity id="C96-2197.6">environment</entity> for <entity id="C96-2197.7">computational</entity> <entity id="C96-2197.8">semantics</entity> . The <entity id="C96-2197.9">system</entity> <entity id="C96-2197.10">provides</entity> a teaching <entity id="C96-2197.11">tool</entity> , a stand alone extendible grapher, and a <entity id="C96-2197.12">library</entity> of <entity id="C96-2197.13">algorithms</entity> together with <entity id="C96-2197.14">test suites</entity> . The teaching <entity id="C96-2197.15">tool</entity> allows <entity id="C96-2197.16">users</entity> to work <entity id="C96-2197.17">step</entity> by <entity id="C96-2197.18">step</entity> through <entity id="C96-2197.19">derivations</entity> of <entity id="C96-2197.20">semantic representations</entity> , and to compare the <entity id="C96-2197.21">properties</entity> of various <entity id="C96-2197.22">semantic</entity> <entity id="C96-2197.23">formalisms</entity> such as Intensional <entity id="C96-2197.24">Logic</entity> , DRT, and <entity id="C96-2197.25">Situation</entity> <entity id="C96-2197.26">Semantics</entity> . The <entity id="C96-2197.27">system</entity> is freely available on the Internet.
</abstract>


</text>

<text id="C00-1006">
<title>
The Effects Of <entity id="C00-1006.1">Word</entity> <entity id="C00-1006.2">Order</entity> And Segmentation On <entity id="C00-1006.3">Translation</entity> <entity id="C00-1006.4">Retrieval</entity> <entity id="C00-1006.5">Performance</entity></title>
<abstract>
This <entity id="C00-1006.6">research</entity> looks at the <entity id="C00-1006.7">effects</entity> of <entity id="C00-1006.8">word</entity> <entity id="C00-1006.9">order</entity> and segmentation on <entity id="C00-1006.10">translation</entity> <entity id="C00-1006.11">retrieval</entity> <entity id="C00-1006.12">performance</entity> for an <entity id="C00-1006.13">experimental</entity> <entity id="C00-1006.14">Japanese-</entity> <entity id="C00-1006.15">English</entity> <entity id="C00-1006.16">translation</entity> <entity id="C00-1006.17">memory</entity> <entity id="C00-1006.18">system</entity> . We <entity id="C00-1006.19">implement</entity> a <entity id="C00-1006.20">number</entity> of both <entity id="C00-1006.21">bag-of-words</entity> and <entity id="C00-1006.22">word</entity> <entity id="C00-1006.23">order-sensitive</entity> <entity id="C00-1006.24">similarity</entity> <entity id="C00-1006.25">metrics</entity> , and <entity id="C00-1006.26">test</entity> each over character-based and <entity id="C00-1006.27">word-based</entity> <entity id="C00-1006.28">indexing</entity> . The <entity id="C00-1006.29">translation</entity> <entity id="C00-1006.30">retrieval</entity> <entity id="C00-1006.31">performance</entity> of each <entity id="C00-1006.32">system</entity> <entity id="C00-1006.33">configuration</entity> is <entity id="C00-1006.34">evaluated</entity> empirically through the <entity id="C00-1006.35">notion</entity> of <entity id="C00-1006.36">word</entity> edit <entity id="C00-1006.37">distance</entity> between <entity id="C00-1006.38">translation</entity> <entity id="C00-1006.39">candidate</entity> <entity id="C00-1006.40">outputs</entity> and the <entity id="C00-1006.41">model</entity> <entity id="C00-1006.42">translation</entity> . Our <entity id="C00-1006.43">results</entity> indicate that character-based <entity id="C00-1006.44">indexing</entity> is consistently superior to <entity id="C00-1006.45">word-based</entity> <entity id="C00-1006.46">indexing</entity> , suggesting that segmentation is an unnecessary luxury in the given <entity id="C00-1006.47">domain</entity> . <entity id="C00-1006.48">Word</entity> <entity id="C00-1006.49">order-sensitive</entity> <entity id="C00-1006.50">approaches</entity> are demonstrated to generally outperform <entity id="C00-1006.51">bag-of-words</entity> <entity id="C00-1006.52">methods</entity> , with <entity id="C00-1006.53">source language</entity> <entity id="C00-1006.54">segment-level</entity> edit <entity id="C00-1006.55">distance</entity> proving the most effective <entity id="C00-1006.56">similarity</entity> <entity id="C00-1006.57">metric</entity> .
</abstract>


</text>

<text id="C00-1014">
<title>
Reusing An <entity id="C00-1014.1">Ontology</entity> To <entity id="C00-1014.2">Generate</entity> Numeral Classifiers
</title>
<abstract>
In this <entity id="C00-1014.3">paper</entity> , we present a <entity id="C00-1014.4">solution</entity> to the <entity id="C00-1014.5">problem</entity> of <entity id="C00-1014.6">generating</entity> <entity id="C00-1014.7">Japanese</entity> numeral <entity id="C00-1014.8">classifiers</entity> using <entity id="C00-1014.9">semantic <entity id="C00-1014.10">classes</entity></entity> from an <entity id="C00-1014.11">ontology</entity> . Most <entity id="C00-1014.12">nouns</entity> must take a numeral <entity id="C00-1014.13">classifier</entity> when they are quantified in <entity id="C00-1014.14">languages</entity> such as <entity id="C00-1014.15">Chinese</entity> , <entity id="C00-1014.16">Japanese</entity> , Korean, Malay and Thai. In <entity id="C00-1014.17">order</entity> to select an appropriate <entity id="C00-1014.18">classifier</entity> , we <entity id="C00-1014.19">propose</entity> an <entity id="C00-1014.20">algorithm</entity> which associates <entity id="C00-1014.21">classifiers</entity> with <entity id="C00-1014.22">semantic classes</entity> and uses inheritance to <entity id="C00-1014.23">list</entity> only those <entity id="C00-1014.24">classifiers</entity> which have to be <entity id="C00-1014.25">listed</entity> . It <entity id="C00-1014.26">generates</entity> sortal <entity id="C00-1014.27">classifiers</entity> with an <entity id="C00-1014.28">accuracy</entity> of 81%. We reuse the <entity id="C00-1014.29">ontology</entity> <entity id="C00-1014.30">provided</entity> by Goi-Taikei  a <entity id="C00-1014.31">Japanese</entity> <entity id="C00-1014.32">lexicon</entity> , and show that it is a reasonable <entity id="C00-1014.33">choice</entity> for this <entity id="C00-1014.34">task</entity> , <entity id="C00-1014.35">requiring</entity> <entity id="C00-1014.36">information</entity> to be entered for less than 6% of <entity id="C00-1014.37">individual</entity> <entity id="C00-1014.38">nouns</entity> .
</abstract>


</text>

<text id="C00-1035">
<title>
Aspects Of <entity id="C00-1035.1">Pattern-</entity> Matching In <entity id="C00-1035.2">Data-</entity> Oriented <entity id="C00-1035.3">Parsing</entity></title> 
<abstract><entity id="C00-1035.4">Data-</entity> Oriented <entity id="C00-1035.5">Parsing</entity> (dop) <entity id="C00-1035.6">ranks</entity> among the best <entity id="C00-1035.7">parsing</entity> <entity id="C00-1035.8">schemes</entity> , <entity id="C00-1035.9">pairing</entity> state-of-the art <entity id="C00-1035.10">parsing</entity> <entity id="C00-1035.11">accuracy</entity> to the psycholinguistic <entity id="C00-1035.12">insight</entity> that larger <entity id="C00-1035.13">chunks</entity> of <entity id="C00-1035.14">syntactic structures</entity> are relevant grammatical and probabilistic <entity id="C00-1035.15">units</entity> . <entity id="C00-1035.16">Parsing</entity> with the <entity id="C00-1035.17">dop-model</entity> , however, seems to involve a lot of CPU <entity id="C00-1035.18">cycles</entity> and a considerable <entity id="C00-1035.19">amount</entity> of double work, brought on by the <entity id="C00-1035.20">concept</entity> of multiple <entity id="C00-1035.21">derivations</entity> , which is necessary for probabilistic <entity id="C00-1035.22">processing</entity> , but which is not convincingly related to a proper linguistic <entity id="C00-1035.23">backbone</entity> . It is however possible to reinterpret the <entity id="C00-1035.24">dop-model</entity> as a <entity id="C00-1035.25">pattern-matching</entity> <entity id="C00-1035.26">model</entity> , which tries to maximize the <entity id="C00-1035.27">size</entity> of the <entity id="C00-1035.28">substructures</entity> that <entity id="C00-1035.29">construct</entity> the <entity id="C00-1035.30">parse</entity> , rather than the <entity id="C00-1035.31">probability</entity> of the <entity id="C00-1035.32">parse</entity> . By emphasizing this <entity id="C00-1035.33">memory-based</entity> <entity id="C00-1035.34">aspect</entity> of the <entity id="C00-1035.35">dop-model</entity> , it is possible to do away with multiple <entity id="C00-1035.36">derivations</entity> , opening up <entity id="C00-1035.37">possibilities</entity> for efficient Viterbi-style <entity id="C00-1035.38">optimizations</entity> , while still retaining acceptable <entity id="C00-1035.39">parsing</entity> <entity id="C00-1035.40">accuracy</entity> through enhanced <entity id="C00-1035.41">context-sensitivity</entity> .
</abstract>


</text>

<text id="C00-1060">
<title>
A Hybrid <entity id="C00-1060.1">Japanese</entity> <entity id="C00-1060.2">Parser</entity> With <entity id="C00-1060.3">Hand-</entity> Crafted Grammar And <entity id="C00-1060.4">Statistics</entity></title>
<abstract>
This <entity id="C00-1060.5">paper</entity> describes a hybrid <entity id="C00-1060.6">parsing</entity> <entity id="C00-1060.7">method</entity> for <entity id="C00-1060.8">Japanese</entity> which uses both a <entity id="C00-1060.9">hand-crafted</entity> grammar and a <entity id="C00-1060.10">statistical</entity> <entity id="C00-1060.11">technique</entity> . The key <entity id="C00-1060.12">feature</entity> of our <entity id="C00-1060.13">system</entity> is that in <entity id="C00-1060.14">order</entity> to estimate likelihood for a <entity id="C00-1060.15">parse tree</entity> , the <entity id="C00-1060.16">system</entity> uses <entity id="C00-1060.17">information</entity> taken from <entity id="C00-1060.18">alternative</entity> <entity id="C00-1060.19">partial</entity> <entity id="C00-1060.20">parse <entity id="C00-1060.21">trees</entity></entity> <entity id="C00-1060.22">generated</entity> by the grammar. This utilization of <entity id="C00-1060.23">alternative</entity> <entity id="C00-1060.24">trees</entity> enables us to <entity id="C00-1060.25">construct</entity> a new <entity id="C00-1060.26">statistical model</entity> <entity id="C00-1060.27">called</entity> Triplet/Quadruplet <entity id="C00-1060.28">Model</entity> .We
</abstract>


</text>

<text id="C00-2101">
<title><entity id="C00-2101.1">Learning</entity> <entity id="C00-2101.2">Semantic-</entity> <entity id="C00-2101.3">Level</entity> <entity id="C00-2101.4">Information Extraction</entity> Rules By <entity id="C00-2101.5">Type-</entity> Oriented ILP
</title>
<abstract>
This <entity id="C00-2101.6">paper</entity> describes an <entity id="C00-2101.7">approach</entity> to using <entity id="C00-2101.8">semantic representations</entity> <entity id="C00-2101.9">information extraction</entity> (IE) inductive <entity id="C00-2101.10">logic programming</entity> (ILP)
</abstract>


</text>

<text id="C00-2105">
<title><entity id="C00-2105.1">Robust</entity> German <entity id="C00-2105.2">Noun</entity> Chunking With A Probabilistic <entity id="C00-2105.3">Context-</entity> Free Grammar
</title>
<abstract>
We present a <entity id="C00-2105.4">noun</entity> chunker for German which is <entity id="C00-2105.5">based</entity> on a head-lexicalised probabilistic <entity id="C00-2105.6">context-free</entity> grammar. A manually <entity id="C00-2105.7">developed</entity> grammar was semi-automatically extended with <entity id="C00-2105.8">robustness</entity> <entity id="C00-2105.9">rules</entity> in <entity id="C00-2105.10">order</entity> to allow <entity id="C00-2105.11">parsing</entity> of unrestricted <entity id="C00-2105.12">text</entity> . The <entity id="C00-2105.13">model <entity id="C00-2105.14">parameters</entity></entity> were learned from unlabelled <entity id="C00-2105.15">training</entity> <entity id="C00-2105.16">data</entity> by a probabilistic <entity id="C00-2105.17">context-free</entity> <entity id="C00-2105.18">parser</entity> . For <entity id="C00-2105.19">extracting</entity> <entity id="C00-2105.20">noun</entity> <entity id="C00-2105.21">chunks</entity> , the <entity id="C00-2105.22">parser</entity> <entity id="C00-2105.23">generates</entity> all possible <entity id="C00-2105.24">noun</entity> <entity id="C00-2105.25">chunk</entity> <entity id="C00-2105.26">analyses</entity> , scores them with a novel <entity id="C00-2105.27">algorithm</entity> which maximizes the best <entity id="C00-2105.28">chunk</entity> <entity id="C00-2105.29">sequence</entity> <entity id="C00-2105.30">criterion</entity> , and chooses the most probable <entity id="C00-2105.31">chunk</entity> <entity id="C00-2105.32">sequence</entity> . An <entity id="C00-2105.33">evaluation</entity> of the chunker on 2,140 <entity id="C00-2105.34">hand-annotated</entity> <entity id="C00-2105.35">noun</entity> <entity id="C00-2105.36">chunks</entity> <entity id="C00-2105.37">yielded</entity> 92% <entity id="C00-2105.38">recall</entity> and 93% <entity id="C00-2105.39">precision</entity> .
</abstract>


</text>

<text id="C00-2147">
<title>
The Week At A Glance - <entity id="C00-2147.1">Cross-</entity> <entity id="C00-2147.2">Language</entity> <entity id="C00-2147.3">Cross-</entity> <entity id="C00-2147.4">Document</entity> <entity id="C00-2147.5">Information Extraction</entity> And <entity id="C00-2147.6">Translation</entity></title>
<abstract>
"Work on the production of <entity id="C00-2147.7">texts</entity> in <entity id="C00-2147.8">English</entity> describing <entity id="C00-2147.9">instances</entity> of a particular <entity id="C00-2147.10">event type</entity> from multiple <entity id="C00-2147.11">news</entity> <entity id="C00-2147.12">sources</entity> will be described. A <entity id="C00-2147.13">system</entity> has been <entity id="C00-2147.14">developed</entity> which <entity id="C00-2147.15">extracts</entity> <entity id="C00-2147.16">events</entity> , such as <entity id="C00-2147.17">meetings</entity> , from <entity id="C00-2147.18">texts</entity> in <entity id="C00-2147.19">English</entity> , Russian, Spanish, and <entity id="C00-2147.20">Japanese</entity> . The <entity id="C00-2147.21">extraction</entity> is currently carried out using only ontological <entity id="C00-2147.22">information</entity> . The <entity id="C00-2147.23">results</entity> of a set of such <entity id="C00-2147.24">extractions</entity> were combined to produce a <entity id="C00-2147.25">table</entity> of <entity id="C00-2147.26">event</entity> <entity id="C00-2147.27">instances</entity> , <entity id="C00-2147.28">date</entity> stamped, with <entity id="C00-2147.29">links</entity> back to the original <entity id="C00-2147.30">documents</entity> . The original <entity id="C00-2147.31">documents</entity> can then be summarized and <entity id="C00-2147.32">translated</entity> by the <entity id="C00-2147.33">system</entity> on demand. By using <entity id="C00-2147.34">techniques</entity> from <entity id="C00-2147.35">information retrieval</entity> , <entity id="C00-2147.36">information extraction</entity> , <entity id="C00-2147.37">summarization</entity> , and <entity id="C00-2147.38">machine translation</entity> , in a <entity id="C00-2147.39">multi-lingual</entity> <entity id="C00-2147.40">environment</entity> , new <entity id="C00-2147.41">documents</entity> can be produced which <entity id="C00-2147.42">provide</entity> ""at a glance"" <entity id="C00-2147.43">access</entity> to <entity id="C00-2147.44">news</entity> on <entity id="C00-2147.45">events</entity> from multiple <entity id="C00-2147.46">sources</entity> . The <entity id="C00-2147.47">paper</entity> concludes with a <entity id="C00-2147.48">discussion</entity> of the key <entity id="C00-2147.49">resources</entity> which need to be <entity id="C00-2147.50">developed</entity> to enhance the <entity id="C00-2147.51">accuracy</entity> and <entity id="C00-2147.52">coverage</entity> of the <entity id="C00-2147.53">techniques</entity> used in our <entity id="C00-2147.54">experiment</entity> . "
</abstract>


</text>

<text id="C00-2161">
<title>
Querying Temporal Databases Using Controlled <entity id="C00-2161.1">Natural Language</entity></title>
<abstract>
Recent years have shown a surge in interest in temporal <entity id="C00-2161.2">database</entity> <entity id="C00-2161.3">systems</entity> , which allow <entity id="C00-2161.4">users</entity> to store <entity id="C00-2161.5">time-dependent</entity> <entity id="C00-2161.6">information</entity> . We present a novel controlled <entity id="C00-2161.7">natural language interface</entity> to temporal <entity id="C00-2161.8">databases</entity> , <entity id="C00-2161.9">based</entity> on <entity id="C00-2161.10">translating</entity> <entity id="C00-2161.11">natural language</entity> <entity id="C00-2161.12">questions</entity> into SQL/Temporal, a temporal database <entity id="C00-2161.13">query language</entity> . The <entity id="C00-2161.14">syntactic <entity id="C00-2161.15">analysis</entity></entity> is done using the <entity id="C00-2161.16">Type-</entity> Logical Grammar <entity id="C00-2161.17">framework</entity> , highlighting its <entity id="C00-2161.18">utility</entity> not only as a theoretical <entity id="C00-2161.19">framework</entity> but also as a practical <entity id="C00-2161.20">tool</entity> . The <entity id="C00-2161.21">semantic <entity id="C00-2161.22">analysis</entity></entity> is done using a novel <entity id="C00-2161.23">theory</entity> of the <entity id="C00-2161.24">semantics</entity> of temporal <entity id="C00-2161.25">questions</entity> , <entity id="C00-2161.26">focusing</entity> on the <entity id="C00-2161.27">role</entity> of temporal <entity id="C00-2161.28">preposition</entity> <entity id="C00-2161.29">phrases</entity> rather than the more traditional <entity id="C00-2161.30">focus</entity> on tense and <entity id="C00-2161.31">aspect</entity> . Our <entity id="C00-2161.32">translation</entity> <entity id="C00-2161.33">method</entity> is considerably simpler than previous attempts in this <entity id="C00-2161.34">direction</entity> . We present a <entity id="C00-2161.35">prototype</entity> <entity id="C00-2161.36">software</entity> <entity id="C00-2161.37">implementation</entity> .
</abstract>


</text>

<text id="C02-1003">
<title><entity id="C02-1003.1">Learning</entity> <entity id="C02-1003.2">Chinese</entity> <entity id="C02-1003.3">Bracketing</entity> <entity id="C02-1003.4">Knowledge Based</entity> On A Bilingual <entity id="C02-1003.5">Language Model</entity></title>
<abstract>
This <entity id="C02-1003.6">paper</entity> <entity id="C02-1003.7">proposes</entity> a new <entity id="C02-1003.8">method</entity> for <entity id="C02-1003.9">automatic</entity> <entity id="C02-1003.10">acquisition</entity> of <entity id="C02-1003.11">Chinese</entity> <entity id="C02-1003.12">bracketing</entity> <entity id="C02-1003.13">knowledge</entity> from <entity id="C02-1003.14">English-</entity> <entity id="C02-1003.15">Chinese</entity> <entity id="C02-1003.16">sentence-aligned</entity> bilingual <entity id="C02-1003.17">corpora</entity> . Bilingual <entity id="C02-1003.18">sentence</entity> <entity id="C02-1003.19">pairs</entity> are first aligned in <entity id="C02-1003.20">syntactic structure</entity> by combining <entity id="C02-1003.21">English</entity> <entity id="C02-1003.22">parse trees</entity> with a <entity id="C02-1003.23">statistical</entity> bilingual <entity id="C02-1003.24">language model</entity> . <entity id="C02-1003.25">Chinese</entity> <entity id="C02-1003.26">bracketing</entity> <entity id="C02-1003.27">knowledge</entity> is then <entity id="C02-1003.28">extracted</entity> automatically. The preliminary <entity id="C02-1003.29">experiments</entity> show automatically learned <entity id="C02-1003.30">knowledge</entity> accords well with manually annotated <entity id="C02-1003.31">brackets</entity> . The <entity id="C02-1003.32">proposed</entity> <entity id="C02-1003.33">method</entity> is particularly useful to acquire <entity id="C02-1003.34">bracketing</entity> <entity id="C02-1003.35">knowledge</entity> for a less <entity id="C02-1003.36">studied</entity> <entity id="C02-1003.37">language</entity> that <entity id="C02-1003.38">lacks</entity> <entity id="C02-1003.39">tools</entity> and <entity id="C02-1003.40">resources</entity> found in a <entity id="C02-1003.41">second language</entity> more <entity id="C02-1003.42">studied</entity> . Although this <entity id="C02-1003.43">paper</entity> discusses <entity id="C02-1003.44">experiments</entity> with <entity id="C02-1003.45">Chinese</entity> and <entity id="C02-1003.46">English</entity> , the <entity id="C02-1003.47">method</entity> is also applicable to other <entity id="C02-1003.48">language <entity id="C02-1003.49">pairs</entity></entity> .
</abstract>


</text>

<text id="C02-1007">
<title>
The <entity id="C02-1007.1">Computation</entity> Of <entity id="C02-1007.2">Word</entity> Associations: Comparing Syntagmatic And Paradigmatic <entity id="C02-1007.3">Approaches</entity></title>
<abstract>
It is shown that <entity id="C02-1007.4">basic</entity> <entity id="C02-1007.5">language</entity> <entity id="C02-1007.6">processes</entity> such as the production of free <entity id="C02-1007.7">word</entity> <entity id="C02-1007.8">associations</entity> and the <entity id="C02-1007.9">generation</entity> of <entity id="C02-1007.10">synonyms</entity> can be simulated using <entity id="C02-1007.11">statistical <entity id="C02-1007.12">models</entity></entity> that analyze the <entity id="C02-1007.13">distribution</entity> of <entity id="C02-1007.14">words</entity> in large <entity id="C02-1007.15">text</entity> <entity id="C02-1007.16">corpora</entity> . According to the law of <entity id="C02-1007.17">association</entity> by contiguity, the <entity id="C02-1007.18">acquisition</entity> of <entity id="C02-1007.19">word</entity> <entity id="C02-1007.20">associations</entity> can be explained by Hebbian learning. The free <entity id="C02-1007.21">word</entity> <entity id="C02-1007.22">associations</entity> as produced by subjects on <entity id="C02-1007.23">presentation</entity> of single <entity id="C02-1007.24">stimulus</entity> <entity id="C02-1007.25">words</entity> can thus be predicted by <entity id="C02-1007.26">applying</entity> <entity id="C02-1007.27">first-order</entity> <entity id="C02-1007.28">statistics</entity> to the <entity id="C02-1007.29">frequencies</entity> of <entity id="C02-1007.30">word</entity> co-occurrences as observed in <entity id="C02-1007.31">texts</entity> . The <entity id="C02-1007.32">generation</entity> of <entity id="C02-1007.33">synonyms</entity> can also be conducted on <entity id="C02-1007.34">co-occurrence</entity> <entity id="C02-1007.35">data</entity> but <entity id="C02-1007.36">requires</entity> <entity id="C02-1007.37">second-order</entity> <entity id="C02-1007.38">statistics</entity> . The <entity id="C02-1007.39">reason</entity> is that <entity id="C02-1007.40">synonyms</entity> rarely occur together but appear in similar <entity id="C02-1007.41">lexical</entity> neighborhoods. Both <entity id="C02-1007.42">approaches</entity> are systematically compared and are validated on empirical <entity id="C02-1007.43">data</entity> . It turns out that for both <entity id="C02-1007.44">tasks</entity> the <entity id="C02-1007.45">performance</entity> of the <entity id="C02-1007.46">statistical</entity> <entity id="C02-1007.47">system</entity> is comparable to the <entity id="C02-1007.48">performance</entity> of human subjects .
</abstract>


</text>

<text id="C02-1023">
<title>
A Chart- <entity id="C02-1023.1">Parsing <entity id="C02-1023.2">Algorithm</entity></entity> For Efficient <entity id="C02-1023.3">Semantic <entity id="C02-1023.4">Analysis</entity></entity></title>
<abstract>
"In some <entity id="C02-1023.5">contexts</entity> , well-formed <entity id="C02-1023.6">natural language</entity> cannot be expected as <entity id="C02-1023.7">input</entity> to <entity id="C02-1023.8">information</entity> or <entity id="C02-1023.9">communication</entity> <entity id="C02-1023.10">systems</entity> . In these <entity id="C02-1023.11">contexts</entity> , the use of grammar-independent <entity id="C02-1023.12">input</entity> ( <entity id="C02-1023.13">sequences</entity> of uninflected <entity id="C02-1023.14">semantic</entity> <entity id="C02-1023.15">units</entity> like e.g. <entity id="C02-1023.16">language-independent</entity> <entity id="C02-1023.17">icons</entity> ) can be an answer to the <entity id="C02-1023.18">users</entity> ' needs. However, this <entity id="C02-1023.19">requires</entity> that an intelligent <entity id="C02-1023.20">system</entity> should be able to interpret this <entity id="C02-1023.21">input</entity> with reasonable <entity id="C02-1023.22">accuracy</entity> and in reasonable <entity id="C02-1023.23">time</entity> . Here we <entity id="C02-1023.24">propose</entity> a <entity id="C02-1023.25">method</entity> allowing a purely <entity id="C02-1023.26">semantic-based</entity> <entity id="C02-1023.27">analysis</entity> of <entity id="C02-1023.28">sequences</entity> of <entity id="C02-1023.29">semantic</entity> <entity id="C02-1023.30">units</entity> . It uses an <entity id="C02-1023.31">algorithm</entity> inspired by the idea of ""chart <entity id="C02-1023.32">parsing</entity> "" known in <entity id="C02-1023.33">Natural Language Processing</entity> , which stores intermediate <entity id="C02-1023.34">parsing</entity> <entity id="C02-1023.35">results</entity> in <entity id="C02-1023.36">order</entity> to bring the <entity id="C02-1023.37">calculation</entity> <entity id="C02-1023.38">time</entity> down. "
</abstract>


</text>

<text id="C02-1029">
<title>
A Generative <entity id="C02-1029.1">Probability Model</entity> For <entity id="C02-1029.2">Unification-</entity> <entity id="C02-1029.3">Based</entity> Grammars
</title>
<abstract>
A generative <entity id="C02-1029.4">probability model</entity> for <entity id="C02-1029.5">unification-based</entity> grammars is presented in which <entity id="C02-1029.6">rule</entity> <entity id="C02-1029.7">probabilities</entity> depend on the <entity id="C02-1029.8">feature structure</entity> of the expanded <entity id="C02-1029.9">constituent</entity> . The presented <entity id="C02-1029.10">model</entity> is the first <entity id="C02-1029.11">model</entity> which <entity id="C02-1029.12">requires</entity> no <entity id="C02-1029.13">normalization</entity> and allows the <entity id="C02-1029.14">application</entity> of <entity id="C02-1029.15">dynamic <entity id="C02-1029.16">programming</entity> <entity id="C02-1029.17">algorithms</entity></entity> for <entity id="C02-1029.18">disambiguation</entity> ( Viterbi ) and <entity id="C02-1029.19">training</entity> (Inside-Outside). Another <entity id="C02-1029.20">advantage</entity> is the small <entity id="C02-1029.21">number</entity> of <entity id="C02-1029.22">parameters</entity> .
</abstract>


</text>

<text id="C02-1038">
<title>
Augmenting <entity id="C02-1038.1">Noun</entity> Taxonomies By <entity id="C02-1038.2">Combining</entity> <entity id="C02-1038.3">Lexical</entity> <entity id="C02-1038.4">Similarity</entity> Metrics
</title>
<abstract>
This <entity id="C02-1038.5">paper</entity> presents a <entity id="C02-1038.6">method</entity> for augmenting <entity id="C02-1038.7">taxonomies</entity> with <entity id="C02-1038.8">domain</entity> <entity id="C02-1038.9">information</entity> using a <entity id="C02-1038.10">simple</entity> <entity id="C02-1038.11">combination</entity> of three existing <entity id="C02-1038.12">lexical</entity> <entity id="C02-1038.13">similarity</entity> <entity id="C02-1038.14">metrics</entity> . The combined <entity id="C02-1038.15">approach</entity> is <entity id="C02-1038.16">evaluated</entity> by comparing their <entity id="C02-1038.17">results</entity> against the annotated SEMCOR <entity id="C02-1038.18">corpus</entity> . An <entity id="C02-1038.19">implementation</entity> is described in which WordNet is augmented with thesaural <entity id="C02-1038.20">information</entity> from the CIDE+ <entity id="C02-1038.21">machine readable dictionary</entity> .
</abstract>


</text>

<text id="C02-1045">
<title>
A <entity id="C02-1045.1">Method</entity> Of <entity id="C02-1045.2">Cluster-</entity> <entity id="C02-1045.3">Based</entity> <entity id="C02-1045.4">Indexing</entity> Of Textual <entity id="C02-1045.5">Data</entity></title>
<abstract>
This <entity id="C02-1045.6">paper</entity> presents a <entity id="C02-1045.7">framework</entity> for <entity id="C02-1045.8">clustering</entity> in <entity id="C02-1045.9">text-based</entity> <entity id="C02-1045.10">information retrieval systems</entity> . The prominent <entity id="C02-1045.11">feature</entity> of the <entity id="C02-1045.12">proposed</entity> <entity id="C02-1045.13">method</entity> is that <entity id="C02-1045.14">documents</entity> , <entity id="C02-1045.15">terms</entity> , and other related elements of textual <entity id="C02-1045.16">information</entity> are <entity id="C02-1045.17">clustered</entity> simultaneously into small overlapping <entity id="C02-1045.18">clusters</entity> . In the <entity id="C02-1045.19">paper</entity> , the mathematical <entity id="C02-1045.20">formulation</entity> and <entity id="C02-1045.21">implementation</entity> of the <entity id="C02-1045.22">clustering</entity> <entity id="C02-1045.23">method</entity> are briefly introduced, together with some <entity id="C02-1045.24">experimental</entity> <entity id="C02-1045.25">results</entity> .
</abstract>


</text>

<text id="C02-1059">
<title><entity id="C02-1059.1">Processing</entity> <entity id="C02-1059.2">Japanese</entity> Self- <entity id="C02-1059.3">Correction</entity> In <entity id="C02-1059.4">Speech</entity> <entity id="C02-1059.5">Dialog</entity> <entity id="C02-1059.6">Systems</entity></title> 
<abstract><entity id="C02-1059.7">Speech</entity> <entity id="C02-1059.8">dialog systems</entity> need to <entity id="C02-1059.9">deal</entity> with various <entity id="C02-1059.10">kinds</entity> of ill-formed <entity id="C02-1059.11">speech</entity> <entity id="C02-1059.12">inputs</entity> that appear in <entity id="C02-1059.13">natural</entity> human-human <entity id="C02-1059.14">dialog</entity> . <entity id="C02-1059.15">Self-correction</entity> (or <entity id="C02-1059.16">speech-repair</entity> ) is a particularly problematic <entity id="C02-1059.17">phenomenon</entity> . Although many ways of <entity id="C02-1059.18">dealing</entity> with <entity id="C02-1059.19">self-correction</entity> have been <entity id="C02-1059.20">proposed</entity> , these have <entity id="C02-1059.21">limitations</entity> in both detecting and correcting for this <entity id="C02-1059.22">phenomenon</entity> . In this <entity id="C02-1059.23">paper</entity> , we <entity id="C02-1059.24">propose</entity> a <entity id="C02-1059.25">method</entity> to overcome these <entity id="C02-1059.26">problems</entity> in <entity id="C02-1059.27">Japanese</entity> <entity id="C02-1059.28">speech</entity> <entity id="C02-1059.29">dialog</entity> . We <entity id="C02-1059.30">evaluate</entity> the <entity id="C02-1059.31">proposed</entity> <entity id="C02-1059.32">method</entity> using our <entity id="C02-1059.33">speech</entity> <entity id="C02-1059.34">dialog</entity> <entity id="C02-1059.35">corpus</entity> and discuss its <entity id="C02-1059.36">limitations</entity> and the work that remains to be done.
</abstract>


</text>

<text id="C02-1122">
<title>
Fertilization Of <entity id="C02-1122.1">Case</entity> <entity id="C02-1122.2">Frame</entity> <entity id="C02-1122.3">Dictionary</entity> For <entity id="C02-1122.4">Robust</entity> <entity id="C02-1122.5">Japanese</entity> <entity id="C02-1122.6">Case</entity> <entity id="C02-1122.7">Analysis</entity></title>
<abstract>
This <entity id="C02-1122.8">paper</entity> <entity id="C02-1122.9">proposes</entity> a <entity id="C02-1122.10">method</entity> of fertilizing a <entity id="C02-1122.11">Japanese</entity> <entity id="C02-1122.12">case</entity> <entity id="C02-1122.13">frame</entity> <entity id="C02-1122.14">dictionary</entity> to handle complicated <entity id="C02-1122.15">expressions</entity> : double nominative <entity id="C02-1122.16">sentences</entity> , non-gapping <entity id="C02-1122.17">relation</entity> of <entity id="C02-1122.18">relative</entity> <entity id="C02-1122.19">clauses</entity> , and <entity id="C02-1122.20">case</entity> change. Our <entity id="C02-1122.21">method</entity> is divided into two stages. In the first stage, we <entity id="C02-1122.22">parse</entity> a large <entity id="C02-1122.23">corpus</entity> and <entity id="C02-1122.24">construct</entity> a <entity id="C02-1122.25">Japanese</entity> <entity id="C02-1122.26">case</entity> <entity id="C02-1122.27">frame</entity> <entity id="C02-1122.28">dictionary</entity> automatically from the <entity id="C02-1122.29">parse</entity> <entity id="C02-1122.30">results</entity> . In the second stage, we <entity id="C02-1122.31">apply</entity> <entity id="C02-1122.32">case</entity> <entity id="C02-1122.33">analysis</entity> to the large <entity id="C02-1122.34">corpus</entity> utilizing the <entity id="C02-1122.35">constructed</entity> <entity id="C02-1122.36">case</entity> <entity id="C02-1122.37">frame</entity> <entity id="C02-1122.38">dictionary</entity> , and upgrade the <entity id="C02-1122.39">case</entity> <entity id="C02-1122.40">frame</entity> <entity id="C02-1122.41">dictionary</entity> by incorporating newly acquired <entity id="C02-1122.42">information</entity> .
</abstract>


</text>

<text id="C02-1169">
<title>
Open- <entity id="C02-1169.1">Domain</entity> Voice-Activated <entity id="C02-1169.2">Question Answering</entity></title>
<abstract>
Voice-Activated <entity id="C02-1169.3">Question Answering</entity> (VAQA) <entity id="C02-1169.4">systems</entity> represent the next <entity id="C02-1169.5">generation</entity> <entity id="C02-1169.6">capability</entity> for universal <entity id="C02-1169.7">access</entity> by integrating state-of-the-art in <entity id="C02-1169.8">question</entity> answering Q&amp;A and <entity id="C02-1169.9">automatic speech recognition</entity> (ASR) in such a way that the <entity id="C02-1169.10">performance</entity> of the combined <entity id="C02-1169.11">system</entity> is better than the <entity id="C02-1169.12">individual</entity> <entity id="C02-1169.13">components</entity> . This <entity id="C02-1169.14">paper</entity> presents an <entity id="C02-1169.15">implemented</entity> VAQA <entity id="C02-1169.16">system</entity> and describes the <entity id="C02-1169.17">techniques</entity> that enable the terative <entity id="C02-1169.18">refinement</entity> of both Q&amp;A and ASR. The <entity id="C02-1169.19">results</entity> of our <entity id="C02-1169.20">experiments</entity> show that spoken <entity id="C02-1169.21">questions</entity> can be <entity id="C02-1169.22">processed</entity> with surprising <entity id="C02-1169.23">accuracy</entity> when using our VAQA <entity id="C02-1169.24">implementation</entity> .
</abstract>


</text>

<text id="C04-1048">
<title><entity id="C04-1048.1">Generating</entity> <entity id="C04-1048.2">Discourse</entity> <entity id="C04-1048.3">Structures</entity> For Written <entity id="C04-1048.4">Text</entity></title>
<abstract>
This <entity id="C04-1048.5">paper</entity> presents a <entity id="C04-1048.6">system</entity> for automatically <entity id="C04-1048.7">generating</entity> <entity id="C04-1048.8">discourse structures</entity> from written <entity id="C04-1048.9">text</entity> . The <entity id="C04-1048.10">system</entity> is divided into two <entity id="C04-1048.11">levels</entity> : <entity id="C04-1048.12">sentence-level</entity> and <entity id="C04-1048.13">text-level</entity> . The <entity id="C04-1048.14">sentence-level</entity> <entity id="C04-1048.15">discourse</entity> <entity id="C04-1048.16">parser</entity> uses <entity id="C04-1048.17">syntactic information</entity> and <entity id="C04-1048.18">cue</entity> <entity id="C04-1048.19">phrases</entity> to <entity id="C04-1048.20">segment</entity> <entity id="C04-1048.21">sentences</entity> into elementary <entity id="C04-1048.22">discourse</entity> <entity id="C04-1048.23">units</entity> and to <entity id="C04-1048.24">generate</entity> <entity id="C04-1048.25">discourse structures</entity> of <entity id="C04-1048.26">sentences</entity> . At the <entity id="C04-1048.27">text-level</entity> , <entity id="C04-1048.28">constraints</entity> about textual adjacency and textual <entity id="C04-1048.29">organization</entity> are integrated in a <entity id="C04-1048.30">beam search</entity> in <entity id="C04-1048.31">order</entity> to <entity id="C04-1048.32">generate</entity> best <entity id="C04-1048.33">discourse structures</entity> . The <entity id="C04-1048.34">experiments</entity> were done with <entity id="C04-1048.35">documents</entity> from the RST <entity id="C04-1048.36">Discourse</entity> Treebank. It shows promising <entity id="C04-1048.37">results</entity> in a reasonable <entity id="C04-1048.38">search space</entity> compared to the <entity id="C04-1048.39">discourse</entity> <entity id="C04-1048.40">trees</entity> <entity id="C04-1048.41">generated</entity> by human <entity id="C04-1048.42">analysts</entity> .
</abstract>


</text>

<text id="C04-1050">
<title>
Improving <entity id="C04-1050.1">Japanese</entity> Zero Pronoun <entity id="C04-1050.2">Resolution</entity> By Global <entity id="C04-1050.3">Word Sense Disambiguation</entity></title>
<abstract>
This <entity id="C04-1050.4">paper</entity> <entity id="C04-1050.5">proposes</entity> unsupervised <entity id="C04-1050.6">word sense disambiguation</entity> <entity id="C04-1050.7">based</entity> on automatically <entity id="C04-1050.8">constructed</entity> <entity id="C04-1050.9">case</entity> <entity id="C04-1050.10">frames</entity> and its incorporation into our zero pronoun <entity id="C04-1050.11">resolution</entity> <entity id="C04-1050.12">system</entity> . The <entity id="C04-1050.13">word <entity id="C04-1050.14">sense disambiguation</entity></entity> is <entity id="C04-1050.15">applied</entity> to <entity id="C04-1050.16">verbs</entity> and <entity id="C04-1050.17">nouns</entity> . We consider that <entity id="C04-1050.18">case</entity> <entity id="C04-1050.19">frames</entity> define <entity id="C04-1050.20">verb</entity> senses and <entity id="C04-1050.21">semantic features</entity> in a <entity id="C04-1050.22">thesaurus</entity> define <entity id="C04-1050.23">noun</entity> senses, respectively, and <entity id="C04-1050.24">perform</entity> <entity id="C04-1050.25">sense disambiguation</entity> by selecting them <entity id="C04-1050.26">based</entity> on <entity id="C04-1050.27">case</entity> <entity id="C04-1050.28">analysis</entity> . In <entity id="C04-1050.29">addition</entity> , according to the one <entity id="C04-1050.30">sense</entity> per <entity id="C04-1050.31">discourse</entity> heuristic, the <entity id="C04-1050.32">word sense disambiguation</entity> <entity id="C04-1050.33">results</entity> are cached and <entity id="C04-1050.34">applied</entity> globally to the subsequent <entity id="C04-1050.35">words</entity> . We integrated this global <entity id="C04-1050.36">word sense disambiguation</entity> into our zero pronoun <entity id="C04-1050.37">resolution</entity> <entity id="C04-1050.38">system</entity> , and conducted <entity id="C04-1050.39">experiments</entity> of zero pronoun <entity id="C04-1050.40">resolution</entity> on two different <entity id="C04-1050.41">domain</entity> <entity id="C04-1050.42">corpora</entity> . Both of the <entity id="C04-1050.43">experimental</entity> <entity id="C04-1050.44">results</entity> indicated the <entity id="C04-1050.45">effectiveness</entity> of our <entity id="C04-1050.46">approach</entity> .
</abstract>


</text>

<text id="C04-1134">
<title>
BiFrameNet: Bilingual <entity id="C04-1134.1">Frame</entity> <entity id="C04-1134.2">Semantics</entity> <entity id="C04-1134.3">Resource</entity> <entity id="C04-1134.4">Construction</entity> By <entity id="C04-1134.5">Cross-</entity> <entity id="C04-1134.6">Lingual</entity> <entity id="C04-1134.7">Induction</entity></title>
<abstract>
"We present a novel <entity id="C04-1134.8">automatic</entity> <entity id="C04-1134.9">approach</entity> to <entity id="C04-1134.10">constructing</entity> a bilingual <entity id="C04-1134.11">semantic network</entity>
the BiFrameNet, to enhance <entity id="C04-1134.12">statistical</entity> and <entity id="C04-1134.13">transfer-based</entity> <entity id="C04-1134.14">machine translation systems</entity> . BiFrameNet is a <entity id="C04-1134.15">frame</entity> <entity id="C04-1134.16">semantic representation</entity> , and contains <entity id="C04-1134.17">semantic structure</entity> <entity id="C04-1134.18">transfers</entity> between <entity id="C04-1134.19">English</entity> and <entity id="C04-1134.20">Chinese</entity> . The <entity id="C04-1134.21">English</entity> FrameNet and the <entity id="C04-1134.22">Chinese</entity> HowNet <entity id="C04-1134.23">provide</entity> us with two different views of the <entity id="C04-1134.24">semantic</entity> <entity id="C04-1134.25">distribution</entity> of <entity id="C04-1134.26">lexicon</entity> by <entity id="C04-1134.27">linguists</entity> . We <entity id="C04-1134.28">propose</entity> to induce the <entity id="C04-1134.29">mapping</entity> between the <entity id="C04-1134.30">English</entity> <entity id="C04-1134.31">lexical entries</entity> in FrameNet to <entity id="C04-1134.32">Chinese</entity> <entity id="C04-1134.33">word</entity> senses in HowNet, furnishing a bilingual <entity id="C04-1134.34">semantic lexicon</entity> which simulates the ""<entity id="C04-1134.35">concept</entity> <entity id="C04-1134.36">lexicon</entity> "" supposedly used by human <entity id="C04-1134.37">translators</entity> , and which can thus be beneficial to <entity id="C04-1134.38">machine translation systems</entity> . BiFrameNet also contains bilingual <entity id="C04-1134.39">example</entity> <entity id="C04-1134.40">sentences</entity> that have the same <entity id="C04-1134.41">semantic roles</entity> . We automatically induce <entity id="C04-1134.42">Chinese</entity> <entity id="C04-1134.43">example</entity> <entity id="C04-1134.44">sentences</entity> and their <entity id="C04-1134.45">semantic roles</entity> , <entity id="C04-1134.46">based</entity> on <entity id="C04-1134.47">semantic <entity id="C04-1134.48">structure</entity></entity> <entity id="C04-1134.49">alignment</entity> from the first stage of our work, as well as shallow <entity id="C04-1134.50">syntactic <entity id="C04-1134.51">structure</entity></entity> . In <entity id="C04-1134.52">addition</entity> to its <entity id="C04-1134.53">utility</entity> for <entity id="C04-1134.54">machine-aided</entity> and <entity id="C04-1134.55">machine translations</entity> , our work is also related to the spatial <entity id="C04-1134.56">models</entity> <entity id="C04-1134.57">proposed</entity> by cognitive <entity id="C04-1134.58">scientists</entity> in the <entity id="C04-1134.59">framework</entity> of artifactual <entity id="C04-1134.60">simulations</entity> of the <entity id="C04-1134.61">translation process</entity> . "
</abstract>


</text>

<text id="E85-1027">
<title>
A <entity id="E85-1027.1">Computational</entity> <entity id="E85-1027.2">Theory</entity> Of Prose Style For <entity id="E85-1027.3">Natural <entity id="E85-1027.4">Language Generation</entity></entity></title>
<abstract>
"1. Where in the <entity id="E85-1027.5">generation process</entity> style is taken into account. 2. How a particular prose style is represented; what ""stylistic <entity id="E85-1027.6">rules</entity> "" look like; 3. What <entity id="E85-1027.7">modifications</entity> to a <entity id="E85-1027.8">generation</entity> <entity id="E85-1027.9">algorithm</entity> are needed; what the <entity id="E85-1027.10">decision</entity> is that <entity id="E85-1027.11">evaluates</entity> stylistic <entity id="E85-1027.12">alternatives</entity> ; 4. What elaborations to the normal <entity id="E85-1027.13">description</entity> of <entity id="E85-1027.14">surface structure</entity> are necessary to make it usable as a plan for the <entity id="E85-1027.15">text</entity> and a <entity id="E85-1027.16">reference</entity> for these <entity id="E85-1027.17">decisions</entity> ; 5. What <entity id="E85-1027.18">kinds</entity> of <entity id="E85-1027.19">information</entity> <entity id="E85-1027.20">decisions</entity> about style have <entity id="E85-1027.21">access</entity> to. Our <entity id="E85-1027.22">theory</entity> emerged out of <entity id="E85-1027.23">design</entity> <entity id="E85-1027.24">experiments</entity> we have made over the past year with our <entity id="E85-1027.25">natural language generation system</entity> , the Zetalisp <entity id="E85-1027.26">program</entity> MUMBLE. In the <entity id="E85-1027.27">process</entity> we have extended MUMBLE through the <entity id="E85-1027.28">addition</entity> of an additional <entity id="E85-1027.29">process</entity> that now mediates between <entity id="E85-1027.30">content</entity> planning and linguistic <entity id="E85-1027.31">realization</entity> . This new <entity id="E85-1027.32">process</entity> , which we <entity id="E85-1027.33">call</entity> ""<entity id="E85-1027.34">attachment</entity> "", <entity id="E85-1027.35">provides</entity> the further significant <entity id="E85-1027.36">benefit</entity> that <entity id="E85-1027.37">text structure</entity> is no longer dictated by the <entity id="E85-1027.38">structure</entity> of the <entity id="E85-1027.39">message</entity> : the sequential <entity id="E85-1027.40">order</entity> and dominance <entity id="E85-1027.41">relationships</entity> of <entity id="E85-1027.42">concepts</entity> in the <entity id="E85-1027.43">message</entity> no longer force one <entity id="E85-1027.44">form</entity> onto the <entity id="E85-1027.45">words</entity> and <entity id="E85-1027.46">phrases</entity> in the <entity id="E85-1027.47">text</entity> . Instead, rhetorical and intentional directives can be interpreted flexibly in the <entity id="E85-1027.48">context</entity> of the ongoing <entity id="E85-1027.49">discourse</entity> and stylistic <entity id="E85-1027.50">preferences</entity> . The <entity id="E85-1027.51">text</entity> is built up through composition under the <entity id="E85-1027.52">direction</entity> of linguistic organizing <entity id="E85-1027.53">principles</entity> , rather than having to follow conceptual <entity id="E85-1027.54">principles</entity> in lockstep. We will begin by describing what we mean by prose style and then introducing the <entity id="E85-1027.55">generation</entity> <entity id="E85-1027.56">task</entity> that lead us to this <entity id="E85-1027.57">theory</entity> , the reproduction of short <entity id="E85-1027.58">encyclopedia</entity> articles on African tribes. We will then use that <entity id="E85-1027.59">task</entity> to <entity id="E85-1027.60">outline</entity> the <entity id="E85-1027.61">parts</entity> of our <entity id="E85-1027.62">theory</entity> and the <entity id="E85-1027.63">operations</entity> of the <entity id="E85-1027.64">attachment</entity> <entity id="E85-1027.65">process</entity> . Finally we will compare our <entity id="E85-1027.66">techniques</entity> to the related work of Davey , McKeown and Derr , and Gabriel , and consider some of the possible psycholinguistic <entity id="E85-1027.67">hypotheses</entity> that it may lead to. "
</abstract>


</text>

<text id="E91-1031">
<title><entity id="E91-1031.1">Prediction</entity> In Chart <entity id="E91-1031.2">Parsing</entity> <entity id="E91-1031.3">Algorithms</entity> For Categorial <entity id="E91-1031.4">Unification</entity> Grammar
</title>
<abstract><entity id="E91-1031.5">Natural language systems</entity> <entity id="E91-1031.6">based</entity> on Categorial <entity id="E91-1031.7">Unification</entity> Grammar (CUG) have mainly employed bottom-up <entity id="E91-1031.8">parsing</entity> <entity id="E91-1031.9">algorithms</entity> for <entity id="E91-1031.10">processing</entity> . Conventional <entity id="E91-1031.11">prediction</entity> <entity id="E91-1031.12">techniques</entity> to <entity id="E91-1031.13">improve</entity> the <entity id="E91-1031.14">efficiency</entity> of the <entity id="E91-1031.15">parsing</entity> <entity id="E91-1031.16">process</entity> , appear to fall short when <entity id="E91-1031.17">parsing</entity> CUG. Nevertheless, <entity id="E91-1031.18">prediction</entity> seems necessary when <entity id="E91-1031.19">parsing</entity> grammars with highly ambiguous <entity id="E91-1031.20">lexicons</entity> or with non-canonical categorial <entity id="E91-1031.21">rules</entity> . In this <entity id="E91-1031.22">paper</entity> we present a lexicalist <entity id="E91-1031.23">prediction</entity> <entity id="E91-1031.24">technique</entity> for CUG and show that this may lead to considerable <entity id="E91-1031.25">gains</entity> in <entity id="E91-1031.26">efficiency</entity> for both bottom-up and top-down <entity id="E91-1031.27">parsing</entity> .
</abstract>


</text>

</doc>

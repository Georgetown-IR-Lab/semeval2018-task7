<?xml version="1.0" encoding="UTF-8"?>
<doc>
<text id="C08-1105">
<title> Prediction of Maximal Projection for Semantic Role Labeling</title>
<abstract>
In <entity id="C08-1105.1">Semantic Role Labeling (SRL)</entity>, <entity id="C08-1105.2">arguments</entity> are usually limited in a <entity id="C08-1105.3">syntax subtree</entity>. It is reasonable to label <entity id="C08-1105.4">arguments</entity> locally in such a <entity id="C08-1105.5">sub-tree</entity> rather than a <entity id="C08-1105.6">whole tree</entity>. To identify <entity id="C08-1105.7">active region arguments</entity>, this paper models <entity id="C08-1105.8">Maximal Projection (MP)</entity>, which is a concept in <entity id="C08-1105.9">D-structure</entity> from the <entity id="C08-1105.10">projection principle</entity> of the <entity id="C08-1105.11">Principle and Parameters theory</entity>.This paper makes a new definition of <entity id="C08-1105.12">MP in S-structure</entity> and proposes two methods to predict it: the <entity id="C08-1105.13">anchor group approach</entity> and the <entity id="C08-1105.14">single anchor approach</entity>. The <entity id="C08-1105.15">anchor group approach</entity> achieves an <entity id="C08-1105.16">accuracy</entity> of 87.75% and the <entity id="C08-1105.17">single anchor approach</entity> achieves 83.63%. Experimental results also indicate that the <entity id="C08-1105.18">prediction of MP</entity> improves <entity id="C08-1105.19">semantic role labeling</entity>.
</abstract>

</text>

<text id="L08-1579">
<title>Generating Bilingual Dictionaries by Transitivity</title>
<abstract>
Recently the LATL has undertaken the development of a <entity id="L08-1579.1">multilingual translation system</entity> based on a <entity id="L08-1579.2">symbolic parsing technology</entity> and on a <entity id="L08-1579.3">transfer-based translation model</entity>. A crucial component of the system is the <entity id="L08-1579.4">lexical database</entity>, notably the <entity id="L08-1579.5">bilingual dictionaries</entity> containing the information for the <entity id="L08-1579.6">lexical transfer</entity> from one <entity id="L08-1579.7">language</entity> to another. As the number of necessary <entity id="L08-1579.8">bilingual dictionaries</entity> is a quadratic function of the number of <entity id="L08-1579.9">languages</entity> considered, we will face the problem of getting a large number of dictionaries. In this paper we discuss a solution to derive a <entity id="L08-1579.10">bilingual dictionary</entity> by <entity id="L08-1579.11">transitivity</entity> using existing ones and to check the generated <entity id="L08-1579.12">translations</entity> in a <entity id="L08-1579.13">parallel corpus</entity>. Our first experiments concerns the generation of two <entity id="L08-1579.14">bilingual dictionaries</entity> and the quality of the entries are very promising. The number of generated entries could however be improved and we conclude the paper with the possible ways we plan to explore.
</abstract>

</text>

<text id="E99-1028">
<title>Word Sense Disambiguation In Untagged Text Based On Term Weight Learning</title>
<abstract>
This paper describes unsupervised learning algorithm for disambiguating <entity id="E99-1028.1">verbal word senses</entity> using <entity id="E99-1028.2">term weight learning</entity>. In our method, <entity id="E99-1028.3">collocations</entity> which characterise every <entity id="E99-1028.4">sense</entity> are extracted using <entity id="E99-1028.5">similarity-based estimation</entity>. For the results, <entity id="E99-1028.6">term weight learning</entity> is performed. Parameters of <entity id="E99-1028.7">term weighting</entity> are then estimated so as to maximise the <entity id="E99-1028.8">collocations</entity> which characterise every <entity id="E99-1028.9">sense</entity> and minimise the other <entity id="E99-1028.10">collocations</entity>. The results of experiment demonstrate the effectiveness of the method.
</abstract>

</text>

<text id="N04-4037">
<title>A Lightweight Semantic Chunker Based On Tagging</title>
<abstract>
In this paper, a framework for the development of a fast, accurate, and highly portable <entity id="N04-4037.1">semantic chunker</entity> is introduced. The framework is based on a non-overlapping, <entity id="N04-4037.2">shallow tree-structured language</entity>. The derivation of the tree is considered as a sequence of <entity id="N04-4037.3">tagging actions</entity> in a predefined <entity id="N04-4037.4">linguistic context</entity>, and a novel <entity id="N04-4037.5">semantic chunker</entity> is accordingly developed. It groups the <entity id="N04-4037.6">phrase chunks</entity> into the arguments of a given <entity id="N04-4037.7">predicate</entity> in a bottom-up fashion. This is quite different from current approaches to <entity id="N04-4037.8">semantic parsing</entity> or <entity id="N04-4037.9">chunking</entity> that depend on full <entity id="N04-4037.10">statistical syntactic parsers</entity> that require <entity id="N04-4037.11">tree bank</entity> style annotation. We compare it with a recently proposed <entity id="N04-4037.12">word-by-word semantic chunker</entity> and present results that show that the <entity id="N04-4037.13">phrase-by-phrase approach</entity> performs better than its <entity id="N04-4037.14">word-by-word counterpart</entity>.
</abstract>

</text>

<text id="E95-1003">
<title>Criteria For Measuring Term Recognition</title>
<abstract>
This paper qualifies what a true <entity id="E95-1003.1">term-recognition</entity> systems would have to recognize. The exact bracketing of the maximal <entity id="E95-1003.2">termform</entity> is then proposed as an achieveable goal upon which current system performance should be measured. How recall and precision metrics are best adapted for measuring <entity id="E95-1003.3">term recognition</entity> is suggested.
</abstract>

</text>

<text id="H91-1080">
<title>Research In Continuous Speech Recognition</title>
<abstract>
The primary objective of this basic research is to develop improved methods and models for <entity id="H91-1080.1">acoustic recognition</entity> of <entity id="H91-1080.2">continuous speech</entity>. The work has focussed on developing accurate and detailed mathematical models of <entity id="H91-1080.3">phonemes</entity> and their coarticulation for the purpose of <entity id="H91-1080.4">large-vocabulary continuous speech recognition</entity>. Important goals of this work are to achieve the highest possible <entity id="H91-1080.5">word recognition</entity> accuracy in <entity id="H91-1080.6">continuous speech</entity> and to develop methods for the rapid adaptation of <entity id="H91-1080.7">phonetic models</entity> to the voice of a new <entity id="H91-1080.8">speaker</entity>.
</abstract>

</text>

<text id="H92-1124">
<title>In-Depth Knowledge-Based Machine Translation</title>
<abstract>
The development of an integrated <entity id="H92-1124.1">knowledge-based machine-aided translation system</entity> called <entity id="H92-1124.2">Pangloss</entity> in collaboration with the Center for Machine Translation (CMT) at CMU and the Computing Research Laboratory (CRL) at New Mexico State University. The ISI part of the collaboration is focused initially on providing the system's output capabilities, primarily in <entity id="H92-1124.3">English</entity> and then in other <entity id="H92-1124.4">languages</entity>, including (some of) <entity id="H92-1124.5">German</entity>, <entity id="H92-1124.6">Chinese</entity>, and <entity id="H92-1124.7">Japanese</entity>. Additional tasks are the maintenance and continued distribution of the <entity id="H92-1124.8">Penman</entity> <entity id="H92-1124.9">sentence generator</entity> and <entity id="H92-1124.10">text planner</entity> and the development of ancillary <entity id="H92-1124.11">knowledge sources</entity> and software.
</abstract>

</text>

<text id="W94-0103">
<title>AMALGAM: Automatic Mapping Among Lexico-Grammatical Annotation Models</title>
<abstract>
The title of this paper playfully contrasts two rather different approaches to <entity id="W94-0103.1">language analysis</entity>. The "<entity id="W94-0103.2">Noisy Channel</entity>" 's are the promoters of statistically based approaches to <entity id="W94-0103.3">language learning</entity>. Many of these studies are based on the Shannons's <entity id="W94-0103.4">Noisy Channel model</entity>. The "<entity id="W94-0103.5">Braying Donkey</entity>" 's are those oriented towards <entity id="W94-0103.6">theoretically motivated language models</entity>. They are interested in any type of <entity id="W94-0103.7">language expressions</entity> (such as the famous "<entity id="W94-0103.8">Donkey Sentences</entity>"), regardless of their frequency in real <entity id="W94-0103.9">language</entity>, because the focus is the study of <entity id="W94-0103.10">human communication</entity>. In the past few years, we supported a more balanced approach. While our major concern is applicability to real <entity id="W94-0103.11">NLP systems</entity>, we think that, after all, quantitative methods in <entity id="W94-0103.12">Computational Linguistic</entity> should provide not only practical tools for <entity id="W94-0103.13">language processing</entity>, but also some linguistic insight. Since, for sake of space, in this paper we cannot give any complete account of our research, we will present examples of "linguistically appealing", automatically acquired, <entity id="W94-0103.14">lexical data</entity> (selectional restrictions of words) obtained trough an integrated use of knowledge-based and statistical techniques. We discuss the pros and cons of adding <entity id="W94-0103.15">symbolic knowledge</entity> to the <entity id="W94-0103.16">corpus</entity> linguistic recipe.
</abstract>

</text>

<text id="W00-0902">
<title>Comparing Corpora With WordSmith Tools: How Large Must The Reference Corpus Be?</title>
<abstract>
<entity id="W00-0902.1">WordSmith Tools</entity> (Scott, 1998) offers a program for comparing <entity id="W00-0902.2">corpora</entity>, known as <entity id="W00-0902.3">Keywords</entity>. <entity id="W00-0902.4">KeyWords</entity> compares a <entity id="W00-0902.5">word list</entity> extracted from what has been called 'the study <entity id="W00-0902.6">corpus</entity>' (the <entity id="W00-0902.7">corpus</entity> which the researcher is interested in describing) with a <entity id="W00-0902.8">word list</entity> made from a <entity id="W00-0902.9">reference corpus</entity>. The only requirement for a <entity id="W00-0902.10">word list</entity> to be accepted as <entity id="W00-0902.11">reference corpus</entity> by the software is that must be larger than the study <entity id="W00-0902.12">corpus</entity>, one of the most pressing questions with respect to using <entity id="W00-0902.13">KeyWords</entity> seems to be what would be the ideal size of a <entity id="W00-0902.14">reference corpus</entity>. The aim of this paper is thus to propose answers to this question. Five <entity id="W00-0902.15">English corpora</entity> were compared to <entity id="W00-0902.16">reference corpora</entity> of various sizes (varying from two to 100 times larger than the study <entity id="W00-0902.17">corpus</entity>). The results indicate that a <entity id="W00-0902.18">reference corpus</entity> that is five times as large as the study <entity id="W00-0902.19">corpus</entity> yielded a larger number of <entity id="W00-0902.20">keywords</entity> than a smaller <entity id="W00-0902.21">reference corpus</entity>. <entity id="W00-0902.22">Corpora</entity> larger than five times the size of the <entity id="W00-0902.23">study corpus</entity> yielded similar amounts of <entity id="W00-0902.24">keywords</entity>. The implication is that a larger <entity id="W00-0902.25">reference corpus</entity> is not always better than a smaller one, for <entity id="W00-0902.26">WordSmith Tools</entity> <entity id="W00-0902.27">Keywords analysis</entity>, while a <entity id="W00-0902.28">reference corpus</entity> that is less than five times the size of the <entity id="W00-0902.29">study corpus</entity> may not be reliable. There seems to be no need for using extremely large <entity id="W00-0902.30">reference corpora</entity>, given that the number of <entity id="W00-0902.31">keywords</entity> yielded do not seem to change by using <entity id="W00-0902.32">corpora</entity> larger than five times the size of the <entity id="W00-0902.33">study corpus</entity>.
</abstract>

</text>

<text id="W02-1501">
<title>Grammar And Lexicon In The Robust Parsing Of Italian Towards A Non-Nave Interplay</title>
<abstract>
In the paper we report a qualitative evaluation of the performance of a <entity id="W02-1501.1">dependency analyser</entity> of <entity id="W02-1501.2">Italian</entity> that runs in both a <entity id="W02-1501.3">non-lexicalised</entity> and a <entity id="W02-1501.4">lexicalised</entity> mode. Results shed light on the contribution of types of <entity id="W02-1501.5">lexical information</entity> to <entity id="W02-1501.6">parsing</entity>.
</abstract>

</text>

<text id="W99-0619">
<title>Word Informativeness And Automatic Pitch Accent Modeling</title>
<abstract>
In <entity id="W99-0619.1">international phonology</entity> and <entity id="W99-0619.2">speech synthesis</entity> research, it has been suggested that the <entity id="W99-0619.3">relative informativeness</entity> of a <entity id="W99-0619.4">word</entity> can be used to predict <entity id="W99-0619.5">pitch prominence</entity>. The more information conveyed by a <entity id="W99-0619.6">word</entity>, the more likely it will be accented. But there are others who express doubts about such a correlation. In this paper, we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted <entity id="W99-0619.7">measures</entity> of <entity id="W99-0619.8">informativeness</entity>. Our experiments show that there is a positive correlation between the <entity id="W99-0619.9">informativeness</entity> of a <entity id="W99-0619.10">word</entity> and its <entity id="W99-0619.11">pitch accent assignment</entity>. They also show that <entity id="W99-0619.12">informativeness</entity> enables statistically significant improvements in <entity id="W99-0619.13">pitch accent prediction</entity>. The computation of <entity id="W99-0619.14">word informativeness</entity> is inexpensive and can be incorporated into <entity id="W99-0619.15">speech synthesis systems</entity> easily.
</abstract>

</text>

<text id="J87-1004">
<title>An Efficient Augmented-Context-Free Parsing Algorithm</title>
<abstract>
"An efficient <entity id="J87-1004.1">parsing algorithm</entity> for <entity id="J87-1004.2">augmented context-free grammars</entity> is introduced, and its application to <entity id="J87-1004.3">on-line natural language interfaces</entity> discussed. The algorithm is a <entity id="J87-1004.4">generalized LR parsing algorithm</entity>, which precomputes an <entity id="J87-1004.5">LR shift-reduce parsing table</entity> (possibly with multiple entries) from a given <entity id="J87-1004.6">augmented context-free grammar</entity>. Unlike the standard <entity id="J87-1004.7">LR parsing algorithm</entity>, it can handle arbitrary <entity id="J87-1004.8">context-free grammars</entity>, including <entity id="J87-1004.9">ambiguous grammars</entity>, while most of the <entity id="J87-1004.10">LR</entity> efficiency is preserved by introducing the concept of a "graph-structured stack". The graph-structured stack allows an <entity id="J87-1004.11">LR shift-reduce parser</entity> to maintain <entity id="J87-1004.12">multiple parses</entity> without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended <entity id="J87-1004.13">chart parsing algorithm</entity> efficiently guided by <entity id="J87-1004.14">LR parsing tables</entity>. The algorithm is fast, due to the <entity id="J87-1004.15">LR table</entity> precomputation. In several experiments with different <entity id="J87-1004.16">English grammars and sentences</entity>, timings indicate a fiveto tenfold speed advantage over Earley's <entity id="J87-1004.17">context-free parsing algorithm</entity>. The algorithm parses a <entity id="J87-1004.18">sentence</entity> strictly from left to right that is, it starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical <entity id="J87-1004.19">on-line parser</entity> based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP AI workstations. The parser is used in the <entity id="J87-1004.20">multi-lingual machine translation</entity> project at CMU. Also, a commercial <entity id="J87-1004.21">on-line parser</entity> for <entity id="J87-1004.22">Japanese language</entity> is being built by Intelligent Technology Incorporation, based on the technique developed at CMU."
</abstract>

</text>

<text id="W04-0859">
<title>The University Of Alicante Systems At Senseval-3</title>
<abstract>
The DLSI-UA team is currently working on several <entity id="W04-0859.1">word sense disambiguation</entity> approaches, both supervised and unsupervised. These approaches are based on different ways to use both <entity id="W04-0859.2">annotated and unannotated data</entity>, and several resources generated from or exploiting <entity id="W04-0859.3">WordNet</entity> (Miller et al., 1993), <entity id="W04-0859.4">WordNet Domains</entity>, <entity id="W04-0859.5">EuroWordNet (EWN)</entity> and additional <entity id="W04-0859.6">corpora</entity>. This paper presents a view of different system results for <entity id="W04-0859.7">Word Sense Disambiguation</entity> in different tasks of SENSEVAL-3.
</abstract>

</text>

<text id="W04-3002">
<title>Hybrid Statistical And Structural Semantic Modeling For Thai Multi-Stage Spoken Language Understanding</title>
<abstract>
This article proposes a <entity id="W04-3002.1">hybrid statistical and structural semantic model</entity> for <entity id="W04-3002.2">multi-stage spoken language understanding (SLU)</entity>. The first stage of this <entity id="W04-3002.3">SLU</entity> utilizes a <entity id="W04-3002.4">weighted finite-state transducer (WFST)-based parser</entity>, which encodes the <entity id="W04-3002.5">regular grammar</entity> of concepts to be extracted. The proposed method improves the <entity id="W04-3002.6">regular grammar</entity> model by incorporating a well-known <entity id="W04-3002.7">n-gram semantic tagger</entity>. This hybrid model thus enhances the <entity id="W04-3002.8">syntax</entity> of <entity id="W04-3002.9">n-gram</entity> outputs while providing robustness against <entity id="W04-3002.10">speech-recognition errors</entity>. With applications to a <entity id="W04-3002.11">Thai</entity> hotel reservation domain, it is shown to outperform both individual models at every stage of the <entity id="W04-3002.12">SLU</entity> system. Under the probabilistic <entity id="W04-3002.13">WFST</entity> framework, the use of TV-best hypotheses from the <entity id="W04-3002.14">speech recognizer</entity> instead of the 1-best can further improve performance requiring only a small additional processing time.
</abstract>

</text>

<text id="W06-1637">
<title>Priming Effects In Combinatory Categorial Grammar</title>
<abstract>
This paper presents a <entity id="W06-1637.1">corpus-based</entity> account of <entity id="W06-1637.2">structural priming</entity> in human <entity id="W06-1637.3">sentence processing</entity>, focusing on the role that <entity id="W06-1637.4">syntactic representations</entity> play in such an account. We estimate the strength of <entity id="W06-1637.5">structural priming effects</entity> from a <entity id="W06-1637.6">corpus</entity> of <entity id="W06-1637.7">spontaneous spoken dialogue</entity>, annotated syntactically with <entity id="W06-1637.8">Combinatory Categorial Grammar (CCG)</entity> derivations. This methodology allows us to test a range of predictions that <entity id="W06-1637.9">CCG</entity> makes about <entity id="W06-1637.10">priming</entity>. In particular, we present evidence for <entity id="W06-1637.11">priming</entity> between <entity id="W06-1637.12">lexical and syntactic categories</entity> encoding partially satisfied sub-categorization frames, and we show that <entity id="W06-1637.13">priming effects</entity> exist both for <entity id="W06-1637.14">incremental and normal-form CCG derivations</entity>.
</abstract>

</text>

<text id="W07-1014">
<title>From indexing the biomedical literature to coding clinical text: experience with MTI and machine learning approaches</title>
<abstract>
This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in <entity id="W07-1014.1">information retrieval</entity> and classification of <entity id="W07-1014.2">medical literature</entity>, to a new task of assigning <entity id="W07-1014.3">ICD-9-CM codes</entity> to the <entity id="W07-1014.4">clinical history</entity> and impression sections of radiology <entity id="W07-1014.5">reports</entity>. The basic methods used are: a modification of the <entity id="W07-1014.6">NLM Medical Text Indexer</entity> system, SVM, k-NN and a simple pattern-matching method. The basic methods are combined using a variant of stacking. Evaluated in the context of a <entity id="W07-1014.7">Medical NLP</entity> Challenge, fusion produced an F-score of 0.85 on the Challenge test set, which is considerably above the mean Challenge F-score of 0.77 for 44 participating groups.
</abstract>

</text>

<text id="P07-1069">
<title>Generating a Table-of-Contents</title>
<abstract>
This paper presents a method for the automatic generation of a <entity id="P07-1069.1">table-of-contents</entity>. This type of <entity id="P07-1069.2">summary</entity> could serve as an effective navigation tool for accessing <entity id="P07-1069.3">information</entity> in long <entity id="P07-1069.4">texts</entity>, such as <entity id="P07-1069.5">books</entity>. To generate a coherent <entity id="P07-1069.6">table-of-contents</entity>, we need to capture both <entity id="P07-1069.7">global dependencies</entity> across different <entity id="P07-1069.8">titles</entity> in the table and local constraints within sections. Our algorithm effectively handles these <entity id="P07-1069.9">complex dependencies</entity> by factoring the model into local and global components, and incrementally constructing the model's output. The results of automatic evaluation and manual assessment confirm the benefits of this design: our system is consistently ranked higher than non-hierarchical baselines.
</abstract>

</text>

<text id="P08-1055">
<title>Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation</title>
<abstract>
Despite its long history, and a great deal of research producing many useful algorithms and observations, research in <entity id="P08-1055.1">cooperative response generation</entity> has had little impact on the recent commercialization of <entity id="P08-1055.2">dialogue technologies</entity>, particularly within the <entity id="P08-1055.3">spoken dialogue</entity> community. We hypothesize that a particular type of cooperative response, <entity id="P08-1055.4">intensional summaries</entity>, are
effective for when users are unfamiliar with the domain. We evaluate this hypothesis with two experiments with <entity id="P08-1055.5">cruiser</entity>, a <entity id="P08-1055.6">DS</entity> for in-car or mobile users to access restaurant information. First, we compare <entity id="P08-1055.7">cruiser</entity> with a baseline <entity id="P08-1055.8">system-initiative DS</entity>, and show that users prefer <entity id="P08-1055.9">cruiser</entity>. Then, we experiment with four algorithms for constructing <entity id="P08-1055.10">intensional summaries</entity> in <entity id="P08-1055.11">cruiser</entity>, and show that two <entity id="P08-1055.12">summary</entity> types are equally effective: <entity id="P08-1055.13">summaries</entity> that maximize domain coverage and <entity id="P08-1055.14">summaries</entity> that maximize utility with respect to a user model.</abstract>

</text>

<text id="P03-1021">
<title>Minimum Error Rate Training In Statistical Machine Translation</title>
<abstract>
Often, the training procedure for <entity id="P03-1021.1">statistical machine translation</entity> models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final <entity id="P03-1021.2">translation</entity> quality on unseen <entity id="P03-1021.3">text</entity>. In this paper, we analyze various training criteria which directly optimize <entity id="P03-1021.4">translation</entity> quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.
</abstract>

</text>

<text id="P06-2108">
<title>Using Word Support Model To Improve Chinese Input System</title>
<abstract>
This paper presents a <entity id="P06-2108.1">word support model (WSM)</entity>. The <entity id="P06-2108.2">WSM</entity> can effectively perform <entity id="P06-2108.3">homophone</entity> selection and <entity id="P06-2108.4">syllable-word segmentation</entity> to improve <entity id="P06-2108.5">Chinese input systems</entity>. The experimental results show that: (1) the <entity id="P06-2108.6">WSM</entity> is able to achieve <entity id="P06-2108.7">tonal (syllables input with four tones)</entity> and <entity id="P06-2108.8">toneless (syllables input without four tones)</entity> <entity id="P06-2108.9">syllable-to-word (STW)</entity> accuracies of 99% and 92%, respectively, among the converted <entity id="P06-2108.10">words</entity>; and (2) while applying the <entity id="P06-2108.11">WSM</entity> as an adaptation processing, together with the <entity id="P06-2108.12">Microsoft Input Method Editor 2003 (MSIME)</entity> and an optimized <entity id="P06-2108.13">bigram model</entity>, the average <entity id="P06-2108.14">tonal and toneless STW</entity> improvements are 37% and 35%, respectively.
</abstract>

</text>

<text id="C94-1052">
<title>TGE: Tlinks Generation Environment</title>
<abstract>
"This paper describes the enhancements made, within a unification framework, based on typed feature structures, in order to support linking of <entity id="C94-1052.1">lexical entries</entity> to their <entity id="C94-1052.2">translation equivalents</entity>. To help this task we have developed an interactive environment: <entity id="C94-1052.3">TGE</entity>. Several experiments, corresponding to rather "closed" <entity id="C94-1052.4">semantic domains</entity>, have been developed in order to generate <entity id="C94-1052.5">lexical cross-relations</entity> between <entity id="C94-1052.6">English</entity> and <entity id="C94-1052.7">Spanish</entity>. </abstract>

</text>

<text id="D08-1086">
<title>Integrating Multi-level Linguistic Knowledge with a Unified Framework for Mandarin Speech Recognition</title>
<abstract>
To improve the <entity id="D08-1086.1">Mandarin</entity> <entity id="D08-1086.2">large vocabulary continuous speech recognition (LVCSR)</entity>, a unified framework based approach is introduced to exploit <entity id="D08-1086.3">multi-level linguistic knowledge</entity>. In this framework, each <entity id="D08-1086.4">knowledge source</entity> is represented by a <entity id="D08-1086.5">Weighted Finite State Transducer (WFST)</entity>, and then they are combined to obtain a so-called analyzer for integrating <entity id="D08-1086.6">multi-level knowledge sources</entity>. Due to the uniform <entity id="D08-1086.7">transducer</entity> representation, any <entity id="D08-1086.8">knowledge source</entity> can be easily integrated into the analyzer, as long as it can be encoded into <entity id="D08-1086.9">WFSTs</entity>. Moreover, as the knowledge in each level is modeled independently and the combination is processed in the model level, the information inherently in each <entity id="D08-1086.10">knowledge source</entity> has a chance to be thoroughly exploited. By simulations, the effectiveness of the analyzer is investigated, and then a <entity id="D08-1086.11">LVCSR</entity> system embedding the presented analyzer is evaluated. Experimental results reveal that this unified framework is an effective approach which significantly improves the performance of <entity id="D08-1086.12">speech recognition</entity> with a 9.9% relative reduction of character error rate on the HUB-4 test set, a widely used <entity id="D08-1086.13">Mandarin speech recognition</entity> task.
</abstract>

</text>

<text id="E03-1009">
<title>Combining Distributional And Morphological Information For Part Of Speech Induction</title>
<abstract>
In this paper we discuss algorithms for clustering <entity id="E03-1009.1">words</entity> into classes from <entity id="E03-1009.2">unlabeled text</entity> using unsupervised algorithms, based on <entity id="E03-1009.3">distributional and morphological information</entity>. We show how the use of <entity id="E03-1009.4">morphological information</entity> can improve the performance on <entity id="E03-1009.5">rare words</entity>, and that this is robust across a wide range of <entity id="E03-1009.6">languages</entity>.
</abstract>

</text>

<text id="C00-2124">
<title>Applying System Combination To Base Noun Phrase Identification</title>
<abstract>
We use seven machine learning algorithms for one task: identifying base <entity id="C00-2124.1">noun phrases</entity>. The results have been processed by different system combination methods and all of these outperformed the best individual result. We have applied the seven learners with the best combinator, a majority vote of the top five systems, to a standard data set and managed to improve the best published result for this data set.
</abstract>

</text>

<text id="I05-2045">
<title>
Unsupervised Feature Selection for <entity id="I05-2045.1">Relation Extraction</entity></title>
<abstract>
"This paper presents an <entity id="I05-2045.2">unsupervised relation extraction algorithm</entity>, which induces <entity id="I05-2045.3">relations</entity> between <entity id="I05-2045.4">entity pairs</entity> by grouping them into a "natural" number of clusters based on the similarity of their <entity id="I05-2045.5">contexts</entity>. Stability-based criterion is used to automatically estimate the number of clusters. For removing noisy <entity id="I05-2045.6">feature words</entity> in clustering procedure, feature selection is conducted by optimizing a trace based criterion subject to some constraint in an unsupervised manner. After <entity id="I05-2045.7">relation clustering</entity> procedure, we employ a <entity id="I05-2045.8">discriminative category matching (DCM)</entity> to find <entity id="I05-2045.9">typical and discriminative words</entity> to represent different <entity id="I05-2045.10">relations</entity>. Experimental results show the effectiveness of our algorithm. "
</abstract>

</text>

<text id="C02-1065">
<title>
Measuring The Similarity Between <entity id="C02-1065.1">Compound Nouns</entity> In Different <entity id="C02-1065.2">Languages</entity> Using <entity id="C02-1065.3">Non-Parallel Corpora</entity></title>
<abstract>
This paper presents a method that measures the similarity between <entity id="C02-1065.4">compound nouns</entity> in different languages to locate <entity id="C02-1065.5">translation equivalents</entity> from <entity id="C02-1065.6">corpora</entity>. The method uses information from unrelated <entity id="C02-1065.7">corpora</entity> in different <entity id="C02-1065.8">languages</entity> that do not have to be parallel. This means that many <entity id="C02-1065.9">corpora</entity> can be used. The method compares the <entity id="C02-1065.10">contexts</entity> of target <entity id="C02-1065.11">compound nouns</entity> and <entity id="C02-1065.12">translation candidates</entity> in the <entity id="C02-1065.13">word</entity> or <entity id="C02-1065.14">semantic attribute</entity> level. In this paper, we show how this measuring method can be applied to select the best <entity id="C02-1065.15">English translation</entity> candidate for <entity id="C02-1065.16">Japanese compound nouns</entity> in more than 70% of the cases.
</abstract>

</text>

<text id="L08-1611">
<title>A Compact Arabic Lexical Semantics Language Resource Based on the Theory of Semantic Fields</title>
<abstract>Applications of <entity id="L08-1611.1">statistical Arabic NLP</entity> in general, and <entity id="L08-1611.2">text mining</entity> in specific, along with the tools underneath perform much better as the <entity id="L08-1611.3">statistical processing</entity> operates on deeper <entity id="L08-1611.4">language factorization(s)</entity> than on <entity id="L08-1611.5">raw text</entity>. <entity id="L08-1611.6">Lexical semantic factorization</entity> is very important in that aspect due to its feasibility, high level of abstraction, and the language independence of its output. In the core of such a factorization lies an <entity id="L08-1611.7">Arabic lexical semantic DB</entity>. While building this <entity id="L08-1611.8">LR</entity>, we had to go beyond the conventional exclusive collection of <entity id="L08-1611.9">words</entity> from <entity id="L08-1611.10">dictionaries</entity> and <entity id="L08-1611.11">thesauri</entity> that cannot alone produce a satisfactory coverage of this highly <entity id="L08-1611.12">inflective and derivative language</entity>. This paper is hence devoted to the design and implementation of an <entity id="L08-1611.13">Arabic lexical semantics LR</entity> that enables the retrieval of the possible senses of any given <entity id="L08-1611.14">Arabic word</entity> at a high coverage. Instead of tying full <entity id="L08-1611.15">Arabic words</entity> to their possible senses, our <entity id="L08-1611.16">LR</entity> flexibly relates <entity id="L08-1611.17">morphologically and PoS-tags constrained Arabic lexical compounds</entity> to a predefined limited set of <entity id="L08-1611.18">semantic fields</entity> across which the standard <entity id="L08-1611.19">semantic relations</entity> are defined. With the aid of the same <entity id="L08-1611.20">large-scale Arabic morphological analyzer</entity> and <entity id="L08-1611.21">PoS tagger</entity> in the runtime, the possible senses of virtually any given <entity id="L08-1611.22">Arabic word</entity> are retrievable.
</abstract>

</text>

<text id="E06-1005">
<title>Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment</title>
<abstract>
This paper describes a novel method for computing a <entity id="E06-1005.1">consensus translation</entity> from the outputs of <entity id="E06-1005.2">multiple machine translation (MT) systems</entity>. The outputs are combined and a possibly new <entity id="E06-1005.3">translation hypothesis</entity> can be generated. Similarly to the well-established <entity id="E06-1005.4">ROVER approach</entity> of ( Fiscus, 1997 ) for combining <entity id="E06-1005.5">speech recognition hypotheses</entity>, the <entity id="E06-1005.6">consensus translation</entity> is computed by voting on a <entity id="E06-1005.7">confusion network</entity>. To create the <entity id="E06-1005.8">confusion network</entity>, we produce pairwise <entity id="E06-1005.9">word alignments</entity> of the original <entity id="E06-1005.10">machine translation hypotheses</entity> with an enhanced <entity id="E06-1005.11">statistical alignment algorithm</entity> that explicitly models <entity id="E06-1005.12">word reordering</entity>. The context of a whole <entity id="E06-1005.13">document</entity> of <entity id="E06-1005.14">translations</entity> rather than a single <entity id="E06-1005.15">sentence</entity> is taken into account to produce the <entity id="E06-1005.16">alignment</entity>. The proposed <entity id="E06-1005.17">alignment and voting approach</entity> was evaluated on several <entity id="E06-1005.18">machine translation tasks</entity>, including a large <entity id="E06-1005.19">vocabulary task</entity>. The <entity id="E06-1005.20">method</entity> was also tested in the framework of <entity id="E06-1005.21">multi-source and speech translation</entity>. On all tasks and conditions, we achieved significant <entity id="E06-1005.22">improvements</entity> in <entity id="E06-1005.23">translation quality</entity>, increasing e.g. the <entity id="E06-1005.24">BLEU score</entity> by as much as 15%.
</abstract>

</text>

<text id="N06-1024">
<title>Fully Parsing The Penn Treebank</title>
<abstract>
We present a two stage <entity id="N06-1024.1">parser</entity> that recovers <entity id="N06-1024.2">Penn Treebank</entity> style <entity id="N06-1024.3">syntactic analyses</entity> of new <entity id="N06-1024.4">sentences</entity> including skeletal <entity id="N06-1024.5">syntactic structure</entity>, and, for the first time, both <entity id="N06-1024.6">function tags</entity> and <entity id="N06-1024.7">empty categories</entity>. The <entity id="N06-1024.8">accuracy</entity> of the first-stage <entity id="N06-1024.9">parser</entity> on the standard <entity id="N06-1024.10">Parseval metric</entity> matches that of the ( Collins, 2003 ) <entity id="N06-1024.11">parser</entity> on which it is based, despite the <entity id="N06-1024.12">data fragmentation</entity> caused by the greatly enriched space of possible node labels. This first stage simultaneously achieves near state-of-the-art <entity id="N06-1024.13">performance</entity> on <entity id="N06-1024.14">recovering function</entity> <entity id="N06-1024.15">tags</entity> with minimal modifications to the underlying <entity id="N06-1024.16">parser</entity>, modifying less than ten lines of code. The second stage achieves state-of-the-art <entity id="N06-1024.17">performance</entity> on the <entity id="N06-1024.18">recovery</entity> of <entity id="N06-1024.19">empty categories</entity> by combining a <entity id="N06-1024.20">linguistically-informed architecture</entity> and a rich <entity id="N06-1024.21">feature set</entity> with the power of modern <entity id="N06-1024.22">machine learning methods</entity>.
</abstract>

</text>

<text id="E95-1041">
<title>An Algorithm To Co-Ordinate Anaphora Resolution And PPS Disambiguation Process</title>
<abstract>
Both <entity id="E95-1041.1">anaphora resolution</entity> and <entity id="E95-1041.2">prepositional phrase (PP) attachment</entity> are the most frequent <entity id="E95-1041.3">ambiguities</entity> in <entity id="E95-1041.4">natural language processing</entity>. Several methods have been proposed to deal with each phenomenon separately, however none of proposed systems has considered the way of dealing both phenomena. We tackle this issue here, proposing an algorithm to co-ordinate the treatment of these two problems efficiently, i.e., the aim is also to exploit at each step all the results that each <entity id="E95-1041.5">component</entity> can provide.
</abstract>

</text>

<text id="H92-1033">
<title>Vocabulary And Environment Adaptation In Vocabulary-Independent Speech Recognition</title>
<abstract>
In this paper, we are looking into the adaptation issues of <entity id="H92-1033.1">vocabulary-independent (VI) systems</entity>. Just as with <entity id="H92-1033.2">speaker adaptation</entity> in <entity id="H92-1033.3">speaker-independent system</entity>, two <entity id="H92-1033.4">vocabulary adaptation algorithms</entity> [5] are implemented in order to tailor the <entity id="H92-1033.5">VI subword models</entity> to the <entity id="H92-1033.6">target vocabulary</entity>. The first algorithm is to generate <entity id="H92-1033.7">vocabulary-adapted clustering decision trees</entity> by focusing on relevant <entity id="H92-1033.8">allophones</entity> during <entity id="H92-1033.9">tree generation</entity> and reduces the <entity id="H92-1033.10">VI error rate</entity> by 9%. The second algorithm, <entity id="H92-1033.11">vocabulary-bias training</entity>, is to give the relevant <entity id="H92-1033.12">allophones</entity> more prominence by assign more weight to them
during <entity id="H92-1033.13">Baum-Welch training</entity> of the <entity id="H92-1033.14">generalized allophonic models</entity> and reduces the <entity id="H92-1033.15">VI error rate</entity> by 15%. Finally, in order to overcome the <entity id="H92-1033.16">degradation</entity> caused by the different <entity id="H92-1033.17">acoustic environments</entity> used for <entity id="H92-1033.18">VI training and testing</entity>, <entity id="H92-1033.19">CDCN</entity> and <entity id="H92-1033.20">ISDCN</entity> originally designed for <entity id="H92-1033.21">microphone adaptation</entity> are incorporated into our <entity id="H92-1033.22">VI system</entity> and both reduce the <entity id="H92-1033.23">degradation</entity> of <entity id="H92-1033.24">VI cross-environment recognition</entity> by 50%. </abstract>

</text>

<text id="H94-1101">
<title>A Knowledge-Based Approach To Indexing Scientific Text</title>
<abstract>
We are developing a system which scans articles from <entity id="H94-1101.1">scientific literature</entity> for the purpose of indexing the <entity id="H94-1101.2">text</entity>. That is, the system should assist in the rapid determination of the <entity id="H94-1101.3">key topics</entity> and content of articles in a particular domain and in the production of brief <entity id="H94-1101.4">phrases</entity> describing the content. The number of correct <entity id="H94-1101.5">concepts</entity> generated should be 80% of the <entity id="H94-1101.6">concepts</entity> present (a <entity id="H94-1101.7">recall rate</entity> of 80%), as compared to the output of human document analysts processing the same material.
</abstract>

</text>

<text id="W97-0503">
<title>Simple NLP Techniques For Expanding Telegraphic Sentences</title>
<abstract>
Some people have disabilities which make it difficult for them to speak in an understandable fashion. The field of <entity id="W97-0503.1">Augmentative and Alternative Communication (AAC)</entity> is concerned with developing methods to augment the communicative ability of such people. Over the past 9 years, the Applied Science and Engineering Laboratories (ASEL) at the University of Delaware and the duPont Hospital for Children, has been involved with applying <entity id="W97-0503.2">natural language processing (NLP) technologies</entity> to the field of <entity id="W97-0503.3">AAC</entity>. One of the major projects at ASEL (The COMPAN-SION project) has been concerned with the application of primarily <entity id="W97-0503.4">lexical semantics</entity> and <entity id="W97-0503.5">sentence generation technology</entity> to expand <entity id="W97-0503.6">telegraphic input</entity> into full <entity id="W97-0503.7">sentences</entity>. While this project has shown some very <entity id="W97-0503.8">promising results</entity>, its direct application to a communication device is somewhat questionable (primarily because of the computational power necessary to make the technique fast). This paper describes some of the problems with bringing Compansion to a standard communication device and introduces some work being done in conjunction with the Prentke Romich Company (PRC) (a well known communication device manufacturer) on developing a pared-down version of Compansion for people with cognitive impairments.
</abstract>

</text>

<text id="W00-1210">
<title>A Trainable Method For Extracting Chinese Entity Names And Their Relations</title>
<abstract>
In this paper we propose a trainable method for extracting <entity id="W00-1210.1">Chinese entity names and their relations</entity>. We view the entire problem as series of <entity id="W00-1210.2">classification problems</entity> and employ <entity id="W00-1210.3">memory-based learning (MBL)</entity> to resolve them. Preliminary results show that this method is efficient, flexible and promising to achieve <entity id="W00-1210.4">better performance</entity> than other existing methods.
</abstract>

</text>

<text id="W02-1813">
<title>Using The Segmentation Corpus To Define An Inventory Of Concatenative Units For Cantonese Speech Synthesis</title>
<abstract>
The problem of <entity id="W02-1813.1">word segmentation</entity> affects all aspects of <entity id="W02-1813.2">Chinese language processing</entity>, including the development of <entity id="W02-1813.3">text-to-speech synthesis systems</entity>. In synthesizing a <entity id="W02-1813.4">Hong Kong Cantonese text</entity>, for example, <entity id="W02-1813.5">words</entity> must be identified in order to model fusion of coda [p] with initial [h], and other similar effects that differentiate <entity id="W02-1813.6">word-internal syllable boundaries</entity> from <entity id="W02-1813.7">syllable</entity> edges that begin or end <entity id="W02-1813.8">words</entity>. Accurate <entity id="W02-1813.9">segmentation</entity> is necessary also for developing any <entity id="W02-1813.10">list of words</entity> large enough to identify the <entity id="W02-1813.11">word-internal cross-syllable sequences</entity> that must be recorded to model such effects using <entity id="W02-1813.12">concatenated synthesis units</entity>. This paper describes our use of the <entity id="W02-1813.13">Segmentation Corpus</entity> to constrain such <entity id="W02-1813.14">units</entity>.
</abstract>

</text>

<text id="W00-0110">
<title>Similarities And Differences Among Semantic Behaviors Of Japanese Adnominal Constituents</title>
<abstract>
This paper treats the <entity id="W00-0110.1">classification</entity> of the <entity id="W00-0110.2">semantic functions</entity> performed by <entity id="W00-0110.3">adnominal constituents</entity> in <entity id="W00-0110.4">Japanese</entity>, where many <entity id="W00-0110.5">parts of speech</entity> act as <entity id="W00-0110.6">adnominal constituents</entity>. In order to establish a formal treatment of the <entity id="W00-0110.7">semantic roles</entity>, the similarities and differences among <entity id="W00-0110.8">adnominal constituents</entity>, i.e. <entity id="W00-0110.9">adjectives</entity> and <entity id="W00-0110.10">"noun + NO" (in English "of + noun") structures</entity>, which have a broad range of <entity id="W00-0110.11">semantic functions</entity>, are discussed. This paper also proposes an objective method of classifying these <entity id="W00-0110.12">constructs</entity> using a large amount of <entity id="W00-0110.13">linguistic data</entity>. The feasibility of this was verified with a <entity id="W00-0110.14">self-organizing semantic map</entity> based on a <entity id="W00-0110.15">neural network model</entity>.
</abstract>

</text>

<text id="J95-1001">
<title>Principled Disambiguation: Discriminating Adjective Senses With Modified Nouns</title>
<abstract>
Recent corpus-based work on <entity id="J95-1001.1">word sense disambiguation</entity> explores the application of <entity id="J95-1001.2">statistical pattern recognition procedures</entity> to <entity id="J95-1001.3">lexical co-occurrence data</entity> from <entity id="J95-1001.4">very large text databases</entity>. In this paper we argue for a <entity id="J95-1001.5">linguistically principled approach</entity> to <entity id="J95-1001.6">disambiguation</entity>, in which relevant <entity id="J95-1001.7">contextual clues</entity> are narrowly defined, in <entity id="J95-1001.8">syntactic</entity> and <entity id="J95-1001.9">semantic terms</entity>, and in which only highly reliable clues are exploited. <entity id="J95-1001.10">Statistical methods</entity> play a definite role in this work, helping to organize and analyze data, but the <entity id="J95-1001.11">disambiguation method</entity> itself does not employ <entity id="J95-1001.12">statistical data</entity> or <entity id="J95-1001.13">decision criteria</entity>. This approach results in improved understanding of the <entity id="J95-1001.14">disambiguation problem</entity> both in general and on a <entity id="J95-1001.15">word-specific</entity> basis and leads to broadly applicable and nearly errorless clues to <entity id="J95-1001.16">word sense</entity>. The approach is illustrated by an experiment discriminating among the <entity id="J95-1001.17">senses</entity> of <entity id="J95-1001.18">adjectives</entity>, which have been relatively neglected in work on <entity id="J95-1001.19">sense disambiguation</entity>. In particular, the paper assesses the potential of <entity id="J95-1001.20">nouns</entity> for discriminating among the <entity id="J95-1001.21">senses</entity> of <entity id="J95-1001.22">adjectives</entity> that modify them. This assessment is based on an empirical study of five of the most frequent <entity id="J95-1001.23">ambiguous adjectives</entity> in <entity id="J95-1001.24">English</entity>: and About three-quarters of all instances of these <entity id="J95-1001.25">adjectives</entity> can be disambiguated almost errorlessly by the <entity id="J95-1001.26">nouns</entity> they modify or by the <entity id="J95-1001.27">syntactic constructions</entity> in which they occur. Such <entity id="J95-1001.28">disambiguation</entity> requires only simple rules, which can be automated easily. Furthermore, a small number of <entity id="J95-1001.29">semantic attributes</entity> supply a compact means of representing the <entity id="J95-1001.30">noun</entity> clues in a very few <entity id="J95-1001.31">rules</entity>. Clues other than <entity id="J95-1001.32">nouns</entity> are required when <entity id="J95-1001.33">modified nouns</entity> are not useable. The <entity id="J95-1001.34">sense</entity> of an <entity id="J95-1001.35">ambiguous modified noun</entity> may be needed to determine the relevant <entity id="J95-1001.36">semantic attribute</entity> for <entity id="J95-1001.37">disambiguation</entity> of a <entity id="J95-1001.38">target adjective</entity>; and other <entity id="J95-1001.39">adjectives</entity>, <entity id="J95-1001.40">verbs</entity>, and <entity id="J95-1001.41">grammatical constructions</entity> all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined <entity id="J95-1001.42">syntactic relations</entity> to the <entity id="J95-1001.43">ambiguous adjective</entity>. Some of these clues, however, may be hard to automate.
</abstract>

</text>

<text id="P98-1108">
<title>Use of Mutual Information Based Character Clusters in Dictionary-less Morphological Analysis of Japanese</title>
<abstract>
For languages whose character set is very large and whose orthography does not require spacing between <entity id="P98-1108.1">words</entity>, such as <entity id="P98-1108.2">Japanese</entity>, <entity id="P98-1108.3">tokenizing</entity> and <entity id="P98-1108.4">part-of-speech tagging</entity> are often the difficult parts of any <entity id="P98-1108.5">morphological analysis</entity>. For practical systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating <entity id="P98-1108.6">character clustering</entity> based on <entity id="P98-1108.7">mutual information</entity> into <entity id="P98-1108.8">Decision-Tree Dictionary-less morphological analysis</entity>. By using <entity id="P98-1108.9">natural classes</entity>, we have confirmed that our <entity id="P98-1108.10">morphological analyzer</entity> has been significantly improved in both <entity id="P98-1108.11">tokenizing</entity> and <entity id="P98-1108.12">tagging</entity> <entity id="P98-1108.13">Japanese</entity> <entity id="P98-1108.14">text</entity>.
</abstract>

</text>

<text id="W05-0906">
<title>Evaluating Summaries And Answers: Two Sides Of The Same Coin?</title>
<abstract>
This paper discusses the convergence between <entity id="W05-0906.1">question answering</entity> and <entity id="W05-0906.2">multi-document summarization</entity>, pointing out implications and opportunities for knowledge transfer in both directions. As a case study in one direction, we discuss the recent development of an <entity id="W05-0906.3">automatic method for evaluating definition questions</entity> based on <entity id="W05-0906.4">n-gram overlap</entity>, a commonly-used technique in <entity id="W05-0906.5">summarization evaluation</entity>. In the other direction, the move towards <entity id="W05-0906.6">topic-oriented summaries</entity> requires an understanding of relevance and <entity id="W05-0906.7">topicality<entity id="W05-0906.8">, issues which have received attention in the <entity id="W05-0906.9">question answering </entity>literature. It is our opinion that <entity id="W05-0906.10">question answering</entity> and <entity id="W05-0906.11">multi-document summarization</entity> represent two complementary approaches to the same problem of satisfying complex user information needs. Although this points to many exciting opportunities for system building, here we primarily focus on implications for </entity>system evaluation</entity>.
</abstract>

</text>

<text id="W03-1729">
<title>SYSTRAN's Chinese Word Segmentation</title>
<abstract>
SYSTRAN'S <entity id="W03-1729.1">Chinese word segmentation</entity> is one important component of its <entity id="W03-1729.2">Chinese-English machine translation system</entity>. The <entity id="W03-1729.3">Chinese word segmentation</entity> module uses a <entity id="W03-1729.4">rule-based approach</entity>, based on a <entity id="W03-1729.5">large dictionary</entity> and <entity id="W03-1729.6">fine-grained linguistic rules</entity>. It works on <entity id="W03-1729.7">general-purpose texts</entity> from different <entity id="W03-1729.8">Chinese-speaking regions</entity>, with comparable <entity id="W03-1729.9">performance</entity>. SYSTRAN participated in the four open tracks in the First International <entity id="W03-1729.10">Chinese Word Segmentation</entity> Bakeoff. This paper gives a general description of the <entity id="W03-1729.11">segmentation module</entity>, as well as the results and analysis of its <entity id="W03-1729.12">performance</entity> in the Bakeoff.</abstract>

</text>

<text id="W07-1315">
<title>ParaMor : Minimally Supervised Induction of Paradigm Structure and Morphological Analysis</title>
<abstract>
Paradigms provide an inherent organizational structure to <entity id="W07-1315.1">natural language morphology</entity>. ParaMor, our <entity id="W07-1315.2">minimally supervised morphology induction algorithm</entity>, retrusses the <entity id="W07-1315.3">word forms</entity> of <entity id="W07-1315.4">raw text corpora</entity> back onto their paradigmatic skeletons; performing on par with state-of-the-art <entity id="W07-1315.5">minimally supervised morphology induction algorithms</entity> at <entity id="W07-1315.6">morphological analysis</entity> of <entity id="W07-1315.7">English</entity> and <entity id="W07-1315.8">German</entity>. ParaMor consists of two <entity id="W07-1315.9">phases</entity>. Our algorithm first constructs <entity id="W07-1315.10">sets of affixes</entity> closely mimicking the paradigms of a language. And with these structures in hand, ParaMor then annotates <entity id="W07-1315.11">word forms</entity> with <entity id="W07-1315.12">morpheme boundaries</entity>. To set ParaMor 's few free parameters we analyze a <entity id="W07-1315.13">training corpus</entity> of <entity id="W07-1315.14">Spanish</entity>. Without adjusting parameters, we induce the <entity id="W07-1315.15">morphological structure</entity> of <entity id="W07-1315.16">English</entity> and <entity id="W07-1315.17">German</entity>. Adopting the <entity id="W07-1315.18">evaluation methodology</entity> of Morpho Challenge 2007 ( Kurimo et al., 2007 ), we compare ParaMor 's <entity id="W07-1315.19">morphological analyses</entity> with Morfessor ( Creutz, 2006 ), a modern <entity id="W07-1315.20">minimally supervised morphology induction system</entity>. ParaMor consistently achieves competitive <entity id="W07-1315.21">F1 measures</entity>.
</abstract>

</text>

<text id="P08-2055">
<title>Computing Confidence Scores for All Sub Parse Trees</title>
<abstract> Computing <entity id="P08-2055.1">confidence scores</entity> for applications, such as <entity id="P08-2055.2">dialogue system</entity>, <entity id="P08-2055.3">information retrieving and extraction</entity>, is an active research area. However, its focus has been primarily on computing <entity id="P08-2055.4">word-, concept-, or utterance-level confidences</entity>. Motivated by the need from sophisticated <entity id="P08-2055.5">dialogue systems</entity> for more effective <entity id="P08-2055.6">dialogs</entity>, we generalize the <entity id="P08-2055.7">confidence annotation</entity> to all the subtrees, the first effort in this line of research. The other contribution of this work is that we incorporated novel long <entity id="P08-2055.8">distance features</entity> to address challenges in computing <entity id="P08-2055.9">multi-level confidence scores</entity>. Using <entity id="P08-2055.10">Conditional Maximum Entropy (CME) classifier</entity> with all the selected features, we reached an <entity id="P08-2055.11">annotation error rate</entity> of 26.0% in the <entity id="P08-2055.12">SWBD corpus</entity>, compared with a <entity id="P08-2055.13">subtree error rate</entity> of 41.91%, a closely related benchmark with the <entity id="P08-2055.14">Charniak parser</entity> from ( Kahn et al., 2005 ).
</abstract>

</text>

<text id="P03-2012">
<title>High-Precision Identification Of Discourse New And Unique Noun Phrases</title>
<abstract><entity id="P03-2012.1">Coreference resolution systems</entity> usually attempt to find a suitable antecedent for (almost) every <entity id="P03-2012.2">noun phrase</entity>. Recent studies, however, show that many definite <entity id="P03-2012.3">NPs</entity> are not <entity id="P03-2012.4">anaphoric</entity>. The same claim, obviously, holds for the indefinites as well. In this study we try to learn automatically two <entity id="P03-2012.5">classifications</entity>, <entity id="P03-2012.6">±discourse_new and ±unique</entity>, relevant for this problem. We use a small <entity id="P03-2012.7">training corpus (MUC-7)</entity>, but also acquire some <entity id="P03-2012.8">data</entity> from the <entity id="P03-2012.9">Internet</entity>. Combining our <entity id="P03-2012.10">classifiers</entity> sequentially, we achieve 88.9% <entity id="P03-2012.11">precision</entity> and 84.6% <entity id="P03-2012.12">recall</entity> for <entity id="P03-2012.13">discourse new entities</entity>. We expect our <entity id="P03-2012.14">classifiers</entity> to provide a good prefiltering for <entity id="P03-2012.15">coreference resolution systems</entity>, improving both their <entity id="P03-2012.16">speed</entity> and <entity id="P03-2012.17">performance</entity>.
</abstract>

</text>

<text id="P01-1013">
<title>Chinese Text Segmentation With MBDP-1: Making The Most Of Training Corpora</title>
<abstract>
This paper describes a system for <entity id="P01-1013.1">segmenting Chinese text</entity> into <entity id="P01-1013.2">words</entity> using the MBDP-1 algorithm. MBDP-1 is a <entity id="P01-1013.3">knowledge-free segmentation algorithm</entity> that bootstraps its own <entity id="P01-1013.4">lexicon</entity>, which starts out empty. Experiments on <entity id="P01-1013.5">Chinese</entity> and <entity id="P01-1013.6">English corpora</entity> show that MBDP-1 reliably outperforms the best previous algorithm when the available <entity id="P01-1013.7">hand-segmented training corpus</entity> is small. As the size of the <entity id="P01-1013.8">hand-segmented training corpus</entity> grows, the <entity id="P01-1013.9">performance</entity> of MBDP-1 converges toward that of the best previous algorithm. The fact that MBDP-1 can be used with a <entity id="P01-1013.10">small corpus</entity> is expected to be useful not only for the rare event of adapting to a new language, but also for the common event of adapting to a new <entity id="P01-1013.11">genre</entity> within the same language.
</abstract>

</text>

<text id="C86-1108">
<title>User Specification Of Syntactic Case Frames In TELI, A Transportable, User-Customized Natural Language Processor</title>
<abstract>
In this paper, we present methods that allow the users of a <entity id="C86-1108.1">natural language processor (NLP)</entity> to define, inspect, and modify any <entity id="C86-1108.2">case frame information</entity> associated with the <entity id="C86-1108.3">words</entity> and <entity id="C86-1108.4">phrases</entity> known to the system. An implementation of this work forms a critical part of the <entity id="C86-1108.5">Transportable English-Language Interface (TELI) system</entity>. However, our techniques have enabled customization capabilities largely independent of the specific <entity id="C86-1108.6">NLP</entity> for which information is being acquired.
</abstract>

</text>

<text id="L08-1114">
<title>Experiments on Processing Overlapping Parallel Corpora</title>
<abstract>
The number and sizes of <entity id="L08-1114.1">parallel corpora</entity> keep growing, which makes it necessary to have automatic methods of processing them: combining, checking and improving <entity id="L08-1114.2">corpora quality</entity>, etc. We here introduce a method which enables performing many of these by exploiting <entity id="L08-1114.3">overlapping parallel corpora</entity>. The method finds the correspondence between <entity id="L08-1114.4">sentence pairs</entity> in two <entity id="L08-1114.5">corpora</entity>: first the corresponding <entity id="L08-1114.6">language parts</entity> of the <entity id="L08-1114.7">corpora</entity> are aligned and then the two resulting <entity id="L08-1114.8">alignments</entity> are compared. The method takes into consideration slight differences in the <entity id="L08-1114.9">source documents</entity>, different levels of <entity id="L08-1114.10">segmentation</entity> of the <entity id="L08-1114.11">input corpora</entity>, encoding differences and other aspects of the task. The paper describes two experiments conducted to test the method. In the first experiment, the <entity id="L08-1114.12">Estonian-English</entity> part of the <entity id="L08-1114.13">JRC-Acquis corpus</entity> was combined with another <entity id="L08-1114.14">corpus</entity> of <entity id="L08-1114.15">legislation texts</entity>. In the second experiment alternatively aligned versions of the JRC-Acquis are compared to each other with the example of all <entity id="L08-1114.16">language pairs</entity> between <entity id="L08-1114.17">English</entity>, <entity id="L08-1114.18">Estonian</entity> and <entity id="L08-1114.19">Latvian</entity>. Several additional conclusions about the corpora can be drawn from the results. The method proves to be effective for several <entity id="L08-1114.20">parallel corpora processing tasks</entity>.
</abstract>

</text>

<text id="E03-1019">
<title>A Flexible Pragmatics-Driven Language Generator For Animated Agents</title>
<abstract>
This paper describes the neca mnlg; a fully implemented<entity id="E03-1019.1"> Multimodal Natural Language Generation module</entity>. The mnlg is deployed as part of the <entity id="E03-1019.2">neca system</entity> which generates <entity id="E03-1019.3">dialogues</entity> between <entity id="E03-1019.4">animated agents</entity>. The <entity id="E03-1019.5">generation module</entity> supports the seamless integration of full <entity id="E03-1019.6">grammar rules</entity>, <entity id="E03-1019.7">templates</entity> and <entity id="E03-1019.8">canned text</entity>. The <entity id="E03-1019.9">generator</entity> takes input which allows for the specification of <entity id="E03-1019.10">syntactic, semantic and pragmatic constraints</entity> on the output.
</abstract>

</text>

<text id="C02-1079">
<title>Best Analysis Selection In Inflectional Languages</title>
<abstract><entity id="C02-1079.1">Ambiguity</entity> is the fundamental property of <entity id="C02-1079.2">natural language</entity>. Perhaps, the most burdensome case of <entity id="C02-1079.3">ambiguity</entity> manifests itself on the <entity id="C02-1079.4">syntactic level of analysis</entity>. In order to face up to the high number of obtained <entity id="C02-1079.5">derivation trees</entity>, this paper describes several techniques for evaluation of the figures of merit, which define a sort order on <entity id="C02-1079.6">parsing trees</entity>. The presented methods are based on <entity id="C02-1079.7">language specific features</entity> of <entity id="C02-1079.8">synthetical languages</entity> and they improve the results of simple <entity id="C02-1079.9">stochastic approaches</entity>.
</abstract>

</text>

<text id="L08-1197">
<title>Dialogue Speech and Images: the Companions Project Data Set</title>
<abstract>
This paper describes part of the <entity id="L08-1197.1">corpus collection</entity> efforts underway in the EC funded Companions project. The Companions project is collecting substantial quantities of <entity id="L08-1197.2">dialogue</entity> a large part of which focus on reminiscing about photographs. The <entity id="L08-1197.3">texts</entity> are in <entity id="L08-1197.4">English</entity> and <entity id="L08-1197.5">Czech</entity>. We describe the context and objectives for which this <entity id="L08-1197.6">dialogue corpus</entity> is being collected, the methodology being used and make observations on the resulting <entity id="L08-1197.7">data</entity>. The <entity id="L08-1197.8">corpora</entity> will be made available to the wider research community through the Companions Project web site.
</abstract>

</text>

<text id="C02-1098">
<title>
<entity id="C02-1098.1">Annotation-Based Multimedia Summarization And Translation</entity></title>
<abstract>
This paper presents techniques for <entity id="C02-1098.2">multimedia annotation</entity> and their application to <entity id="C02-1098.3">video summarization and translation</entity>. Our tool for <entity id="C02-1098.4">annotation</entity> allows users to easily create <entity id="C02-1098.5">annotation</entity> including <entity id="C02-1098.6">voice transcripts</entity>, <entity id="C02-1098.7">video scene descriptions</entity>, and <entity id="C02-1098.8">visual/auditory object descriptions</entity>. The module for <entity id="C02-1098.9">voice transcription</entity> is capable of <entity id="C02-1098.10">multilingual spoken language identification and recognition</entity>. A <entity id="C02-1098.11">video scene description</entity> consists of <entity id="C02-1098.12">semi-automatically detected keyframes</entity> of each scene in a video clip and time codes of scenes. A <entity id="C02-1098.13">visual object description</entity> is created by tracking and interactive naming of people and objects in video scenes. The <entity id="C02-1098.14">text data</entity> in the <entity id="C02-1098.15">multimedia annotation</entity> are <entity id="C02-1098.16">syntactically and semantically structured</entity> using <entity id="C02-1098.17">linguistic annotation</entity>. The proposed<entity id="C02-1098.18"> multimedia summarization</entity> works upon a <entity id="C02-1098.19">multimodal document</entity> that consists of a video, <entity id="C02-1098.20">keyframes</entity> of <entity id="C02-1098.21">scenes</entity>, and <entity id="C02-1098.22">transcripts</entity> of the <entity id="C02-1098.23">scenes</entity>. The <entity id="C02-1098.24">multimedia translation</entity> automatically generates several versions of <entity id="C02-1098.25">multimedia content</entity> in different <entity id="C02-1098.26">languages</entity>.
</abstract>

</text>

<text id="E03-1029">
<title>Automatic Construction Of Machine Translation Knowledge Using Translation Literalness</title>
<abstract>
When <entity id="E03-1029.1">machine translation (MT) knowledge</entity> is automatically constructed from <entity id="E03-1029.2">bilingual corpora</entity>, <entity id="E03-1029.3">redundant rules</entity> are acquired due to translation variety. These <entity id="E03-1029.4">rules</entity> increase <entity id="E03-1029.5">ambiguity</entity> or cause incorrect <entity id="E03-1029.6">MT results</entity>. To overcome this problem, we constrain the <entity id="E03-1029.7">sentences</entity> used for <entity id="E03-1029.8">knowledge extraction</entity> to "the appropriate <entity id="E03-1029.9">bilingual sentences</entity> for the <entity id="E03-1029.10">MT</entity>". In this paper, we propose a method using <entity id="E03-1029.11">translation literalness</entity> to select appropriate <entity id="E03-1029.12">sentences</entity> or <entity id="E03-1029.13">phrases</entity>. The <entity id="E03-1029.14">translation correspondence rate (TCR)</entity> is defined as the <entity id="E03-1029.15">literalness measure</entity>. Based on the <entity id="E03-1029.16">TCR</entity>, two <entity id="E03-1029.17">automatic construction methods</entity> are tested. One is to filter the <entity id="E03-1029.18">corpus</entity> before <entity id="E03-1029.19">rule acquisition</entity>. The other is to split the <entity id="E03-1029.20">acquisition process</entity> into two phases, where a <entity id="E03-1029.21">bilingual sentence</entity> is divided into <entity id="E03-1029.22">literal parts</entity> and the other parts before different generalizations are applied. The effects are evaluated by the <entity id="E03-1029.23">MT quality</entity>, and about 4.9% of <entity id="E03-1029.24">MT results</entity> were improved by the latter method.
</abstract>

</text>

<text id="L08-1405">
<title>Words in Contexts: Digital Editions of Literary Journals in the AAC - Austrian Academy Corpus</title>
<abstract>
"In this paper two highly innovative <entity id="L08-1405.1">digital editions</entity> will be presented. For the creation and the implementation of these <entity id="L08-1405.2">editions</entity> the latest developments within <entity id="L08-1405.3">corpus research</entity> have been taken into account. The <entity id="L08-1405.4">digital editions</entity> of the <entity id="L08-1405.5">historical literary journals</entity> "Die Fackel" (published by Karl Kraus in Vienna from 1899 to 1936) and "Der Brenner" (published by Ludwig Ficker in Innsbruck from 1910 to 1954) have been developed within the corpus research framework of the <entity id="L08-1405.6">"AAC - Austrian Academy Corpus</entity>" at the Austrian Academy of Sciences in collaboration with other researchers and programmers in the <entity id="L08-1405.7">AAC</entity> from Vienna together with the graphic designer Anne Burdick from Los Angeles. For the creation of these <entity id="L08-1405.8">scholarly digital editions</entity> the <entity id="L08-1405.9">AAC edition philosophy and edition principles</entity> have been applied whereby new <entity id="L08-1405.10">corpus research methods</entity> have been made use of for questions of <entity id="L08-1405.11">computational philology</entity> and <entity id="L08-1405.12">textual studies</entity> in a digital environment. The examples of the <entity id="L08-1405.13">digital online editions</entity> of the <entity id="L08-1405.14">literary journals</entity> "Die Fackel" and "Der Brenner" will give insights into the potentials and the benefits of making <entity id="L08-1405.15">corpus research methods and techniques</entity> available for scholarly research into <entity id="L08-1405.16">language</entity> and <entity id="L08-1405.17">literature</entity>. "
</abstract>

</text>

<text id="I05-3026">
<title>Description of the HKU Chinese Word Segmentation System for Sighan Bakeoff 2005</title>
<abstract>
In this paper, we describe in brief our <entity id="I05-3026.1">system</entity> for the <entity id="I05-3026.2">Second International Chinese Word Segmentation Bakeoff</entity> sponsored by the <entity id="I05-3026.3">ACL-SIGHAN</entity>. We participated in all tracks at the <entity id="I05-3026.4">bakeoff</entity>. The evaluation results show our <entity id="I05-3026.5">system</entity> can achieve an <entity id="I05-3026.6">F measure</entity> of 0.9400.967 for different <entity id="I05-3026.7">testing corpora</entity>.
</abstract>

</text>

<text id="N04-1030">
<title>Shallow Semantic Parsing Using Support Vector Machines</title>
<abstract>
In this paper, we propose a <entity id="N04-1030.1">machine learning algorithm</entity> for <entity id="N04-1030.2">shallow semantic parsing</entity>, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our <entity id="N04-1030.3">algorithm</entity> is based on <entity id="N04-1030.4">Support Vector Machines</entity> which we show give an improvement in performance over earlier <entity id="N04-1030.5">classifiers</entity>. We show performance improvements through a number of new <entity id="N04-1030.6">features</entity> and measure their ability to generalize to a new <entity id="N04-1030.7">test set</entity> drawn from the <entity id="N04-1030.8">AQUAINT corpus</entity>.
</abstract>

</text>

<text id="A94-1010">
<title>Improving Language Models By Clustering Training Sentences</title>
<abstract>
Many of the kinds of <entity id="A94-1010.1">language model</entity> used in <entity id="A94-1010.2">speech understanding</entity> suffer from imperfect <entity id="A94-1010.3">modeling</entity> of <entity id="A94-1010.4">intra-sentential contextual influences</entity>. I argue that this problem can be addressed by clustering the <entity id="A94-1010.5">sentences</entity> in a <entity id="A94-1010.6">training corpus</entity> automatically into <entity id="A94-1010.7">sub corpora</entity> on the criterion of <entity id="A94-1010.8">entropy reduction</entity>, and calculating separate <entity id="A94-1010.9">language model parameters</entity> for each <entity id="A94-1010.10">cluster</entity>. This kind of <entity id="A94-1010.11">clustering</entity> offers a way to represent important <entity id="A94-1010.12">contextual effects</entity> and can therefore significantly improve the performance of a <entity id="A94-1010.13">model</entity>. It also offers a reasonably automatic means to gather evidence on whether a <entity id="A94-1010.14">more complex, context-sensitive model</entity> using the same general kind of <entity id="A94-1010.15">linguistic information</entity> is likely to reward the effort that would be required to develop it: if <entity id="A94-1010.16">clustering</entity> improves the performance of a <entity id="A94-1010.17">model</entity>, this proves the existence of further <entity id="A94-1010.18">context dependencies</entity>, not exploited by the <entity id="A94-1010.19">unclustered model</entity>. As evidence for these claims, I present results showing that <entity id="A94-1010.20">clustering</entity> improves some <entity id="A94-1010.21">models</entity> but not others for the <entity id="A94-1010.22">ATIS domain</entity>. These results are consistent with other findings for such <entity id="A94-1010.23">models</entity>, suggesting that the existence or otherwise of an improvement brought about by <entity id="A94-1010.24">clustering</entity> is indeed a good pointer to whether it is worth developing further the <entity id="A94-1010.25">unclustered model</entity>.
</abstract>

</text>

<text id="A00-3005">
<title>Corpus-Based Syntactic Error Detection Using Syntactic Patterns</title>
<abstract>
This paper presents a <entity id="A00-3005.1">parsing system</entity> for the <entity id="A00-3005.2">detection of syntactic errors</entity>. It combines a <entity id="A00-3005.3">robust partial parser</entity> which obtains the <entity id="A00-3005.4">main sentence components</entity> and a <entity id="A00-3005.5">finite-state parser</entity> used for the description of <entity id="A00-3005.6">syntactic error patterns</entity>. The <entity id="A00-3005.7">system</entity> has been tested on a <entity id="A00-3005.8">corpus</entity> of real <entity id="A00-3005.9">texts</entity>, containing both <entity id="A00-3005.10">correct and incorrect sentences</entity>, with promising results.
</abstract>

</text>

<text id="H92-1111">
<title>Evaluating Text Understanding Systems</title>
<abstract>
The objectives of this project are to advance our understanding of the merits of current <entity id="H92-1111.1">text analysis techniques</entity>, as applied to the <entity id="H92-1111.2">performance</entity> of <entity id="H92-1111.3">realistic text analysis tasks</entity>, and to achieve this understanding by means of a sound <entity id="H92-1111.4">performance evaluation methodology</entity>. The <entity id="H92-1111.5">performance data</entity> can be interpreted in light of information known about the <entity id="H92-1111.6">text interpretation techniques</entity> for the various systems to yield qualitative insight into the relative validity of those <entity id="H92-1111.7">techniques</entity> for the <entity id="H92-1111.8">text analysis task</entity>. The <entity id="H92-1111.9">data</entity> can also be used as a means for determining which research areas are most critical to the successful <entity id="H92-1111.10">performance</entity> of the <entity id="H92-1111.11">task</entity>. The most recent <entity id="H92-1111.12">performance evaluation</entity> was conducted in May 1991, on systems contributed by 15 R&amp;D sites. The <entity id="H92-1111.13">evaluation task</entity> was intended to yield insight into <entity id="H92-1111.14">text analysis technology</entity>, including the use of <entity id="H92-1111.15">information retrieval technology</entity> (<entity id="H92-1111.16">document retrieval and categorization</entity>) instead of or in concert with <entity id="H92-1111.17">language understanding technology</entity>. The <entity id="H92-1111.18">evaluation</entity> concluded with the <entity id="H92-1111.19">Third Message Understanding Conference (MUC-3)</entity>.
</abstract>

</text>

<text id="A00-1010">
<title>Talk'n'Travel: A Conversational System For Air Travel Planning</title>
<abstract>
We describe <entity id="A00-1010.1">Talk'n'Travel</entity>, a <entity id="A00-1010.2">spoken dialogue language system</entity> for making air travel plans over the telephone. <entity id="A00-1010.3">Talk'n'Travel</entity> is a fully <entity id="A00-1010.4">conversational, mixed-initiative system</entity> that allows the <entity id="A00-1010.5">user</entity> to specify the <entity id="A00-1010.6">constraints</entity> on his travel plan in arbitrary order, ask <entity id="A00-1010.7">questions</entity>, etc., in <entity id="A00-1010.8">general spoken English</entity>. The <entity id="A00-1010.9">system</entity> operates according to a <entity id="A00-1010.10">plan-based agenda mechanism</entity>, rather than a <entity id="A00-1010.11">finite state network</entity>, and attempts to negotiate with the <entity id="A00-1010.12">user</entity> when not all of his <entity id="A00-1010.13">constraints</entity> can be met.
</abstract>

</text>

<text id="W99-0210">
<title>Coreference-Oriented Interlingual Slot Structure And Machine Translation</title>
<abstract>
One of the main problems of many <entity id="W99-0210.1">commercial Machine Translation (MT) and experimental systems</entity> is that they do not carry out a correct <entity id="W99-0210.2">pronominal anaphora generation</entity>. As mentioned in Mitkov (1996), solving the <entity id="W99-0210.3">anaphora</entity> and extracting the <entity id="W99-0210.4">antecedent</entity> are key issues in a correct <entity id="W99-0210.5">translation</entity>. In this paper, we propose an <entity id="W99-0210.6">Interlingual mechanism</entity> that we have called <entity id="W99-0210.7">Interlingual Slot Structure (ISS)</entity> based on <entity id="W99-0210.8">Slot Structure (SS)</entity> presented in Ferrfindez et al. (1997). The <entity id="W99-0210.9">SS</entity> stores the <entity id="W99-0210.10">lexical, syntactic, morphologic and semantic information</entity> of every <entity id="W99-0210.11">constituent</entity> of the <entity id="W99-0210.12">grammar</entity>. The <entity id="W99-0210.13">mechanism ISS</entity> allows us to translate <entity id="W99-0210.14">pronouns</entity> between different <entity id="W99-0210.15">languages</entity>. In this paper, we have proposed and evaluated <entity id="W99-0210.16">ISS</entity> for the <entity id="W99-0210.17">translation</entity> between <entity id="W99-0210.18">Spanish and English languages</entity>. We have compared <entity id="W99-0210.19">pronominal anaphora resolution</entity> both in <entity id="W99-0210.20">English</entity> and <entity id="W99-0210.21">Spanish</entity> to accomplish a study of the existing discrepancies between two <entity id="W99-0210.22">languages</entity>. This <entity id="W99-0210.23">mechanism</entity> could be added to a <entity id="W99-0210.24">MT system</entity> such as an additional module to solve <entity id="W99-0210.25">anaphora generation problem</entity>. 
</abstract>

</text>

<text id="W01-0906">
<title>Verification And Validation Of Language Processing Systems: Is It Evaluation?</title>
<abstract>
If <entity id="W01-0906.1">Natural Language Processing (NLP) systems</entity> are viewed as <entity id="W01-0906.2">intelligent systems</entity> then we should be able to make use of <entity id="W01-0906.3">verification and validation (V&amp;V) approaches</entity> and methods that have been developed in the <entity id="W01-0906.4">intelligent systems community</entity>. This paper addresses <entity id="W01-0906.5">language engineering infrastructure issues</entity> by considering whether <entity id="W01-0906.6">standard V&amp;V methods</entity> are fundamentally different than the <entity id="W01-0906.7">evaluation practices</entity> commonly used for <entity id="W01-0906.8">NLP systems</entity>, and proposes practical approaches for applying <entity id="W01-0906.9">V&amp;V</entity> in the context of <entity id="W01-0906.10">language processing systems</entity>. We argue that <entity id="W01-0906.11">evaluation</entity>, as it is performed in the <entity id="W01-0906.12">NL community</entity>, can be improved by supplementing it with methods from the <entity id="W01-0906.13">V&amp;V community</entity>.
</abstract>

</text>

<text id="W03-1102">
<title>A Practical Text Summarizer By Paragraph Extraction For Thai</title>
<abstract>
In this paper, we propose a practical approach for extracting the most relevant <entity id="W03-1102.1">paragraphs</entity> from the original <entity id="W03-1102.2">document</entity> to form a <entity id="W03-1102.3">summary</entity> for <entity id="W03-1102.4">Thai text</entity>. The idea of our approach is to exploit both the <entity id="W03-1102.5">local and global properties</entity> of <entity id="W03-1102.6">paragraphs</entity>. The <entity id="W03-1102.7">local property</entity> can be considered as <entity id="W03-1102.8">clusters</entity> of <entity id="W03-1102.9">significant words</entity> within each <entity id="W03-1102.10">paragraph</entity>, while the <entity id="W03-1102.11">global property</entity> can be thought of as <entity id="W03-1102.12">relations</entity> of all <entity id="W03-1102.13">paragraphs</entity> in a <entity id="W03-1102.14">document</entity>. These two <entity id="W03-1102.15">properties</entity> are combined for ranking and extracting <entity id="W03-1102.16">summaries</entity>. Experimental results on <entity id="W03-1102.17">real-world data</entity> sets are encouraging.
</abstract>

</text>

<text id="W08-0410">
<title>Inductive Detection of Language Features via Clustering Minimal Pairs: Toward Feature-Rich Grammars in Machine Translation</title>
<abstract><entity id="W08-0410.1">Syntax-based Machine Translation systems</entity> have recently become a focus of research with much hope that they will outperform <entity id="W08-0410.2">traditional Phrase- Based Statistical Machine Translation (PBSMT)</entity>. Toward this goal, we present a method for analyzing the <entity id="W08-0410.3">morphosyntactic content</entity> of <entity id="W08-0410.4">language</entity> from an <entity id="W08-0410.5">Elicitation Corpus</entity> such as the one included in the <entity id="W08-0410.6">LDC's</entity> upcoming <entity id="W08-0410.7">LCTL language packs</entity>. The presented method discovers a <entity id="W08-0410.8">mapping</entity> between <entity id="W08-0410.9">morphemes</entity> and <entity id="W08-0410.10">linguistically relevant features</entity>. By providing this tool that can augment <entity id="W08-0410.11">structure-based MT models</entity> with these <entity id="W08-0410.12">rich features</entity>, we believe the <entity id="W08-0410.13">discriminative power</entity> of current <entity id="W08-0410.14">models</entity> can be improved. We conclude by outlining how the resulting <entity id="W08-0410.15">output</entity> can then be used in inducing a <entity id="W08-0410.16">morphosyntactically feature-rich grammar</entity> for <entity id="W08-0410.17">AVENUE</entity>, a modern <entity id="W08-0410.18">syntax-based MT system</entity>.
</abstract>

</text>

<text id="P98-2243">
<title>How to Thematically Segment Texts by using Lexical Cohesion?</title>
<abstract>
This article outlines a <entity id="P98-2243.1">quantitative method</entity> for segmenting <entity id="P98-2243.2">texts</entity> into <entity id="P98-2243.3">thematically coherent units</entity>. This <entity id="P98-2243.4">method</entity> relies on a <entity id="P98-2243.5">network of lexical collocations</entity> to compute the <entity id="P98-2243.6">thematic coherence</entity> of the different parts of a <entity id="P98-2243.7">text</entity> from the <entity id="P98-2243.8">lexical cohesiveness</entity> of their <entity id="P98-2243.9">words</entity>. We also present the results of an experiment about locating <entity id="P98-2243.10">boundaries</entity> between a series of concatened <entity id="P98-2243.11">texts</entity>.
</abstract>

</text>

<text id="W04-1908">
<title>Automated Induction Of Sense In Context</title>
<abstract>
In this work, we introduce a <entity id="W04-1908.1">model</entity> for <entity id="W04-1908.2">sense assignment</entity> which relies on assigning <entity id="W04-1908.3">senses</entity> to the <entity id="W04-1908.4">contexts</entity> within which <entity id="W04-1908.5">words</entity> appear, rather than to the <entity id="W04-1908.6">words</entity> themselves. We argue that <entity id="W04-1908.7">word senses</entity> as such are not directly encoded in the <entity id="W04-1908.8">lexicon</entity> of the <entity id="W04-1908.9">language</entity>. Rather, each <entity id="W04-1908.10">word</entity> is associated with one or more <entity id="W04-1908.11">stereotypical syntagmatic patterns</entity>, which we call <entity id="W04-1908.12">selection contexts</entity>.
</abstract>

</text>

<text id="W06-0702">
<title>Challenges In Evaluating Summaries Of Short Stories</title>
<abstract>
This paper presents experiments with the <entity id="W06-0702.1">evaluation</entity> of automatically produced <entity id="W06-0702.2">summaries</entity> of literary short stories. The <entity id="W06-0702.3">summaries</entity> are tailored to a particular purpose of helping a reader decide whether she wants to read the story. The <entity id="W06-0702.4">evaluation procedure</entity> includes <entity id="W06-0702.5">extrinsic and intrinsic measures</entity>, as well as <entity id="W06-0702.6">subjective and factual judgments</entity> about the <entity id="W06-0702.7">summaries</entity> pronounced by human subjects. The experiments confirm the experience of summarizing more conventional <entity id="W06-0702.8">genres</entity>: <entity id="W06-0702.9">sentence overlap</entity> between <entity id="W06-0702.10">human- and machine-made summaries</entity> is not a complete picture of the <entity id="W06-0702.11">quality</entity> of a <entity id="W06-0702.12">summary</entity>. In fact, in our case, <entity id="W06-0702.13">sentence overlap</entity> does not correlate well with <entity id="W06-0702.14">human judgment</entity>. We explain the <entity id="W06-0702.15">evaluation procedures</entity> and discuss several challenges of evaluating <entity id="W06-0702.16">summaries</entity> of works of fiction.
</abstract>

</text>

<text id="W01-0512">
<title>The Unknown Word Problem: A Morphological Analysis Of Japanese Using Maximum Entropy Aided By A Dictionary</title>
<abstract>
In this paper we describe a <entity id="W01-0512.1">morphological analysis method</entity> based on a <entity id="W01-0512.2">maximum entropy model</entity>. This <entity id="W01-0512.3">method</entity> uses a <entity id="W01-0512.4">model</entity> that can not only consult a <entity id="W01-0512.5">dictionary</entity> with a large amount of <entity id="W01-0512.6">lexical information</entity> but can also identify <entity id="W01-0512.7">unknown words</entity> by learning certain <entity id="W01-0512.8">characteristics</entity>. The <entity id="W01-0512.9">model</entity> has the potential to overcome the <entity id="W01-0512.10">unknown word problem</entity>.
</abstract>

</text>

<text id="W06-1707">
<title>Corporator: A Tool For Creating RSS-Based Specialized Corpora</title>
<abstract>
This paper presents a new approach and a <entity id="W06-1707.1">software</entity> for collecting <entity id="W06-1707.2">specialized corpora</entity> on the Web. This approach takes advantage of a very popular <entity id="W06-1707.3">XML-based norm</entity> used on the Web for sharing content among websites: <entity id="W06-1707.4">RSS (Really Simple Syndication)</entity>. After a brief introduction to <entity id="W06-1707.5">RSS</entity>, we explain the interest of this type of <entity id="W06-1707.6">data sources</entity> in the framework of <entity id="W06-1707.7">corpus development</entity>. Finally, we present <entity id="W06-1707.8">Corporator</entity>, an <entity id="W06-1707.9">Open Source software</entity> which was designed for collecting <entity id="W06-1707.10">corpus</entity> from <entity id="W06-1707.11">RSS feeds</entity>.
</abstract>

</text>

<text id="P07-2015">
<title>Support Vector Machines for Query-focused Summarization trained and evaluated on Pyramid data</title>
<abstract>
This paper presents the use of <entity id="P07-2015.1">Support Vector Machines (SVM)</entity> to detect relevant information to be included in a <entity id="P07-2015.2">query-focused summary</entity>. Several <entity id="P07-2015.3">SVMs</entity> are trained using information from pyramids of <entity id="P07-2015.4">summary content units</entity>. Their <entity id="P07-2015.5">performance</entity> is compared with the best performing systems in <entity id="P07-2015.6">DUC-2005</entity>, using both <entity id="P07-2015.7">ROUGE</entity> and <entity id="P07-2015.8">autoPan</entity>, an <entity id="P07-2015.9">automatic scoring method</entity> for <entity id="P07-2015.10">pyramid evaluation</entity>.
</abstract>

</text>

<text id="W08-1105">
<title>Dependency Tree Based Sentence Compression</title>
<abstract>
We present a novel <entity id="W08-1105.1">unsupervised method</entity> for <entity id="W08-1105.2">sentence compression</entity> which relies on a <entity id="W08-1105.3">dependency tree representation</entity> and shortens <entity id="W08-1105.4">sentences</entity> by removing <entity id="W08-1105.5">subtrees</entity>. An <entity id="W08-1105.6">automatic evaluation</entity> shows that our <entity id="W08-1105.7">method</entity> obtains result comparable or superior to the state of the art. We demonstrate that the choice of the <entity id="W08-1105.8">parser</entity> affects the <entity id="W08-1105.9">performance</entity> of the <entity id="W08-1105.10">system</entity>. We also apply the <entity id="W08-1105.11">method</entity> to <entity id="W08-1105.12">German</entity> and report the results of an evaluation with humans.
</abstract>

</text>

<text id="P05-2005">
<title>Exploiting Named Entity Taggers In A Second Language</title>
<abstract>
In this work we present a method for <entity id="P05-2005.1">Named Entity Recognition (NER)</entity>. Our method does not rely on complex <entity id="P05-2005.2">linguistic resources</entity>, and apart from a hand coded system, we do not use any <entity id="P05-2005.3">language-dependent tools</entity>. The only information we use is automatically extracted from the <entity id="P05-2005.4">documents</entity>, without human intervention. Moreover, the method performs well even without the use of the hand coded system. The experimental results are very encouraging. Our approach even outperformed the hand coded system on <entity id="P05-2005.5">NER</entity> in <entity id="P05-2005.6">Spanish</entity>, and it achieved high <entity id="P05-2005.7">accuracies</entity> in <entity id="P05-2005.8">Portuguese</entity>.
</abstract>

</text>

<text id="C90-3005">
<title>A Karaka Based Approach To Parsing Of Indian Languages</title>
<abstract>
A <entity id="C90-3005.1">karaka based approach</entity> to <entity id="C90-3005.2">parsing</entity> of <entity id="C90-3005.3">Indian languages</entity> is described. It has been used for building a <entity id="C90-3005.4">parser</entity> of <entity id="C90-3005.5">Hindi</entity> for a <entity id="C90-3005.6">prototype Machine Translation system</entity>. A <entity id="C90-3005.7">lexicalized grammar formalism</entity> has been developed that allows <entity id="C90-3005.8">constraints</entity> to bo specified between 'demand' and 'source' words (e.g., between <entity id="C90-3005.9">verb</entity> and its <entity id="C90-3005.10">karaka roles</entity>). The <entity id="C90-3005.11">parser</entity> has two important novel features: (i) It has a <entity id="C90-3005.12">local word grouping phase</entity> in which <entity id="C90-3005.13">word groups</entity> are formed using <entity id="C90-3005.14">'local' information</entity> only. They are formed based on <entity id="C90-3005.15">finite state machine specifications</entity> thus resulting in a fast grouper. (ii) The <entity id="C90-3005.16">parser</entity> is a <entity id="C90-3005.17">general constraint solver</entity>. It first transforms the <entity id="C90-3005.18">constraints</entity> to an <entity id="C90-3005.19">integer programming problem</entity> and then solves it.
</abstract>

</text>

<text id="L08-1559">
<title>Automatic Identification of Temporal Information in Tourism Web Pages</title>
<abstract>
This paper presents our work on the <entity id="L08-1559.1">detection</entity> of <entity id="L08-1559.2">temporal information</entity> in <entity id="L08-1559.3">web pages</entity>. The <entity id="L08-1559.4">pages</entity> examined within the scope of this study were taken from the tourism sector and the <entity id="L08-1559.5">temporal information</entity> in question is thus particular to this area. The differences that exist between <entity id="L08-1559.6">extraction</entity> from <entity id="L08-1559.7">plain textual data</entity> and <entity id="L08-1559.8">extraction</entity> from the web are brought to light. These differences mainly concern the spatial arrangement of the <entity id="L08-1559.9">text</entity>, the use of <entity id="L08-1559.10">punctuation</entity> and the respect of traditional <entity id="L08-1559.11">syntactic rules</entity>. The <entity id="L08-1559.12">temporal expressions</entity> to be extracted are classified into two kinds: <entity id="L08-1559.13">temporal information</entity> that concerns one particular event and <entity id="L08-1559.14">repetitive temporal information</entity>. We adopt a <entity id="L08-1559.15">symbolic approach</entity> relying on <entity id="L08-1559.16">patterns</entity> and <entity id="L08-1559.17">rules</entity> for the <entity id="L08-1559.18">detection</entity>, <entity id="L08-1559.19">extraction</entity> and <entity id="L08-1559.20">annotation</entity> of <entity id="L08-1559.21">temporal expressions</entity>; our method is based on the use of <entity id="L08-1559.22">transducers</entity>. First evaluations have shown promising results. Since the visual structure of a <entity id="L08-1559.23">web page</entity> is very important and often informs the user before he has even read the <entity id="L08-1559.24">text</entity>, a <entity id="L08-1559.25">semiotic study</entity> is also presented in this paper.
</abstract>

</text>

<text id="L08-1196">
<title>F0 of Adolescent Speakers - First Results for the German Ph@ttSessionz Database</title>
<abstract>
The first release of the <entity id="L08-1196.1">German Ph@ttSessionz speech database</entity> contains <entity id="L08-1196.2">read and spontaneous speech</entity> from 864 <entity id="L08-1196.3">adolescent speakers</entity> and is the largest <entity id="L08-1196.4">database</entity> of its kind for <entity id="L08-1196.5">German</entity>. It was recorded via the WWW in over 40 public schools in all <entity id="L08-1196.6">dialect regions</entity> of Germany. In this paper, we present a <entity id="L08-1196.7">cross-sectional study</entity> of <entity id="L08-1196.8">f0 measurements</entity> on this <entity id="L08-1196.9">database</entity>. The <entity id="L08-1196.10">study</entity> documents the profound changes in male voices at the age 13-15. Furthermore, it shows that on a perceptive mel-scale, there is little difference in the <entity id="L08-1196.11">relative f0 variability</entity> for <entity id="L08-1196.12">male and female speakers</entity>. A closer analysis reveals that <entity id="L08-1196.13">f0 variability</entity> is dependent on the <entity id="L08-1196.14">speech style</entity> and both the length and the type of the <entity id="L08-1196.15">utterance</entity>. The <entity id="L08-1196.16">study</entity> provides statistically reliable <entity id="L08-1196.17">voice parameters</entity> of <entity id="L08-1196.18">adolescent speakers</entity> for <entity id="L08-1196.19">German</entity>. The results may contribute to making <entity id="L08-1196.20">spoken dialog systems</entity> more robust by restricting <entity id="L08-1196.21">user input</entity> to <entity id="L08-1196.22">utterances</entity> with low <entity id="L08-1196.23">f0 variability</entity>.
</abstract>

</text>

<text id="C96-2185">
<title>Korean Language Engineering: Current Status Of The Information Platform</title>
<abstract><entity id="C96-2185.1">Language engineering</entity> implements functions of a <entity id="C96-2185.2">language</entity> and <entity id="C96-2185.3">information</entity> via <entity id="C96-2185.4">computers</entity>. The need for <entity id="C96-2185.5">language engineering platforms</entity> has been generally recognized and several researches are being undertaken around the world. Our goal is to establish <entity id="C96-2185.6">Korean information platform of linguistic resources and tools</entity> for <entity id="C96-2185.7">Korean language</entity> and information communities. The <entity id="C96-2185.8">platform</entity> will support researchers and engineers with well-developed and <entity id="C96-2185.9">standardized resources</entity> and <entity id="C96-2185.10">application tools</entity> thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain. This paper reports the <entity id="C96-2185.11">components</entity> and the current status of the project, and the importance of the effort.
</abstract>

</text>

<text id="E83-1003">
<title>Iterative Operations</title>
<abstract>
We present in this article, as a part of <entity id="E83-1003.1">aspectual operation system</entity>, a <entity id="E83-1003.2">generation system</entity> of <entity id="E83-1003.3">iterative expressions</entity> using a set of <entity id="E83-1003.4">operators</entity> called <entity id="E83-1003.5">iterative operators</entity>. In order to execute the <entity id="E83-1003.6">iterative operations</entity> efficiently, we have classified previously <entity id="E83-1003.7">propositions</entity> denoting a single <entity id="E83-1003.8">occurrence</entity> of a <entity id="E83-1003.9">single event</entity> into three groupes. The definition of a <entity id="E83-1003.10">single event</entity> is given recursively. The classification has been carried out especially in consideration of the <entity id="E83-1003.11">durative / non-durative character</entity> of the denoted <entity id="E83-1003.12">events</entity> and also in consideration of existence / non-existence of a culmination point (or a <entity id="E83-1003.13">boundary</entity>) in the <entity id="E83-1003.14">events</entity>. The <entity id="E83-1003.15">operations</entity> concerned with <entity id="E83-1003.16">iteration</entity> have either the effect of giving a <entity id="E83-1003.17">boundary</entity> to an <entity id="E83-1003.18">event</entity> ( in the case of a <entity id="E83-1003.19">non-bounded event</entity>) or of extending an <entity id="E83-1003.20">event</entity> through <entity id="E83-1003.21">repetitions</entity>. The <entity id="E83-1003.22">operators</entity> concerned are: N,F.. <entity id="E83-1003.23">direct iterative operators</entity>; I,G.. <entity id="E83-1003.24">boundary giving operators</entity>; I.. <entity id="E83-1003.25">extending operator</entity>. There are <entity id="E83-1003.26">direct and indirect operations</entity>: the direct ones change a <entity id="E83-1003.27">non-repetitious proposition</entity> into a repetitious one directly, whereas the indirect ones change it indirectly. The <entity id="E83-1003.28">indirect iteration</entity> is indicated with S. The <entity id="E83-1003.29">scope</entity> of each <entity id="E83-1003.30">operator</entity> is not uniquely definable, though the <entity id="E83-1003.31">mutual relation</entity> of the <entity id="E83-1003.32">operators</entity> can be given more or less explicitly.
</abstract>

</text>

<text id="H94-1080">
<title>
A <entity id="H94-1080.1">One Pass Decoder Design</entity> For <entity id="H94-1080.2">Large Vocabulary Recognition</entity></title>
<abstract>
To achieve reasonable <entity id="H94-1080.3">accuracy</entity> in <entity id="H94-1080.4">large vocabulary speech recognition systems</entity>, it is important to use detailed <entity id="H94-1080.5">acoustic models</entity> together with good <entity id="H94-1080.6">long span language models</entity>. For example, in the <entity id="H94-1080.7">Wall Street Journal (WSJ) task</entity> both <entity id="H94-1080.8">cross-word triphones</entity> and a <entity id="H94-1080.9">trigram language model</entity> are necessary to achieve <entity id="H94-1080.10">state-of-the-art performance</entity>. However, when using these <entity id="H94-1080.11">models</entity>, the size of a <entity id="H94-1080.12">pre-compiled recognition network</entity> can make a <entity id="H94-1080.13">standard Viterbi earch</entity> infeasible and hence, either <entity id="H94-1080.14">multiple-pass or asynchronous stack decoding schemes</entity> are typically used. In this paper, we show that <entity id="H94-1080.15">time-synchronous one-pass decoding</entity> using <entity id="H94-1080.16">cross-word triphones</entity> and a <entity id="H94-1080.17">trigram language model</entity> can be implemented using a dynamically built <entity id="H94-1080.18">tree-structured network</entity>. This approach avoids the compromises inherent in using <entity id="H94-1080.19">fast-matches</entity> or preliminary passes and is relatively efficient in implementation. It was included in the <entity id="H94-1080.20">HTK large vocabulary speech recognition system</entity> used for the <entity id="H94-1080.21">1993 ARPA WSJ evaluation</entity> and experimental results are presented for that task.
</abstract>

</text>

<text id="D08-1090">
<title>Language and Translation Model Adaptation using Comparable Corpora</title>
<abstract>
Traditionally, <entity id="D08-1090.1">statistical machine translation systems</entity> have relied on <entity id="D08-1090.2">parallel bi-lingual data</entity> to train a <entity id="D08-1090.3">translation model</entity>. While <entity id="D08-1090.4">bi-lingual parallel data</entity> are expensive to generate, <entity id="D08-1090.5">monolingual data</entity> are relatively common. Yet <entity id="D08-1090.6">monolingual data</entity> have been under-utilized, having been used primarily for training a <entity id="D08-1090.7">language model</entity> in the <entity id="D08-1090.8">target language</entity>. This paper describes a novel method for utilizing <entity id="D08-1090.9">monolingual target data</entity> to improve the performance of a <entity id="D08-1090.10">statistical machine translation system</entity> on news stories. The method exploits the existence of <entity id="D08-1090.11">comparable text</entity> -
multiple <entity id="D08-1090.12">texts</entity> in the <entity id="D08-1090.13">target language</entity> that discuss the same or similar stories as found in the <entity id="D08-1090.14">source language document</entity>. For every <entity id="D08-1090.15">source document</entity> that is to be translated, a large <entity id="D08-1090.16">monolingual data set</entity> in the <entity id="D08-1090.17">target language</entity> is searched for <entity id="D08-1090.18">documents</entity> that might be comparable to the<entity id="D08-1090.19">source documents</entity>. These <entity id="D08-1090.20">documents</entity> are then used to adapt the <entity id="D08-1090.21">MT system</entity> to increase the probability of generating texts that resemble the <entity id="D08-1090.22">comparable document</entity>. Experimental results obtained by adapting both the <entity id="D08-1090.23">language and translation models</entity> show substantial gains over the <entity id="D08-1090.24">baseline system</entity>.
</abstract>

</text>

<text id="E06-2007">
<title>Selecting The "Right" Number Of Senses Based On Clustering Criterion Functions</title>
<abstract>
This paper describes an <entity id="E06-2007.1">unsupervised knowledge-lean methodology</entity> for automatically determining the number of <entity id="E06-2007.2">senses</entity> in which an <entity id="E06-2007.3">ambiguous word</entity> is used in a large <entity id="E06-2007.4">corpus</entity>. It is based on the use of global <entity id="E06-2007.5">criterion functions</entity> that assess the quality of a <entity id="E06-2007.6">clustering solution</entity>.
</abstract>

</text>

<text id="M91-1032">
<title>UNISYS: Description Of The UNISYS System Used For MUC-3</title>
<abstract>
This paper describes the <entity id="M91-1032.1">Unisys MUC-3 text understanding system</entity>, a <entity id="M91-1032.2">system</entity> based upon a <entity id="M91-1032.3">three-tiered approach</entity> to <entity id="M91-1032.4">text processing</entity> in which a powerful <entity id="M91-1032.5">knowledge-based form</entity> of <entity id="M91-1032.6">information retrieval</entity> plays a central role. This <entity id="M91-1032.7">knowledge-based form</entity> of <entity id="M91-1032.8">information retrieval</entity> makes it possible to define an effective level of <entity id="M91-1032.9">text analysis</entity> that falls somewhere between what is possible with standard <entity id="M91-1032.10">keyword-based information retrieval techniques</entity> and <entity id="M91-1032.11">deep linguistic analysis</entity>. The Unisys Center for Advanced Information Technology (CAIT) has a long-standing commitment to <entity id="M91-1032.12">NLP research and development</entity>, with the <entity id="M91-1032.13">Pundit NLP system</entity> developed at CAIT serving as the Center's primary research vehicle [3]. The <entity id="M91-1032.14">Unisys MUC-3 system</entity>, however, consists primarily of components that are less than 7 months old and still in a developmental stage. Although the <entity id="M91-1032.15">three-tiered processing approach</entity> that the <entity id="M91-1032.16">MUC-3 system's architecture</entity> is based upon includes <entity id="M91-1032.17">Pundit</entity> as its third <entity id="M91-1032.18">level of (linguistic) analysis</entity>, the incorporation of <entity id="M91-1032.19">Pundit</entity> into the <entity id="M91-1032.20">MUC-3 system</entity> was not achieved in time for the final <entity id="M91-1032.21">MUC-3 test</entity> in May 1991. A decision was made to focus on the development of a <entity id="M91-1032.22">knowledge-based information retrieval component</entity>, and this precluded the integration of <entity id="M91-1032.23">Pundit</entity> into the <entity id="M91-1032.24">prototype</entity>.
</abstract>

</text>

<text id="A00-1026">
<title>Extracting Molecular Binding Relationships From Biomedical Text</title>
<abstract>
<entity id="A00-1026.1">ARBITER</entity> is a <entity id="A00-1026.2">Prolog program</entity> that extracts assertions about <entity id="A00-1026.3">macromolecular binding relationships</entity> from <entity id="A00-1026.4">biomedical text</entity>. We describe the <entity id="A00-1026.5">domain knowledge</entity> and the under-specified <entity id="A00-1026.6">linguistic analyses</entity> that support the identification of these <entity id="A00-1026.7">predications</entity>. After discussing a formal evaluation of <entity id="A00-1026.8">ARBITER</entity>, we report on its application to 491,000 <entity id="A00-1026.9">MEDLINE abstracts</entity>, during which almost 25,000 <entity id="A00-1026.10">binding relationships</entity> suitable for entry into a <entity id="A00-1026.11">database</entity> of <entity id="A00-1026.12">macro-molecular function</entity> were extracted.
</abstract>

</text>

<text id="H92-1046">
<title>Lexical Disambiguation Using Simulated Annealing</title>
<abstract>
The <entity id="H92-1046.1">resolution</entity> of <entity id="H92-1046.2">lexical ambiguity</entity> is important for most <entity id="H92-1046.3">natural language processing tasks</entity>, and a range of <entity id="H92-1046.4">computational techniques</entity> have been proposed for its solution. None of these has yet proven effective on a large scale. In this paper, we describe a method for <entity id="H92-1046.5">lexical disambiguation</entity> of <entity id="H92-1046.6">text</entity> using the <entity id="H92-1046.7">definitions</entity> in a <entity id="H92-1046.8">machine-readable dictionary</entity> together with the technique of <entity id="H92-1046.9">simulated annealing</entity>. The method operates on complete <entity id="H92-1046.10">sentences</entity> and attempts to select the optimal combinations of <entity id="H92-1046.11">word senses</entity> for all the <entity id="H92-1046.12">words</entity> in the <entity id="H92-1046.13">sentence</entity> simultaneously. The <entity id="H92-1046.14">words</entity> in the <entity id="H92-1046.15">sentences</entity> may be any of the 28,000 <entity id="H92-1046.16">headwords</entity> in <entity id="H92-1046.17">Longman 's Dictionary of Contemporary English (LDOCE)</entity> and are disambiguated relative to the <entity id="H92-1046.18">senses</entity> given in <entity id="H92-1046.19">LDOCE</entity>. Our initial results on a sample set of 50 <entity id="H92-1046.20">sentences</entity> are comparable to those of other <entity id="H92-1046.21">researchers</entity>, and the fully <entity id="H92-1046.22">automatic method</entity> requires no <entity id="H92-1046.23">hand coding</entity> of <entity id="H92-1046.24">lexical entries</entity>, or <entity id="H92-1046.25">hand tagging</entity> of <entity id="H92-1046.26">text</entity>.
</abstract>

</text>

<text id="H01-1015">
<title>DATE: A Dialogue Act Tagging Scheme For Evaluation Of Spoken Dialogue Systems</title>
<abstract>
This paper describes a <entity id="H01-1015.1">dialogue act tagging scheme</entity> developed for the purpose of providing finer-grained quantitative <entity id="H01-1015.2">dialogue metrics</entity> for comparing and evaluating <entity id="H01-1015.3">DARPA COMMUNICATOR spoken dialogue systems</entity>. We show that these <entity id="H01-1015.4">dialogue act metrics</entity> can be used to quantify the amount of effort spent in a <entity id="H01-1015.5">dialogue</entity> maintaining the <entity id="H01-1015.6">channel of communication</entity> or, establishing the frame for communication, as opposed to actually carrying out the travel planning task that the system is designed to support. We show that the use of these <entity id="H01-1015.7">metrics</entity> results in a 7% <entity id="H01-1015.8">improvement</entity> in the <entity id="H01-1015.9">fit</entity> in <entity id="H01-1015.10">models</entity> of <entity id="H01-1015.11">user satisfaction</entity>. We suggest that <entity id="H01-1015.12">dialogue act metrics</entity> can ultimately support more focused qualitative analysis of the role of various <entity id="H01-1015.13">dialogue strategy parameters</entity>, e.g. <entity id="H01-1015.14">initiative</entity>, across <entity id="H01-1015.15">dialogue systems</entity>, thus clarifying what development paths might be feasible for enhancing <entity id="H01-1015.16">user satisfaction</entity> in future versions of these <entity id="H01-1015.17">systems</entity>.
</abstract>

</text>

<text id="W97-0909">
<title>NLP And Industry: Transfer And Reuse Of Technologies</title>
<abstract>
This paper describes a useful set of <entity id="W97-0909.1">NLP tools</entity> which has been successfully applied to many different kinds of industrial requirements spanning multiple domains and applications at Boeing. The tools can be combined to constitute a full-spectrum <entity id="W97-0909.2">natural language system</entity> and can be customized for new domains relatively easily. To date, this array of <entity id="W97-0909.3">formal and natural language processing technologies</entity> has been used to perform mass changes to <entity id="W97-0909.4">legacy textual databases</entity> and to facilitate <entity id="W97-0909.5">user interfacing</entity> to <entity id="W97-0909.6">relational databases</entity> and <entity id="W97-0909.7">software applications</entity>.
</abstract>

</text>

<text id="W00-1403">
<title>An Empirical Study Of Multilingual Natural Language Generation: What Should A Text Planner Do?</title>
<abstract>
We present <entity id="W00-1403.1">discourse annotation</entity> work aimed at constructing a <entity id="W00-1403.2">parallel corpus</entity> of <entity id="W00-1403.3">Rhetorical Structure trees</entity> for a collection of <entity id="W00-1403.4">Japanese texts</entity> and their corresponding <entity id="W00-1403.5">English translations</entity>. We discuss implications of our empirical findings for the task of <entity id="W00-1403.6">text planning</entity> in the context of implementing <entity id="W00-1403.7">multilingual natural language generation systems</entity>.
</abstract>

</text>

<text id="W03-0903">
<title>Less Is More: Using A Single Knowledge Representation In Dialogue Systems</title>
<abstract>
The approach to <entity id="W03-0903.1">knowledge representation</entity> taken in a <entity id="W03-0903.2">multi-modal multi-domain dialogue system - SmartKom -</entity> is presented. We focus on the <entity id="W03-0903.3">ontological and representational issues</entity> and choices helping to construct an <entity id="W03-0903.4">ontology</entity>, which is shared by multiple <entity id="W03-0903.5">components of the system</entity>, can be re-used in different projects and applied to various tasks. Finally, examples highlighting the usefulness of our <entity id="W03-0903.6">approach</entity> are given.
</abstract>

</text>

<text id="W08-0216">
<title>Support Collaboration by Teaching Fundamentals</title>
<abstract>
This paper argues for teaching <entity id="W08-0216.1">computer science</entity> to <entity id="W08-0216.2">linguists</entity> through a general course at the introductory graduate level whose goal is to prepare students of all backgrounds for collaborative <entity id="W08-0216.3">computational research</entity>, especially in the <entity id="W08-0216.4">sciences</entity>. We describe our work over the past three years in creating a <entity id="W08-0216.5">model course</entity> in the area, called <entity id="W08-0216.6">Computational Thinking</entity>.
</abstract>

</text>

<text id="P98-2136">
<title>Confirmation in Multimodal Systems</title>
<abstract>Systems that attempt to understand <entity id="P98-2136.1">natural human input</entity> make mistakes, even humans. However, humans avoid misunderstandings by confirming doubtful <entity id="P98-2136.2">input</entity>. <entity id="P98-2136.3">Multimodal systems</entity> - those that combine <entity id="P98-2136.4">simultaneous input</entity> from more than one <entity id="P98-2136.5">modality</entity>, for example <entity id="P98-2136.6">speech</entity> and <entity id="P98-2136.7">gesture</entity> - have historically been designed so that they either request confirmation of <entity id="P98-2136.8">speech</entity>, their primary <entity id="P98-2136.9">modality</entity>, or not at all. Instead, we experimented with delaying confirmation until after the <entity id="P98-2136.10">speech</entity> and <entity id="P98-2136.11">gesture</entity> were combined into a complete <entity id="P98-2136.12">multimodal command</entity>. In controlled experiments, subjects achieved more <entity id="P98-2136.13">commands</entity> per minute at a lower <entity id="P98-2136.14">error rate</entity> when the <entity id="P98-2136.15">system</entity> delayed confirmation, than compared to when subjects confirmed only <entity id="P98-2136.16">speech</entity>. In addition, this style of late confirmation meets the user's expectation that confirmed <entity id="P98-2136.17">commands</entity> should be executable.</abstract>

</text>

<text id="W04-0902">
<title>Solving Logic Puzzles: From Robust Processing To Precise Semantics</title>
<abstract>
This paper presents intial work on a <entity id="W04-0902.1">system</entity> that bridges from <entity id="W04-0902.2">robust, broad-coverage natural language processing</entity> to <entity id="W04-0902.3">precise semantics</entity> and <entity id="W04-0902.4">automated reasoning</entity>, focusing on solving <entity id="W04-0902.5">logic puzzles</entity> drawn from sources such as the <entity id="W04-0902.6">Law School Admission Test (LSAT)</entity> and the analytic section of the <entity id="W04-0902.7">Graduate Record Exam (GRE)</entity>. We highlight key challenges, and discuss the <entity id="W04-0902.8">representations</entity> and <entity id="W04-0902.9">performance</entity> of the <entity id="W04-0902.10">prototype system</entity>.
</abstract>

</text>

<text id="W06-0113">
<title>A SVM-Based Model For Chinese Functional Chunk Parsing</title>
<abstract>
<entity id="W06-0113.1">Functional chunks</entity> are defined as a series of <entity id="W06-0113.2">non-overlapping, non-nested segments of text</entity> in a <entity id="W06-0113.3">sentence</entity>, representing the <entity id="W06-0113.4">implicit grammatical relations</entity> between the <entity id="W06-0113.5">sentence-level predicates</entity> and their <entity id="W06-0113.6">arguments</entity>. Its <entity id="W06-0113.7">top-down scheme</entity> and complexity of <entity id="W06-0113.8">internal constitutions</entity> bring in a new challenge for <entity id="W06-0113.9">automatic parser</entity>. In this paper, a new <entity id="W06-0113.10">parsing model</entity> is proposed to formulate the complete <entity id="W06-0113.11">chunking problem</entity> as a series of <entity id="W06-0113.12">boundary detection</entity> subtasks. Each of these subtasks is only in charge of detecting one type of the <entity id="W06-0113.13">chunk boundaries</entity>. As each subtask could be modeled as a <entity id="W06-0113.14">binary classification problem</entity>, a lot of <entity id="W06-0113.15">machine learning techniques</entity> could be applied. In our experiments, we only focus on the <entity id="W06-0113.16">subject-predicate (SP) and predicate-object (PO) boundary detection</entity> subtasks. By applying <entity id="W06-0113.17">SVM algorithm</entity> to these subtasks, we have achieved the best <entity id="W06-0113.18">F-Score</entity> of 76.56% and 82.26% respectively.
</abstract>

</text>

<text id="W06-2501">
<title>Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts</title>
<abstract>
In this paper, we introduce a <entity id="W06-2501.1">WordNet-based measure</entity> of <entity id="W06-2501.2">semantic relatedness</entity> by combining the structure and content of <entity id="W06-2501.3">WordNet</entity> with <entity id="W06-2501.4">co-occurrence information</entity> derived from <entity id="W06-2501.5">raw text</entity>. We use the <entity id="W06-2501.6">co-occurrence information</entity> along with the <entity id="W06-2501.7">WordNet definitions</entity> to build <entity id="W06-2501.8">gloss vectors</entity> corresponding to each <entity id="W06-2501.9">concept</entity> in <entity id="W06-2501.10">WordNet</entity>. <entity id="W06-2501.11">Numeric scores of relatedness</entity> are assigned to a pair of <entity id="W06-2501.12">concepts</entity> by measuring the <entity id="W06-2501.13">cosine of the angle</entity> between their respective <entity id="W06-2501.14">gloss vectors</entity>. We show that this <entity id="W06-2501.15">measure</entity> compares favorably to other measures with respect to <entity id="W06-2501.16">human judgments of semantic relatedness</entity>, and that it performs well when used in a <entity id="W06-2501.17">word sense disambiguation algorithm</entity> that relies on <entity id="W06-2501.18">semantic relatedness</entity>. This <entity id="W06-2501.19">measure</entity> is flexible in that it can make comparisons between any two <entity id="W06-2501.20">concepts</entity> without regard to their <entity id="W06-2501.21">part of speech</entity>. In addition, it can be adapted to different domains, since any <entity id="W06-2501.22">plain text corpus</entity> can be used to derive the <entity id="W06-2501.23">co–occurrence information</entity>.
</abstract>

</text>

<text id="W07-1403">
<title>Precision-focused Textual Inference</title>
<abstract>
This paper describes our <entity id="W07-1403.1">system</entity> as used in the <entity id="W07-1403.2">RTE3 task</entity>. The <entity id="W07-1403.3">system</entity> maps <entity id="W07-1403.4">premise and hypothesis pairs</entity> into an <entity id="W07-1403.5">abstract knowledge representation (AKR)</entity> and then performs <entity id="W07-1403.6">entailment and contradiction detection (ecd)</entity> on the resulting <entity id="W07-1403.7">AKRs</entity>. Two versions of <entity id="W07-1403.8">ECD</entity> were used in <entity id="W07-1403.9">RTE3</entity>, one with <entity id="W07-1403.10">strict ECD</entity> and one with <entity id="W07-1403.11">looser ECD</entity>.
</abstract>

</text>

<text id="P07-1105">
<title>Grammar Approximation by Representative Sublanguage : A New Model for Language Learning</title>
<abstract>
We propose a new <entity id="P07-1105.1">language learning model</entity> that learns a <entity id="P07-1105.2">syntactic-semantic grammar</entity> from a small number of <entity id="P07-1105.3">natural language strings</entity> annotated with their <entity id="P07-1105.4">semantics</entity>, along with <entity id="P07-1105.5">basic assumptions</entity> about <entity id="P07-1105.6">natural language syntax</entity>. We show that the <entity id="P07-1105.7">search space</entity> for <entity id="P07-1105.8">grammar induction</entity> is a <entity id="P07-1105.9">complete grammar lattice</entity>, which guarantees the uniqueness of the learned <entity id="P07-1105.10">grammar</entity>.
</abstract>

</text>

<text id="C08-1062">
<title>PNR2: Ranking Sentences with Positive and Negative Reinforcement for Query-Oriented Update Summarization</title>
<abstract><entity id="C08-1062.1">Query-oriented update summarization</entity> is an emerging <entity id="C08-1062.2">summarization task</entity> very recently. It brings new challenges to the <entity id="C08-1062.3">sentence ranking algorithms</entity> that require not only to locate the important and <entity id="C08-1062.4">query-relevant information</entity>, but also to capture the new <entity id="C08-1062.5">information</entity> when <entity id="C08-1062.6">document collections</entity> evolve. In this paper, we propose a novel <entity id="C08-1062.7">graph based sentence ranking algorithm</entity>, namely <entity id="C08-1062.8">PNR2</entity>, for <entity id="C08-1062.9">update summarization</entity>. Inspired by the intuition that “a <entity id="C08-1062.10">sentence</entity> receives a <entity id="C08-1062.11">positive influence</entity> from the <entity id="C08-1062.12">sentences</entity> that correlate to it in the same <entity id="C08-1062.13">collection</entity>, whereas a <entity id="C08-1062.14">sentence</entity> receives a <entity id="C08-1062.15">negative influence</entity> from the <entity id="C08-1062.16">sentences</entity> that correlates to it in the different (perhaps previously read) <entity id="C08-1062.17">collection</entity>”, <entity id="C08-1062.18">PNR2</entity> models both the <entity id="C08-1062.19">positive and the negative mutual reinforcement</entity> in the <entity id="C08-1062.20">ranking process</entity>. Automatic evaluation on the <entity id="C08-1062.21">DUC 2007 data set pilot task</entity> demonstrates the effectiveness of the <entity id="C08-1062.22">algorithm</entity>. 
</abstract>

</text>

<text id="P04-1038">
<title>Chinese Verb Sense Discrimination Using An EM Clustering Model With Rich Linguistic Features</title>
<abstract>
This paper discusses the application of the <entity id="P04-1038.1">Expectation-Maximization (EM) clustering algorithm</entity> to the task of <entity id="P04-1038.2">Chinese verb sense discrimination</entity>. The <entity id="P04-1038.3">model</entity> utilized <entity id="P04-1038.4">rich linguistic features</entity> that capture <entity id="P04-1038.5">predicate-argument structure information</entity> of the <entity id="P04-1038.6">target verbs</entity>. A <entity id="P04-1038.7">semantic taxonomy</entity> for <entity id="P04-1038.8">Chinese nouns</entity>, which was built semi-automatically based on two <entity id="P04-1038.9">electronic Chinese semantic dictionaries</entity>, was used to provide <entity id="P04-1038.10">semantic features</entity> for the <entity id="P04-1038.11">model</entity>. <entity id="P04-1038.12">Purity</entity> and <entity id="P04-1038.13">normalized mutual information</entity> were used to evaluate the <entity id="P04-1038.14">clustering performance</entity> on 12 <entity id="P04-1038.15">Chinese verbs</entity>. The experimental results show that the <entity id="P04-1038.16">EM clustering model</entity> can learn <entity id="P04-1038.17">sense or sense group distinctions</entity> for most of the <entity id="P04-1038.18">verbs</entity> successfully. We further enhanced the <entity id="P04-1038.19">model</entity> with certain <entity id="P04-1038.20">fine-grained semantic categories</entity> called <entity id="P04-1038.21">lexical sets</entity>. Our results indicate that these <entity id="P04-1038.22">lexical sets</entity> improve the <entity id="P04-1038.23">model</entity>'s performance for the three most challenging <entity id="P04-1038.24">verbs</entity> chosen from the first set of experiments.
</abstract>

</text>

<text id="C88-2156">
<title>Figuring Out Most Plausible Interpretation From Spatial Descriptions</title>
<abstract>
The problem we want to handle in this paper is vagueness. A notion of <entity id="C88-2156.1">space</entity>, which we basically have, plays an important part in the faculty of thinking and <entity id="C88-2156.2">speech</entity>. In this paper, we concentrate on a particular class of <entity id="C88-2156.3">spatial descriptions</entity>, namely <entity id="C88-2156.4">descriptions</entity> about <entity id="C88-2156.5">positional relations</entity> on a <entity id="C88-2156.6">two-dimensional space</entity>. A <entity id="C88-2156.7">theoretical device</entity> we present in this paper is called the <entity id="C88-2156.8">potential model</entity>. The <entity id="C88-2156.9">potential model</entity> provides a means for accumulating from <entity id="C88-2156.10">fragmentary information</entity>. It is possible to derive <entity id="C88-2156.11">maximally plausible interpretation</entity> from a <entity id="C88-2156.12">chunk of information</entity> accumulated in the <entity id="C88-2156.13">model</entity>. When new <entity id="C88-2156.14">information</entity> is given, the potential <entity id="C88-2156.15">model</entity> is modelled so that that new <entity id="C88-2156.16">information</entity> is taken into account. As a result, the <entity id="C88-2156.17">interpretations</entity> with <entity id="C88-2156.18">maximal plausibility</entity> may change. A program called <entity id="C88-2156.19">SPRINT (SPatial Relation INTerpreter)</entity> reflecting our theory is in the way of construction.
</abstract>

</text>

<text id="C88-1082">
<title>Linguistic Processing Using A Dependency Structure Grammar For Speech Recognition And Understanding</title>
<abstract>
This paper proposes an efficient <entity id="C88-1082.1">linguistic processing strategy</entity> for <entity id="C88-1082.2">speech recognition and understanding</entity> using a <entity id="C88-1082.3">dependency structure grammar</entity>. The <entity id="C88-1082.4">strategy</entity> includes <entity id="C88-1082.5">parsing</entity> and <entity id="C88-1082.6">phrase prediction algorithms</entity>. After <entity id="C88-1082.7">speech processing</entity> and <entity id="C88-1082.8">phrase recognition</entity> based on <entity id="C88-1082.9">phoneme recognition</entity>, the <entity id="C88-1082.10">parser</entity> extracts the <entity id="C88-1082.11">sentence</entity> with the best <entity id="C88-1082.12">likelihood</entity> taking account of the <entity id="C88-1082.13">phonetic likelihood</entity> of <entity id="C88-1082.14">phrase candidates</entity> and the <entity id="C88-1082.15">linguistic likelihood</entity> of the <entity id="C88-1082.16">semantic inter-phrase dependency relationships</entity>. A fast <entity id="C88-1082.17">parsing algorithm</entity> using <entity id="C88-1082.18">breadth-first search</entity> is also proposed. The <entity id="C88-1082.19">predictor</entity> pre-selects the <entity id="C88-1082.20">phrase candidates</entity> using <entity id="C88-1082.21">transition rules</entity> combined with a <entity id="C88-1082.22">dependency structure</entity> to reduce the amount of <entity id="C88-1082.23">phonetic processing</entity>. The proposed <entity id="C88-1082.24">linguistic processor</entity> has been tested through <entity id="C88-1082.25">speech recognition experiments</entity>. The experimental results show that it greatly increases the <entity id="C88-1082.26">accuracy</entity> of <entity id="C88-1082.27">speech recognitions</entity>, and the <entity id="C88-1082.28">breadth-first parsing algorithm</entity> and <entity id="C88-1082.29">predictor</entity> increase <entity id="C88-1082.30">processing speed</entity>.
</abstract>

</text>

<text id="L08-1178">
<title>Development and Alignment of a Domain-Specific Ontology for Question Answering</title>
<abstract>
With the appearance of <entity id="L08-1178.1">Semantic Web technologies</entity>, it becomes possible to develop novel, sophisticated <entity id="L08-1178.2">question answering systems</entity>, where <entity id="L08-1178.3">ontologies</entity> are usually used as the <entity id="L08-1178.4">core knowledge component</entity>. In the EU-funded project, <entity id="L08-1178.5">QALL-ME</entity>, a <entity id="L08-1178.6">domain-specific ontology</entity> was developed and applied for <entity id="L08-1178.7">question answering</entity> in the domain of tourism, along with the assistance of two upper <entity id="L08-1178.8">ontologies</entity> for <entity id="L08-1178.9">concept expansion and reasoning</entity>. This paper focuses on the development of the <entity id="L08-1178.10">QALL-ME ontology</entity> in the tourism domain and its <entity id="L08-1178.11">alignment</entity> with the <entity id="L08-1178.12">upper ontologies - WordNet and SUMO</entity>. The design of the <entity id="L08-1178.13">ontology</entity> is presented in the paper, and a <entity id="L08-1178.14">semi-automatic alignment procedure</entity> is described with some <entity id="L08-1178.15">alignment results</entity> given as well. Furthermore, the <entity id="L08-1178.16">aligned ontology</entity> was used to semantically annotate original <entity id="L08-1178.17">data</entity> obtained from the tourism web sites and <entity id="L08-1178.18">natural language questions</entity>. The <entity id="L08-1178.19">storage schema</entity> of the <entity id="L08-1178.20">annotated data</entity> and the <entity id="L08-1178.21">data access method</entity> for retrieving answers from the <entity id="L08-1178.22">annotated data</entity> are also reported in the paper.
</abstract>

</text>

<text id="C02-1138">
<title>Towards Automatic Generation Of Natural Language Generation Systems</title>
<abstract><entity id="C02-1138.1">Systems</entity> that interact with the <entity id="C02-1138.2">user</entity> via <entity id="C02-1138.3">natural language</entity> are in their infancy. As these <entity id="C02-1138.4">systems</entity> mature and become more complex, it would be desirable for a <entity id="C02-1138.5">system developer</entity> if there were an <entity id="C02-1138.6">automatic method</entity> for creating <entity id="C02-1138.7">natural language generation components</entity> that can produce quality output efficiently. We conduct experiments that show that this goal appears to be realizable. In particular we discuss a <entity id="C02-1138.8">natural language generation system</entity> that is composed of <entity id="C02-1138.9">SPoT</entity>, a trainable <entity id="C02-1138.10">sentence planner</entity>, and <entity id="C02-1138.11">FERGUS, a stochastic surface realizer</entity>. We show how these <entity id="C02-1138.12">stochastic NLG components</entity> can be made to work together, that they can be ported to new domains with apparent ease, and that such <entity id="C02-1138.13">NLG components</entity> can be integrated in a <entity id="C02-1138.14">real-time dialog system</entity>.
</abstract>

</text>

<text id="L08-1016">
<title>
A <entity id="L08-1016.1">Comparative Cross-Domain Study</entity> of the Occurrence of Laughter in Meeting and Seminar <entity id="L08-1016.2">Corpora</entity></title>
<abstract>
Laughter is an intrinsic <entity id="L08-1016.3">component</entity> of <entity id="L08-1016.4">human-human interaction</entity>, and current <entity id="L08-1016.5">automatic speech understanding paradigms</entity> stand to gain significantly from its <entity id="L08-1016.6">detection</entity> and <entity id="L08-1016.7">modeling</entity>. In the current work, we produce a <entity id="L08-1016.8">manual segmentation</entity> of laughter in a large <entity id="L08-1016.9">corpus</entity> of <entity id="L08-1016.10">interactive multi-party seminars</entity>, which promises to be a valuable resource for <entity id="L08-1016.11">acoustic modeling</entity> purposes. More importantly, we quantify the occurrence of laughter in this new domain, and contrast our observations with findings for laughter in <entity id="L08-1016.12">multi-party meetings</entity>. Our <entity id="L08-1016.13">analyses</entity> show that, with respect to the majority of <entity id="L08-1016.14">measures</entity> we explore, the occurrence of laughter in both domains is quite similar.
</abstract>

</text>

<text id="C02-2028">
<title>Chinese Syntactic Parsing Based On Extended GLR Parsing Algorithm With PCFG</title>
<abstract>
This paper presents an <entity id="C02-2028.1">extended GLR parsing algorithm</entity> with <entity id="C02-2028.2">grammar PCFG*</entity> that is based on <entity id="C02-2028.3">Tomita's GLR parsing algorithm</entity> and extends it further. We also define a new <entity id="C02-2028.4">grammar PCFG*</entity> that is based on <entity id="C02-2028.5">PCFG</entity> and assigns not only <entity id="C02-2028.6">probability</entity> but also <entity id="C02-2028.7">frequency</entity> associated with each <entity id="C02-2028.8">rule</entity>. So our <entity id="C02-2028.9">syntactic parsing system</entity> is implemented based on <entity id="C02-2028.10">rule-based approach</entity> and <entity id="C02-2028.11">statistics approach</entity>. Furthermore our experiments are executed in two fields: <entity id="C02-2028.12">Chinese base noun phrase identification</entity> and <entity id="C02-2028.13">full syntactic parsing</entity>. And the results of these two fields are compared from three ways. The experiments prove that the <entity id="C02-2028.14">extended GLR parsing algorithm</entity> with <entity id="C02-2028.15">PCFG*</entity> is an efficient <entity id="C02-2028.16">parsing method</entity> and a straightforward way to combine <entity id="C02-2028.17">statistical property</entity> with <entity id="C02-2028.18">rules</entity>. The experiment results of these two fields are presented in this paper.
</abstract>

</text>

<text id="L08-1535">
<title>A Proper Approach to Japanese Morphological Analysis: Dictionary, Model, and Evaluation</title>
<abstract>
In this paper, we discuss <entity id="L08-1535.1">lemma identification</entity> in <entity id="L08-1535.2">Japanese morphological analysis</entity>, which is crucial for a proper formulation of <entity id="L08-1535.3">morphological analysis</entity> that benefits not only NLP researchers but also corpus linguists. Since <entity id="L08-1535.4">Japanese words</entity> often have variation in <entity id="L08-1535.5">orthography</entity> and the <entity id="L08-1535.6">vocabulary</entity> of <entity id="L08-1535.7">Japanese</entity> consists of <entity id="L08-1535.8">words</entity> of several different origins, it sometimes happens that more than one <entity id="L08-1535.9">writing form</entity> corresponds to the same <entity id="L08-1535.10">lemma</entity> and that a single <entity id="L08-1535.11">writing form</entity> corresponds to two or more <entity id="L08-1535.12">lemmas</entity> with different <entity id="L08-1535.13">readings</entity> and/or <entity id="L08-1535.14">meanings</entity>. The mapping from a <entity id="L08-1535.15">writing form</entity> onto a <entity id="L08-1535.16">lemma</entity> is important in <entity id="L08-1535.17">linguistic analysis</entity> of <entity id="L08-1535.18">corpora</entity>. The current study focuses on <entity id="L08-1535.19">disambiguation</entity> of <entity id="L08-1535.20">heteronyms</entity>, <entity id="L08-1535.21">words</entity> with the same <entity id="L08-1535.22">writing form</entity> but with different <entity id="L08-1535.23">word forms</entity>. To resolve <entity id="L08-1535.24">heteronym ambiguity</entity>, we make use of <entity id="L08-1535.25">goshu information</entity>, the <entity id="L08-1535.26">classification</entity> of <entity id="L08-1535.27">words</entity> based on their <entity id="L08-1535.28">origin</entity>. Founded on the fact that <entity id="L08-1535.29">words</entity> of some <entity id="L08-1535.30">goshu classes</entity> are more likely to combine into <entity id="L08-1535.31">compound words</entity> than <entity id="L08-1535.32">words</entity> of other <entity id="L08-1535.33">classes</entity>, we employ a <entity id="L08-1535.34">statistical model</entity> based on <entity id="L08-1535.35">CRFs</entity> using <entity id="L08-1535.36">goshu information</entity>. <entity id="L08-1535.37">Experimental results</entity> show that the use of <entity id="L08-1535.38">goshu information</entity> considerably improves the <entity id="L08-1535.39">performance</entity> of <entity id="L08-1535.40">heteronym disambiguation</entity> and <entity id="L08-1535.41">lemma identification</entity>, suggesting that goshu <entity id="L08-1535.42">information</entity> solves the <entity id="L08-1535.43">lemma identification task</entity> very effectively.
</abstract>

</text>

<text id="L08-1003">
<title>Event Detection and Summarization in Weblogswith Temporal Collocations</title>
<abstract>
This paper deals with the relationship between weblog content and time. With the proposed <entity id="L08-1003.1">temporal mutual information</entity>, we analyze the <entity id="L08-1003.2">collocations</entity> in time dimension, and the interesting <entity id="L08-1003.3">collocations</entity> related to special events. The <entity id="L08-1003.4">temporal mutual information</entity> is employed to observe the strength of <entity id="L08-1003.5">term-to-term associations</entity> over time. An <entity id="L08-1003.6">event detection algorithm</entity> identifies the <entity id="L08-1003.7">collocations</entity> that may cause an event in a specific timestamp. An <entity id="L08-1003.8">event summarization algorithm</entity> retrieves a set of <entity id="L08-1003.9">collocations</entity> which describe an event. We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the <entity id="L08-1003.10">temporal collocations</entity> capture the <entity id="L08-1003.11">real world semantics</entity> and real world events over time.
</abstract>

</text>

<text id="N04-4025">
<title>Automated Team Discourse Annotation And Performance Prediction Using LSA</title>
<abstract>
We describe two approaches to analyzing and <entity id="N04-4025.1">tagging</entity> team discourse using <entity id="N04-4025.2">Latent Semantic Analysis (LSA)</entity> to predict team performance. The first approach automatically categorizes the contents of each statement made by each of the three team members using an established set of <entity id="N04-4025.3">tags</entity>. Performance predicting the <entity id="N04-4025.4">tags</entity> automatically was 15% below <entity id="N04-4025.5">human agreement</entity>. These tagged statements are then used to predict team performance. The second approach measures the semantic content of the <entity id="N04-4025.6">dialogue</entity> of the team as a whole and accurately predicts the team's performance on a simulated military mission.
</abstract>

</text>

<text id="A94-1044">
<title>TECHDOC: Multilingual Generation Of Online And Offline Instructional Text</title>
<abstract>
Supplying <entity id="A94-1044.1">technical documentation</entity> accompanying a product in multiple <entity id="A94-1044.2">languages</entity> is a growing problem, particularly in Europe with its legislation on the common market. A huge amount of <entity id="A94-1044.3">translation work</entity> needs to be done when creating and updating <entity id="A94-1044.4">technical documentation</entity>. In response to these needs, the <entity id="A94-1044.5">TECHDOC project</entity> aims at supporting the creation and maintenance of <entity id="A94-1044.6">technical documentation</entity> by <entity id="A94-1044.7">knowledge-based, multilingual generation</entity>. The idea is to build up a <entity id="A94-1044.8">knowledge base</entity> that includes a model of the product in question, and to produce <entity id="A94-1044.9">documentation</entity> in multiple <entity id="A94-1044.10">languages</entity> automatically. At present, the <entity id="A94-1044.11">system</entity> produces maintenance instructions in <entity id="A94-1044.12">English</entity>, <entity id="A94-1044.13">German</entity>, and <entity id="A94-1044.14">French</entity>. The application domain of <entity id="A94-1044.15">TECHDOC</entity> is <entity id="A94-1044.16">technical manuals</entity>, where one has to do with <entity id="A94-1044.17">"real-world" texts</entity>: the domain is practical instead of a "toy world". At the same time, the <entity id="A94-1044.18">language</entity> that is used in such <entity id="A94-1044.19">manuals</entity> tends to be relatively simple; one mostly finds straightforward instructions that have been written with the intention to produce <entity id="A94-1044.20">text</entity> that can be readily understood by a person who is executing some maintenance activity. Moreover, the structure of <entity id="A94-1044.21">manual sections</entity> is largely uniform and amenable to <entity id="A94-1044.22">formalization</entity>. </abstract>

</text>

<text id="H91-1079">
<title>Adaptive Natural Language Processing</title>
<abstract>
The objective of this project is a <entity id="H91-1079.1">pilot study</entity> of several new ideas for the <entity id="H91-1079.2">automatic adaptation</entity> and <entity id="H91-1079.3">improvement</entity> of <entity id="H91-1079.4">natural language processing (NLP) systems</entity>. The effort focuses particularly on automatically inferring the <entity id="H91-1079.5">meaning</entity> of new <entity id="H91-1079.6">words</entity> in <entity id="H91-1079.7">context</entity> and on developing <entity id="H91-1079.8">partial interpretations</entity> of <entity id="H91-1079.9">language</entity> that is either fragmentary or beyond the capability of the <entity id="H91-1079.10">NLP system</entity> to understand. The techniques are being evaluated in a message processing domain, such as <entity id="H91-1079.11">automatic data base update</entity> based on articles from The Wall Street Journal on corporate takeover bids. The <entity id="H91-1079.12">NLP system</entity> uses <entity id="H91-1079.13">large annotated corpora</entity>, such as those being developed under the <entity id="H91-1079.14">DARPA-funded TREE-BANK project</entity> at the University of Pennsylvania, to adapt by acquiring <entity id="H91-1079.15">syntactic and semantic information</entity> from the <entity id="H91-1079.16">annotated examples</entity>. <entity id="H91-1079.17">Statistical language modeling</entity>, based on <entity id="H91-1079.18">probability estimates</entity> derived from the <entity id="H91-1079.19">large corpora</entity>, will provide a means of ranking alternative interpretations of fragments. This <entity id="H91-1079.20">pilot study</entity>, running from March 1990 through March 1991, is designed to test the feasibility of such a new approach.
</abstract>

</text>

<text id="H92-1116">
<title>WORDNET: A Lexical Database For English</title>
<abstract>
<entity id="H92-1116.1">WordNet</entity> is a <entity id="H92-1116.2">lexical database</entity> for <entity id="H92-1116.3">English</entity> organized in accordance with current <entity id="H92-1116.4">psycholinguistic theories</entity>. <entity id="H92-1116.5">Lexicalized concepts</entity> are organized by <entity id="H92-1116.6">semantic relations</entity> (<entity id="H92-1116.7">synonymy</entity>, <entity id="H92-1116.8">antonymy</entity>, <entity id="H92-1116.9">hyponymy</entity>, <entity id="H92-1116.10">meronymy</entity>, etc.) for <entity id="H92-1116.11">nouns</entity>, <entity id="H92-1116.12">verbs</entity>, and <entity id="H92-1116.13">adjectives</entity>. Work under this grant is intended to extend and upgrade <entity id="H92-1116.14">WordNet</entity>, to make it generally available, and to develop it as a tool for use in practical <entity id="H92-1116.15">applications</entity>. In order to make it available for <entity id="H92-1116.16">information retrieval</entity> and <entity id="H92-1116.17">machine translation</entity>, a system is being developed <entity id="H92-1116.18">English text</entity> as input and automatically gives as output the same <entity id="H92-1116.19">text</entity> augmented by <entity id="H92-1116.20">syntactic and semantic anotations</entity> that disambiguate all of the <entity id="H92-1116.21">substantive words</entity>. Initially, the <entity id="H92-1116.22">semantic tagging</entity> is being done manually so that we can (1) obtain extensive experience with the <entity id="H92-1116.23">tagging process</entity> and (2) create a <entity id="H92-1116.24">database</entity> of correctly tagged <entity id="H92-1116.25">text</entity> for use in testing proposals for <entity id="H92-1116.26">automatic sense disambiguation</entity>.
</abstract>

</text>

<text id="X96-1060">
<title>How To Get Information About TIPSTER</title>
<abstract>
Determine if <entity id="X96-1060.1">TIPSTER technology</entity> and the <entity id="X96-1060.2">TIPSTER Architecture</entity> are what you need.Gather requirements for your <entity id="X96-1060.3">application</entity> and develop a <entity id="X96-1060.4">Concept of Operation</entity>.Investigate what <entity id="X96-1060.5">applications</entity>, similar to yours, have been implemented and by which contractors. Review previous designs involving <entity id="X96-1060.6">TIPSTER technology</entity>, to support you design process.Determine if your <entity id="X96-1060.7">application</entity> can benefit from upgrading to advanced <entity id="X96-1060.8">TIPSTER technology</entity> that has been developed since your <entity id="X96-1060.9">application</entity> was implemented.
</abstract>

</text>

<text id="H05-1080">
<title>A Self-Learning Context-Aware Lemmatizer For German</title>
<abstract>
Accurate <entity id="H05-1080.1">lemmatization</entity> of <entity id="H05-1080.2">German nouns</entity> mandates the use of a <entity id="H05-1080.3">lexicon</entity>. Comprehensive <entity id="H05-1080.4">lexicons</entity>, however, are expensive to build and maintain. We present a <entity id="H05-1080.5">self-learning lemmatizer</entity> capable of automatically creating a <entity id="H05-1080.6">full-form lexicon</entity> by processing <entity id="H05-1080.7">German documents</entity>.
</abstract>

</text>

<text id="W02-0211">
<title>Discourse Processing For Explanatory Essays In Tutorial Applications</title>
<abstract>
The <entity id="W02-0211.1">Why-Atlas tutoring system</entity> presents students with <entity id="W02-0211.2">qualitative physics questions</entity> and encourages them to explain their answers via <entity id="W02-0211.3">natural language</entity>. Although there are inexpensive techniques for analyzing explanations, we claim that better understanding is necessary for use within <entity id="W02-0211.4">tutoring systems</entity>. In this paper we describe how <entity id="W02-0211.5">Why-Atlas</entity> creates and utilizes a <entity id="W02-0211.6">proof-based representation</entity> of student essays. We describe how it creates the proof given the output of <entity id="W02-0211.7">sentence-level understanding</entity>, how it uses the <entity id="W02-0211.8">proofs</entity> to give students feedback, some preliminary <entity id="W02-0211.9">runtime measures</entity>, and the work we are currently doing to derive additional benefits from a <entity id="W02-0211.10">proof-based approach</entity> for <entity id="W02-0211.11">tutoring applications</entity>.
</abstract>

</text>

<text id="W03-1712">
<title>Building A Large Chinese Corpus Annotated With Semantic Dependency</title>
<abstract>
At present most of <entity id="W03-1712.1">corpora</entity> are annotated mainly with <entity id="W03-1712.2">syntactic knowledge</entity>. In this paper, we attempt to build a large <entity id="W03-1712.3">corpus</entity> and annotate <entity id="W03-1712.4">semantic knowledge</entity> with <entity id="W03-1712.5">dependency grammar</entity>. We believe that <entity id="W03-1712.6">words</entity> are the basic units of <entity id="W03-1712.7">semantics</entity>, and the structure and <entity id="W03-1712.8">meaning</entity> of a <entity id="W03-1712.9">sentence</entity> consist mainly of a series of <entity id="W03-1712.10">semantic dependencies</entity> between individual <entity id="W03-1712.11">words</entity>. A 1,000,000-word-scale <entity id="W03-1712.12">corpus</entity> annotated with semantic dependency has been built. Compared with <entity id="W03-1712.13">syntactic knowledge</entity>, <entity id="W03-1712.14">semantic knowledge</entity> is more difficult to annotate, for <entity id="W03-1712.15">ambiguity problem</entity> is more serious. In the paper, the strategy to improve consistency is addressed, and congruence is defined to measure the consistency of <entity id="W03-1712.16">tagged corpus</entity>. Finally, we will compare our <entity id="W03-1712.17">corpus</entity> with other well-known <entity id="W03-1712.18">corpora</entity>.
</abstract>

</text>

<text id="J81-2002">
<title>Relaxation Techniques For Parsing Grammatically Ill-Formed Input In Natural Language Understanding Systems</title>
<abstract>
This paper investigates several language phenomena either considered deviant by linguistic standards or insufficiently addressed by existing approaches. These include <entity id="J81-2002.1">co-occurrence violations</entity>, some forms of <entity id="J81-2002.2">ellipsis</entity> and <entity id="J81-2002.3">extraneous forms</entity>, and <entity id="J81-2002.4">conjunction</entity>. <entity id="J81-2002.5">Relaxation techniques</entity> for their treatment in <entity id="J81-2002.6">Natural Language Understanding Systems</entity> are discussed. These <entity id="J81-2002.7">techniques</entity>, developed within the <entity id="J81-2002.8">Augmented Transition Network (ATN) model</entity>, are shown to be adequate to handle many of these cases.
</abstract>

</text>

<text id="P98-1044">
<title>Veins Theory: A Model of Global Discourse Cohesion and Coherence</title>
<abstract>
In this paper, we propose a generalization of <entity id="P98-1044.1">Centering Theory (CT)</entity> ( Grosz, Joshi, Weinstein (1995)) called <entity id="P98-1044.2">Veins Theory (VT)</entity>, which extends the applicability of <entity id="P98-1044.3">centering rules</entity> from local to <entity id="P98-1044.4">global discourse</entity>. A key facet of the <entity id="P98-1044.5">theory</entity> involves the identification of <entity id="P98-1044.6">veins</entity> over <entity id="P98-1044.7">discourse structure trees</entity> such as those defined in <entity id="P98-1044.8">RST</entity>, which delimit domains of <entity id="P98-1044.9">referential accessibility</entity> for each unit in a <entity id="P98-1044.10">discourse</entity>. Once identified, <entity id="P98-1044.11">reference chains</entity> can be extended across <entity id="P98-1044.12">segment boundaries</entity>, thus enabling the application of <entity id="P98-1044.13">CT</entity> over the entire <entity id="P98-1044.14">discourse</entity>. We describe the processes by which <entity id="P98-1044.15">veins</entity> are defined over <entity id="P98-1044.16">discourse structure trees</entity> and how <entity id="P98-1044.17">CT</entity> can be applied to <entity id="P98-1044.18">global discourse</entity> by using these <entity id="P98-1044.19">chains</entity>. We also define a <entity id="P98-1044.20">discourse smoothness index</entity> which can be used to compare different <entity id="P98-1044.21">discourse structures and interpretations</entity>, and show how <entity id="P98-1044.22">VT</entity> can be used to abstract a span of <entity id="P98-1044.23">text</entity> in the context of the whole <entity id="P98-1044.24">discourse</entity>. Finally, we validate our <entity id="P98-1044.25">theory</entity> by analyzing examples from <entity id="P98-1044.26">corpora</entity> of <entity id="P98-1044.27">English</entity>, <entity id="P98-1044.28">French</entity>, and <entity id="P98-1044.29">Romanian</entity>.
</abstract>

</text>

<text id="W04-2507">
<title>HITIQA: Scenario Based Question Answering</title>
<abstract>
In this paper we describe some preliminary results of qualitative evaluation of the <entity id="W04-2507.1">answering system HITIQA (High-Quality Interactive Question Answering)</entity> which has been developed over the last 2 years as an advanced research tool for <entity id="W04-2507.2">information analysts</entity>. <entity id="W04-2507.3">HITIQA</entity> is an <entity id="W04-2507.4">interactive open-domain question answering technology</entity> designed to allow <entity id="W04-2507.5">analysts</entity> to pose complex <entity id="W04-2507.6">exploratory questions</entity> in <entity id="W04-2507.7">natural language</entity> and obtain relevant <entity id="W04-2507.8">information units</entity> to prepare their <entity id="W04-2507.9">briefing reports</entity> in order to satisfy a given <entity id="W04-2507.10">scenario</entity>. The <entity id="W04-2507.11">system</entity> uses novel <entity id="W04-2507.12">data-driven semantics</entity> to conduct a clarification dialogue with the <entity id="W04-2507.13">user</entity> that explores the <entity id="W04-2507.14">scope</entity> and the <entity id="W04-2507.15">context</entity> of the desired <entity id="W04-2507.16">answer space</entity>. The <entity id="W04-2507.17">system</entity> has undergone extensive <entity id="W04-2507.18">hands-on evaluations</entity> by a group of <entity id="W04-2507.19">intelligence analysts</entity> representing various foreign intelligence services. This evaluation validated the overall approach in <entity id="W04-2507.20">HITIQA</entity> but also exposed limitations of the current <entity id="W04-2507.21">prototype</entity>.
</abstract>

</text>

<text id="W06-1419">
<title> Evaluations Of NLG Systems: Common Corpus And Tasks Or Common Dimensions And Metrics?</title>
<abstract>
"In this position paper, we argue that a common task and <entity id="W06-1419.1">corpus</entity> are not the only ways to evaluate <entity id="W06-1419.2">Natural Language Generation (NLG) systems</entity>. It might be, in fact, too narrow a view on evaluation and thus not be the best way to evaluate these systems. The aim of a common task and corpus is to allow for a comparative evaluation of systems, looking at the systems' performances. It is thus a <entity id="W06-1419.3">"system-oriented" view of evaluation</entity>. We argue here that, if we are to take a system oriented view of evaluation, the community might be better served by enlarging the view of evaluation, defining common dimensions and <entity id="W06-1419.4">metrics</entity> to evaluate systems and approaches. We also argue that <entity id="W06-1419.5">end-user (or usability) evaluations</entity> form another important aspect of a system's evaluation and should not be forgotten. "
</abstract>

</text>

<text id="W07-1013">
<title>A shared task involving multi-label classification of clinical free text</title>
<abstract>
This paper reports on a shared task involving the assignment of ICD-9-CM codes to <entity id="W07-1013.1">radiology reports</entity>. Two features distinguished this task from previous shared tasks in the <entity id="W07-1013.2">biomedical domain</entity>. One is that it resulted in the first freely distributable <entity id="W07-1013.3">corpus</entity> of <entity id="W07-1013.4">fully anonymized clinical text</entity>. This <entity id="W07-1013.5">resource</entity> is permanently available and will (we hope) facilitate future research. The other key feature of the task is that it required <entity id="W07-1013.6">categorization</entity> with respect to a large and commercially significant <entity id="W07-1013.7">set of labels</entity>. The number of participants was larger than in any previous <entity id="W07-1013.8">biomedical challenge task</entity>. We describe the<entity id="W07-1013.9">data production process</entity> and the <entity id="W07-1013.10">evaluation measures</entity>, and give a preliminary analysis of the results. Many systems performed at levels approaching the <entity id="W07-1013.11">inter-coder agreement</entity>, suggesting that <entity id="W07-1013.12">human-like performance</entity> on this task is within the reach of <entity id="W07-1013.13">currently available technologies</entity>.
</abstract>

</text>

<text id="N07-2055">
<title>A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation</title>
<abstract>
In this paper we describe <entity id="N07-2055.1">automatic information nuggetization</entity> and its application to <entity id="N07-2055.2">text comparison</entity>. More specifically, we take a close look at how <entity id="N07-2055.3">machine-generated nuggets</entity> can be used to create <entity id="N07-2055.4">evaluation material</entity>. A <entity id="N07-2055.5">semiautomatic annotation scheme</entity> is designed to produce <entity id="N07-2055.6">gold-standard data</entity> with exceptionally high <entity id="N07-2055.7">inter-human agreement</entity>.
</abstract>

</text>

<text id="P07-3007">
<title>Kinds of Features for Chinese Opinionated Information Retrieval</title>
<abstract>
This paper presents the results of experiments in which we tested different kinds of <entity id="P07-3007.1">features</entity> for <entity id="P07-3007.2">retrieval</entity> of <entity id="P07-3007.3">Chinese opinionated texts</entity>. We assume that the task of <entity id="P07-3007.4">retrieval</entity> of <entity id="P07-3007.5">opinionated texts (OIR)</entity> can be regarded as a subtask of general <entity id="P07-3007.6">IR</entity>, but with some distinct <entity id="P07-3007.7">features</entity>. The experiments showed that the best results were obtained from the combination of <entity id="P07-3007.8">character-based processing</entity>, <entity id="P07-3007.9">dictionary look up</entity> (<entity id="P07-3007.10">maximum matching</entity>) and a <entity id="P07-3007.11">negation check</entity>.
</abstract>

</text>

<text id="P02-1065">
<title>Memory-Based Learning Of Morphology With Stochastic Transducers</title>
<abstract>
This paper discusses the <entity id="P02-1065.1">supervised learning</entity> of <entity id="P02-1065.2">morphology</entity> using <entity id="P02-1065.3">stochastic transducers</entity>, trained using the <entity id="P02-1065.4">Expectation-Maximization (EM) algorithm</entity>. Two approaches are presented: first, using the <entity id="P02-1065.5">transducers</entity> directly to model the process, and secondly using them to define a <entity id="P02-1065.6">similarity measure</entity>, related to the <entity id="P02-1065.7">Fisher kernel method</entity> ( Jaakkola and Haussler, 1998 ), and then using a <entity id="P02-1065.8">Memory-Based Learning(MBL) technique</entity>. These are evaluated and compared on<entity id="P02-1065.9">data sets</entity> from <entity id="P02-1065.10">English</entity>, <entity id="P02-1065.11">German</entity>, <entity id="P02-1065.12">Slovene</entity> and <entity id="P02-1065.13">Arabic</entity>.
</abstract>

</text>

<text id="P06-1025">
<title>Dependencies Between Student State And Speech Recognition Problems In Spoken Tutoring Dialogues</title>
<abstract><entity id="P06-1025.1">Speech recognition problems</entity> are a reality in current <entity id="P06-1025.2">spoken dialogue systems</entity>. In order to better understand these phenomena, we study dependencies between <entity id="P06-1025.3">speech recognition problems</entity> and several higher level <entity id="P06-1025.4">dialogue factors</entity> that define our notion of student state: frustration/anger, certainty and correctness. We apply <entity id="P06-1025.5">Chi Square (%2) analysis</entity> to a <entity id="P06-1025.6">corpus of speech-based computer tutoring dialogues</entity> to discover these dependencies both within and across turns. Significant dependencies are combined to produce interesting insights regarding <entity id="P06-1025.7">speech recognition problems</entity> and to propose new strategies for handling these problems. We also find that tutoring, as a new domain for <entity id="P06-1025.8">speech applications</entity>, exhibits interesting tradeoffs and new factors to consider for <entity id="P06-1025.9">spoken dialogue design</entity>.
</abstract>

</text>

<text id="C94-1012">
<title>Coping With Ambiguity In A Large-Scale Machine Translation System</title>
<abstract>
In an <entity id="C94-1012.1">interlingual knowledge-based machine translation system</entity>, <entity id="C94-1012.2">ambiguity</entity> arises when the <entity id="C94-1012.3">source language analyzer</entity> produces more than one <entity id="C94-1012.4">interlingua expression</entity> for a <entity id="C94-1012.5">source sentence</entity>. This can have a negative impact on <entity id="C94-1012.6">translation quality</entity>, since a <entity id="C94-1012.7">target sentence</entity> may be produced from an unintended meaning. In this paper we describe the methods used in the <entity id="C94-1012.8">KANT machine translation system</entity> to reduce or eliminate <entity id="C94-1012.9">ambiguity</entity> in a <entity id="C94-1012.10">large-scale application domain</entity>. We also test these methods on a large <entity id="C94-1012.11">corpus</entity> of <entity id="C94-1012.12">test sentences</entity>, in order to illustrate how the different <entity id="C94-1012.13">disambiguation methods</entity> reduce the average number of <entity id="C94-1012.14">parses</entity> per <entity id="C94-1012.15">sentence</entity>.
</abstract>

</text>

<text id="D08-1012">
<title>Coarse-to-Fine Syntactic Machine Translation using Language Projections</title>
<abstract>
The intersection of <entity id="D08-1012.1">tree transducer-based translation models</entity> with <entity id="D08-1012.2">n-gram language models</entity> results in huge <entity id="D08-1012.3">dynamic programs</entity> for <entity id="D08-1012.4">machine translation decoding</entity>. We propose a <entity id="D08-1012.5">multipass</entity>, <entity id="D08-1012.6">coarse-to-fine approach</entity> in which the <entity id="D08-1012.7">language model complexity</entity> is incrementally introduced. In contrast to previous <entity id="D08-1012.8">order-based bigram-to-trigram approaches</entity>, we focus on <entity id="D08-1012.9">encoding-based methods</entity>, which use a <entity id="D08-1012.10">clustered encoding</entity> of the <entity id="D08-1012.11">target language</entity>. Across various <entity id="D08-1012.12">encoding schemes</entity>, and for multiple <entity id="D08-1012.13">language pairs</entity>, we show speed-ups of up to 50 times over <entity id="D08-1012.14">single-pass decoding</entity> while improving <entity id="D08-1012.15">BLEU score</entity>. Moreover, our entire <entity id="D08-1012.16">decoding cascade for trigram language models</entity> is faster than the corresponding <entity id="D08-1012.17">bigram pass</entity> alone of a <entity id="D08-1012.18">bigram-to-trigram decoder</entity>.
</abstract>

</text>

<text id="L08-1071">
<title>Challenges in Pronoun Resolution System for Biomedical Text</title>
<abstract>
This paper presents our findings on the feasibility of doing <entity id="L08-1071.1">pronoun resolution</entity> for <entity id="L08-1071.2">biomedical texts</entity>, in comparison with conducting <entity id="L08-1071.3">pronoun resolution</entity> for the <entity id="L08-1071.4">newswire domain</entity>. In our experiments, we built a simple <entity id="L08-1071.5">machine learning-based pronoun resolution system</entity>, and evaluated the system on three different <entity id="L08-1071.6">corpora</entity>: <entity id="L08-1071.7">MUC</entity>, <entity id="L08-1071.8">ACE</entity>, and <entity id="L08-1071.9">GENIA </entity>. <entity id="L08-1071.10">Comparative statistics</entity> not only reveal the noticeable issues in constructing an effective <entity id="L08-1071.11">pronoun resolution system</entity> for a new domain, but also provides a comprehensive view of those <entity id="L08-1071.12">corpora</entity> often used for this task.
</abstract>

</text>

<text id="C00-1005">
<title>An Ontology Of Systematic Relations For A Shared Grammar Of Slavic</title>
<abstract>
Sharing portions of <entity id="C00-1005.1">grammars</entity> across <entity id="C00-1005.2">languages</entity> greatly reduces the costs of <entity id="C00-1005.3">multilingual grammar engineering</entity>. <entity id="C00-1005.4">Related languages</entity> share a much wider range of <entity id="C00-1005.5">linguistic information</entity> than typically assumed in standard <entity id="C00-1005.6">multilingual grammar architectures</entity>. Taking <entity id="C00-1005.7">grammatical relatedness</entity> seriously, we are particularly interested in designing <entity id="C00-1005.8">linguistically motivated grammatical resources</entity> for <entity id="C00-1005.9">Slavic languages</entity> to be used in <entity id="C00-1005.10">applied and theoretical computational linguistics</entity>. In order to gain the perspective of a <entity id="C00-1005.11">language-family oriented grammar design</entity>, we consider an array of <entity id="C00-1005.12">systematic relations</entity> that can hold between <entity id="C00-1005.13">syntactical units</entity>. While the categorisation of <entity id="C00-1005.14">primitive linguistic entities</entity> tends to be <entity id="C00-1005.15">language-specific</entity> or even <entity id="C00-1005.16">construction-specific</entity>, the relations holding between them allow various degrees of abstraction. On the basis of <entity id="C00-1005.17">Slavic data</entity>, we show how a <entity id="C00-1005.18">domain ontology</entity> conceptualising <entity id="C00-1005.19">morpho-syntactic "building blocks"</entity> can serve as a basis of a <entity id="C00-1005.20">shared grammar of Slavic</entity>.
</abstract>

</text>

<text id="L08-1561">
<title>
<entity id="L08-1561.1">Spatiotemporal Annotation</entity> Using <entity id="L08-1561.2">MiniSTEx</entity>: how to deal with <entity id="L08-1561.3">Alternative, Foreign, Vague and/or Obsolete Names</entity>?</title>
<abstract>
We are currently developing <entity id="L08-1561.4">MiniSTEx</entity>, a <entity id="L08-1561.5">spatiotemporal annotation system</entity> to handle <entity id="L08-1561.6">temporal and/or geospatial information</entity> directly and indirectly expressed in <entity id="L08-1561.7">texts</entity>. In the end, the aim is to locate all <entity id="L08-1561.8">eventualities</entity> in a <entity id="L08-1561.9">text</entity> on a time axis and/or a map to ensure an optimal base for <entity id="L08-1561.10">automatic temporal and geospatial reasoning</entity>. A first version of <entity id="L08-1561.11">MiniSTEx</entity> was originally developed for <entity id="L08-1561.12">Dutch</entity>, keeping in mind that it should also be useful for other <entity id="L08-1561.13">European languages</entity>, and for <entity id="L08-1561.14">multilingual applications</entity>. In order to meet these desiderata we need the <entity id="L08-1561.15">MiniSTEx system</entity> to be able to draw the conclusions <entity id="L08-1561.16">human readers</entity> belonging to the intended audience would also draw, e.g. based on their <entity id="L08-1561.17">(spatiotemporal) world knowledge</entity>, i.e. the <entity id="L08-1561.18">common knowledge</entity> such <entity id="L08-1561.19">readers</entity> share. The <entity id="L08-1561.20">world knowledge</entity> <entity id="L08-1561.21">MiniSTEx</entity> uses is contained in interconnected tables in a <entity id="L08-1561.22">database</entity>. At the moment it is used for <entity id="L08-1561.23">Dutch</entity> and <entity id="L08-1561.24">English</entity>. Special attention will be paid to the problems we face when looking at older <entity id="L08-1561.25">texts</entity> or <entity id="L08-1561.26">recent historical or encyclopedic texts</entity>, i.e. <entity id="L08-1561.27">texts</entity> with lots of <entity id="L08-1561.28">references</entity> to <entity id="L08-1561.29">times</entity> and <entity id="L08-1561.30">locations</entity> that are not compatible with our current maps and calendars.
</abstract>

</text>

<text id="H01-1034">
<title>
Improving <entity id="H01-1034.1">Information Extraction</entity> By <entity id="H01-1034.2">Modeling Errors</entity> In <entity id="H01-1034.3">Speech Recognizer Output</entity></title>
<abstract>
In this paper we describe a technique for improving the performance of an <entity id="H01-1034.4">information extraction system</entity> for <entity id="H01-1034.5">speech data</entity> by explicitly modeling the errors in the <entity id="H01-1034.6">recognizer output</entity>. The approach combines a <entity id="H01-1034.7">statistical model</entity> of <entity id="H01-1034.8">named entity</entity> states with a <entity id="H01-1034.9">lattice representation</entity> of hypothesized <entity id="H01-1034.10">words</entity> and errors annotated with <entity id="H01-1034.11">recognition confidence scores</entity>. Additional refinements include the use of multiple <entity id="H01-1034.12">error types</entity>, improved <entity id="H01-1034.13">confidence estimation</entity>, and <entity id="H01-1034.14">multipass processing</entity>. In combination, these techniques improve <entity id="H01-1034.15">named entity recognition performance</entity> over a <entity id="H01-1034.16">text-based baseline</entity> by 28%.
</abstract>

</text>

<text id="I05-2006">
<title>A Question Answer System Based on Confirmed Knowledge Developed by Using Mails Posted to a Mailing List</title>
<abstract>
In this paper, we report a <entity id="I05-2006.1">QA system</entity> which can answer how <entity id="I05-2006.2">type questions</entity> based on the <entity id="I05-2006.3">confirmed knowledge base</entity> which was developed by using <entity id="I05-2006.4">mails</entity> posted to a <entity id="I05-2006.5">mailing list</entity>. We first discuss a problem of developing a <entity id="I05-2006.6">knowledge base</entity> by using <entity id="I05-2006.7">natural language documents</entity>: <entity id="I05-2006.8">wrong information</entity> in <entity id="I05-2006.9">natural language documents</entity>. Then, we describe a method of detecting <entity id="I05-2006.10">wrong information</entity> in <entity id="I05-2006.11">mails</entity> posted to a <entity id="I05-2006.12">mailing list</entity> and developing a <entity id="I05-2006.13">knowledge base</entity> by using these <entity id="I05-2006.14">mails</entity>. Finally, we show that <entity id="I05-2006.15">question and answer mails</entity> posted to a <entity id="I05-2006.16">mailing list</entity> can be used as a <entity id="I05-2006.17">knowledge base</entity> for a <entity id="I05-2006.18">QA system</entity>.
</abstract>

</text>

<text id="N04-1025">
<title>A Language Modeling Approach To Predicting Reading Difficulty</title>
<abstract>
We demonstrate a new research approach to the problem of predicting the <entity id="N04-1025.1">reading difficulty</entity> of a <entity id="N04-1025.2">text passage</entity>, by recasting <entity id="N04-1025.3">readability</entity> in terms of <entity id="N04-1025.4">statistical language modeling</entity>. We derive a measure based on an extension of <entity id="N04-1025.5">multinomial naive Bayes classification</entity> that combines multiple <entity id="N04-1025.6">language models</entity> to estimate the most likely grade level for a given <entity id="N04-1025.7">passage</entity>. The resulting classifier is not specific to any particular subject and can be trained with relatively little <entity id="N04-1025.8">labeled data</entity>. We perform predictions for individual <entity id="N04-1025.9">Web pages</entity> in English and compare our performance to widely-used <entity id="N04-1025.10">semantic variables</entity> from traditional <entity id="N04-1025.11">readability measures</entity>. We show that with minimal changes, the classifier may be retrained for use with French <entity id="N04-1025.12">Web documents</entity>. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all <entity id="N04-1025.13">test sets</entity>. Some traditional <entity id="N04-1025.14">semantic variables</entity> such as <entity id="N04-1025.15">type-token ratio</entity> gave the best performance on commercial calibrated <entity id="N04-1025.16">test passages</entity>, while our <entity id="N04-1025.17">language modeling approach</entity> gave better accuracy for <entity id="N04-1025.18">Web documents</entity> and very short <entity id="N04-1025.19">passages</entity> (less than 10 <entity id="N04-1025.20">words</entity>).
</abstract>

</text>

<text id="M93-1024">
<title>Description Of The LINK System Used For MUC-5</title>
<abstract>
Over the past five years, we have developed a <entity id="M93-1024.1">natural language processing (NLP) system</entity> called <entity id="M93-1024.2">LINK</entity>. <entity id="M93-1024.3">LINK</entity> is a <entity id="M93-1024.4">unification-based system</entity>, in which all <entity id="M93-1024.5">syntactic and semantic analysis</entity> is performed in a single step. <entity id="M93-1024.6">Syntactic and semantic information</entity> are both represented in the <entity id="M93-1024.7">grammar</entity> in a uniform manner, similar to <entity id="M93-1024.8">HPSG</entity> ( Pollard and Sag, 1987 ). <entity id="M93-1024.9">LINK</entity> has been used in several <entity id="M93-1024.10">information extraction applications</entity>. In a project with General Motors, <entity id="M93-1024.11">LINK</entity> was used to process terse <entity id="M93-1024.12">free-form descriptions</entity> of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them. In this very narrow domain, <entity id="M93-1024.13">LINK</entity> achieved recall and precision rates of 80-85%. Most recently, we used the <entity id="M93-1024.14">LINK system</entity> to participate in <entity id="M93-1024.15">MUC-4</entity>. During this competition, we developed initial versions of pre and postprocessing modules which were further developed in <entity id="M93-1024.16">MUC-5</entity>. In <entity id="M93-1024.17">MUC-4</entity>, <entity id="M93-1024.18">LINK</entity> achieved recall and precision rates of about 40%.
</abstract>

</text>

<text id="A00-2039">
<title>Finite-State Reduplication In One-Level Prosodic Morphology</title>
<abstract><entity id="A00-2039.1">Reduplication</entity>, a central instance of <entity id="A00-2039.2">prosodic morphology</entity>, is particularly challenging for <entity id="A00-2039.3">state-of-the-art computational morphology</entity>, since it involves copying of some part of a <entity id="A00-2039.4">phonological string</entity>. In this paper I advocate a <entity id="A00-2039.5">finite-state method</entity> that combines <entity id="A00-2039.6">enriched lexical representations</entity> via intersection to implement the copying. The proposal includes a <entity id="A00-2039.7">resource-conscious variant of automata</entity> and can benefit from the existence of <entity id="A00-2039.8">lazy algorithms</entity>. Finally, the implementation of a complex case from <entity id="A00-2039.9">Koasati</entity> is presented.
</abstract>

</text>

<text id="H92-1094">
<title>Augmenting With Slot Filler Relevancy Signatures Data</title>
<abstract>
Human readers can reliably identify many relevant <entity id="H92-1094.1">texts</entity> merely by skimming the <entity id="H92-1094.2">texts</entity> for domain-specific cues. These quick relevancy judgements require two steps: (1) recognizing an <entity id="H92-1094.3">expression</entity> that is highly relevant to the <entity id="H92-1094.4">given domain</entity>, e.g. "were killed" in the <entity id="H92-1094.5">domain of terrorism</entity>, and (2) verifying that the <entity id="H92-1094.6">context</entity> surrounding the <entity id="H92-1094.7">expression</entity> is consistent with the <entity id="H92-1094.8">relevancy guidelines</entity> for the <entity id="H92-1094.9">domain</entity>, e.g. "5 soldiers were killed by guerrillas" is not consistent with the <entity id="H92-1094.10">terrorism domain</entity> since victims of terrorist acts must be civilians. The <entity id="H92-1094.11">Relevancy Signatures Algorithm</entity> attempts to simulate the first step in this process by deriving <entity id="H92-1094.12">reliable relevancy cues</entity> from a <entity id="H92-1094.13">corpus of training texts</entity> and using these cues to quickly identify <entity id="H92-1094.14">new texts</entity> that are highly likely to be relevant. But since this <entity id="H92-1094.15">algorithm</entity> makes no attempt to look beyond the relevancy cues, it will occasionally misclassify <entity id="H92-1094.16">texts</entity> when the <entity id="H92-1094.17">surrounding context</entity> contains additional information that makes the <entity id="H92-1094.18">text</entity> irrelevant.
</abstract>

</text>

<text id="H01-1050">
<title>Mandarin-English Information: Investigating Translingual Speech Retrieval</title>
<abstract>
This paper describes the <entity id="H01-1050.1">Mandarin-English Information (MEI) project</entity>, where we investigated the problem of <entity id="H01-1050.2">cross-language spoken document retrieval (CL-SDR)</entity>, and developed one of the first <entity id="H01-1050.3">English-Chinese CL-SDR systems</entity>. Our system accepts an entire <entity id="H01-1050.4">English news story (text)</entity> as <entity id="H01-1050.5">query</entity>, and retrieves <entity id="H01-1050.6">relevant Chinese broadcast news stories (audio)</entity> from the <entity id="H01-1050.7">document collection</entity>. Hence this is a <entity id="H01-1050.8">cross-language and cross-media retrieval task</entity>. We applied a multi-scale approach to our problem, which unifies the use of <entity id="H01-1050.9">phrases</entity>, <entity id="H01-1050.10">words</entity> and <entity id="H01-1050.11">subwords</entity> in <entity id="H01-1050.12">retrieval</entity>. The <entity id="H01-1050.13">English queries</entity> are translated into <entity id="H01-1050.14">Chinese</entity> by means of a <entity id="H01-1050.15">dictionary-based approach</entity>, where we have integrated <entity id="H01-1050.16">phrase-based translation</entity> with <entity id="H01-1050.17">word-by-word translation</entity>. <entity id="H01-1050.18">Untranslatable named entities</entity> are transliterated by a <entity id="H01-1050.19">novel subword translation technique</entity>. The multi-scale approach can be divided into three subtasks - <entity id="H01-1050.20">multi-scale query formulation</entity>, <entity id="H01-1050.21">multi-scale audio indexing</entity> (by <entity id="H01-1050.22">speech recognition</entity>) and <entity id="H01-1050.23">multi-scale retrieval</entity>. Experimental results demonstrate that the use of <entity id="H01-1050.24">phrase-based translation</entity> and <entity id="H01-1050.25">subword translation</entity> gave performance gains, and <entity id="H01-1050.26">multi-scale retrieval</entity> outperforms <entity id="H01-1050.27">word-based retrieval</entity>.
</abstract>

</text>

<text id="W98-1240">
<title>The Segmentation Problem In Morphology Learning</title>
<abstract>
"Recently there has been a <entity id="W98-1240.1">large literature</entity> on various approaches to learning <entity id="W98-1240.2">morphology</entity>, and the success and cognitive plausibility of different approaches (Rumelhart and McClelland (1986), MacWhinney and Leinbach (1991) arguing for <entity id="W98-1240.3">connectionist models</entity>, Pinker and Prince (1988), Lachter and Bever (1988), Marcus et al. (1992) arguing against <entity id="W98-1240.4">connectionist models</entity>, Ling and Marinov (1993), Ling (1994) using <entity id="W98-1240.5">ID3/C4.5 decision trees</entity>, and Mooney and Califf (1995, 1996) using <entity id="W98-1240.6">inductive logic programming decision lists</entity>, among others). However - except for a couple of forays into German - this <entity id="W98-1240.7">literature</entity> has been exclusively concerned with the learning of the <entity id="W98-1240.8">English past tense</entity>. This has not worried some. Ling is happy to describe it as "a landmark task". But while the <entity id="W98-1240.9">English past tense</entity> has some interesting <entity id="W98-1240.10">features</entity> in its combination of <entity id="W98-1240.11">regular rules</entity> with <entity id="W98-1240.12">semi-productive strong verb patterns</entity>, it is in many other respects a very trivial <entity id="W98-1240.13">morphological system</entity> - reflecting the generally <entity id="W98-1240.14">vestigal nature</entity> of <entity id="W98-1240.15">inflectional morphology</entity> within <entity id="W98-1240.16">modern English</entity>.
</abstract>

</text>

<text id="W01-0804">
<title>Logical Form Equivalence: The Case Of Referring Expressions Generation</title>
<abstract>
We examine the principle of <entity id="W01-0804.1">co-extensivity</entity> which underlies current algorithms for the generation of <entity id="W01-0804.2">referring expressions</entity>, and investigate to what extent the principle allows these algorithms to be generalized. The discussion focusses on the <entity id="W01-0804.3">generation</entity> of <entity id="W01-0804.4">complex Boolean descriptions</entity> and <entity id="W01-0804.5">sentence aggregation</entity>.
</abstract>

</text>

<text id="W03-1002">
<title>Statistical Machine Translation Using Coercive Two-Level Syntactic Transduction</title>
<abstract>
We define, implement and evaluate a novel model for <entity id="W03-1002.1">statistical machine translation</entity>, which is based on <entity id="W03-1002.2">shallow syntactic analysis</entity> (<entity id="W03-1002.3">part-of-speech tagging</entity> and <entity id="W03-1002.4">phrase chunking</entity>) in both the <entity id="W03-1002.5">source and target languages</entity>. It is able to model <entity id="W03-1002.6">long-distance constituent motion</entity> and other <entity id="W03-1002.7">syntactic phenomena</entity> without requiring a <entity id="W03-1002.8">full parse</entity> in either <entity id="W03-1002.9">language</entity>. We also examine aspects of <entity id="W03-1002.10">lexical transfer</entity>, suggesting and exploring a <entity id="W03-1002.11">concept</entity> of <entity id="W03-1002.12">translation coercion</entity>.
</abstract>

</text>

<text id="W08-0309">
<title>Further Meta-Evaluation of Machine Translation</title>
<abstract>
This paper analyzes the <entity id="W08-0309.1">translation quality</entity> of <entity id="W08-0309.2">machine translation systems</entity> for 10 <entity id="W08-0309.3">language pairs</entity> translating between <entity id="W08-0309.4">Czech</entity>, <entity id="W08-0309.5">English</entity>, <entity id="W08-0309.6">French</entity>, <entity id="W08-0309.7">German</entity>, <entity id="W08-0309.8">Hungarian</entity>, and <entity id="W08-0309.9">Spanish</entity>. We report the <entity id="W08-0309.10">translation quality</entity> of over 30 diverse <entity id="W08-0309.11">translation systems</entity> based on a <entity id="W08-0309.12">large-scale manual evaluation</entity> involving hundreds of hours of effort. We use the <entity id="W08-0309.13">human judgments</entity> of the systems to analyze <entity id="W08-0309.14">automatic evaluation metrics</entity> for <entity id="W08-0309.15">translation quality</entity>, and we report the strength of the correlation with human judgments at both the system-level and at the <entity id="W08-0309.16">sentence-level</entity>. We validate our <entity id="W08-0309.17">manual evaluation methodology</entity> by measuring <entity id="W08-0309.18">intra- and inter-annotator agreement</entity>, and collecting timing information.
</abstract>

</text>

<text id="P98-2220">
<title>Automatic English-Chinese name transliteration for development of multilingual resources</title>
<abstract>
In this paper, we describe issues in the <entity id="P98-2220.1">translation</entity> of <entity id="P98-2220.2">proper names</entity> from <entity id="P98-2220.3">English</entity> to <entity id="P98-2220.4">Chinese</entity> which we have faced in constructing a system for <entity id="P98-2220.5">multilingual text generation</entity> supporting both <entity id="P98-2220.6">languages</entity>. We introduce an algorithm for mapping from <entity id="P98-2220.7">English names</entity> to <entity id="P98-2220.8">Chinese characters</entity> based on (1) heuristics about relationships between <entity id="P98-2220.9">English spelling</entity> and <entity id="P98-2220.10">pronunciation</entity>, and (2) consistent relationships between <entity id="P98-2220.11">English phonemes</entity> and <entity id="P98-2220.12">Chinese characters</entity>.
</abstract>

</text>

<text id="W04-1306">
<title>On The Acquisition Of Phonological Representations</title>
<abstract><entity id="W04-1306.1">Language learners</entity> must acquire the <entity id="W04-1306.2">grammar</entity> (rules, constraints, principles) of their <entity id="W04-1306.3">language</entity> as well as representations at various levels. I will argue that representations are part of the <entity id="W04-1306.4">grammar</entity> and must be acquired together with other aspects of <entity id="W04-1306.5">grammar</entity>; thus, <entity id="W04-1306.6">grammar acquisition</entity> may not presuppose <entity id="W04-1306.7">knowledge</entity> of representations. Further, I will argue that the goal of a learning model should not be to try to match or approximate target forms directly, because strategies to do so are defeated by the disconnect between principles of <entity id="W04-1306.8">grammar</entity> and the effects they produce. Rather, learners should use target forms as evidence bearing on the selection of the correct <entity id="W04-1306.9">grammar</entity>. I will draw on two areas of <entity id="W04-1306.10">phonology</entity> to illustrate these arguments. The first is the <entity id="W04-1306.11">grammar of stress</entity>, or <entity id="W04-1306.12">metrical phonology</entity>, which has received much attention in the <entity id="W04-1306.13">learning model literature</entity>. The second concerns the acquisition of <entity id="W04-1306.14">phonological features and contrasts</entity>. This aspect of acquisition turns out, contrary to first appearances, to pose challenging problems for learning models.
</abstract>

</text>

<text id="W06-0119">
<title>BMM-Based Chinese Word Segmentor With Word Support Model For The SIGHAN Bakeoff 2006</title>
<abstract>
This paper describes a <entity id="W06-0119.1">Chinese word segmentor (CWS)</entity> for the <entity id="W06-0119.2">third International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2006)</entity>. We participate in the <entity id="W06-0119.3">word segmentation task</entity> at the <entity id="W06-0119.4">Microsoft Research (MSR) closed testing track</entity>. Our <entity id="W06-0119.5">CWS</entity> is based on <entity id="W06-0119.6">backward maximum matching</entity> with <entity id="W06-0119.7">word support model (WSM)</entity> and <entity id="W06-0119.8">contextual-based Chinese unknown word identification</entity>. From the scored results and our experimental results, it shows <entity id="W06-0119.9">WSM</entity> can improve our <entity id="W06-0119.10">previous CWS</entity>, which was reported at the <entity id="W06-0119.11">SIGHAN Bakeoff 2005</entity>, about 1% of <entity id="W06-0119.12">F-measure</entity>.
</abstract>

</text>

<text id="W06-3103">
<title>Morpho-Syntactic Arabic Preprocessing For Arabic To English Statistical Machine Translation</title>
<abstract>
The <entity id="W06-3103.1">Arabic language</entity> has far richer <entity id="W06-3103.2">systems of inflection and derivation</entity> than <entity id="W06-3103.3">English</entity> which has very little <entity id="W06-3103.4">morphology</entity>. This <entity id="W06-3103.5">morphology difference</entity> causes a large gap between the <entity id="W06-3103.6">vocabulary sizes</entity> in any given <entity id="W06-3103.7">parallel training corpus</entity>. <entity id="W06-3103.8">Segmentation</entity> of <entity id="W06-3103.9">inflected Arabic words</entity> is a way to smooth its <entity id="W06-3103.10">highly morphological nature</entity>. In this paper, we describe some <entity id="W06-3103.11">statistically and linguistically motivated methods</entity> for <entity id="W06-3103.12">Arabic word segmentation</entity>. Then, we show the efficiency of <entity id="W06-3103.13">proposed methods</entity> on the <entity id="W06-3103.14">Arabic-English BTEC and NIST tasks</entity>.
</abstract>

</text>

<text id="W06-1647">
<title>Lexicon Acquisition For Dialectal Arabic Using Transductive Learning</title>
<abstract>
We investigate the problem of learning a <entity id="W06-1647.1">part-of-speech (POS) lexicon</entity> for a <entity id="W06-1647.2">resource-poor language</entity>, <entity id="W06-1647.3">dialectal Arabic</entity>. Developing a <entity id="W06-1647.4">high-quality lexicon</entity> is often the first step towards building a <entity id="W06-1647.5">POS tagger</entity>, which is in turn the front-end to many <entity id="W06-1647.6">NLP systems</entity>. We frame the <entity id="W06-1647.7">lexicon acquisition problem</entity> as a <entity id="W06-1647.8">transductive learning problem</entity>, and perform comparisons on three <entity id="W06-1647.9">transductive algorithms</entity>: <entity id="W06-1647.10">Transductive SVMs</entity>, <entity id="W06-1647.11">Spectral Graph Transducers</entity>, and a <entity id="W06-1647.12">novel Transductive Clustering method</entity>. We demonstrate that <entity id="W06-1647.13">lexicon learning</entity> is an important task in <entity id="W06-1647.14">resource-poor domains</entity> and leads to significant improvements in <entity id="W06-1647.15">tagging accuracy</entity> for <entity id="W06-1647.16">dialectal Arabic</entity>.
</abstract>

</text>

<text id="P07-2013">
<title>An API for Measuring the Relatedness of Words in Wikipedia</title>
<abstract>
We present an <entity id="P07-2013.1">API</entity> for computing the <entity id="P07-2013.2">semantic relatedness</entity> of <entity id="P07-2013.3">words</entity> in <entity id="P07-2013.4">Wikipedia</entity>.
</abstract>

</text>

<text id="C08-1138">
<title>Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation</title>
<abstract>
This paper presents a general platform, namely <entity id="C08-1138.1">synchronous tree sequence substitution grammar (STSSG)</entity>, for the <entity id="C08-1138.2">grammar comparison study</entity> in <entity id="C08-1138.3">Translational Equivalence Modeling (TEM)</entity> and <entity id="C08-1138.4">Statistical Machine Translation (SMT)</entity>. Under the <entity id="C08-1138.5">STSSG platform</entity>, we compare the <entity id="C08-1138.6">expressive abilities</entity> of <entity id="C08-1138.7">various grammars</entity> through <entity id="C08-1138.8">synchronous parsing</entity> and a <entity id="C08-1138.9">real translation platform</entity> on a variety of <entity id="C08-1138.10">Chinese-English bilingual corpora</entity>. Experimental results show that the <entity id="C08-1138.11">STSSG</entity> is able to better explain the data in <entity id="C08-1138.12">parallel corpora</entity> than <entity id="C08-1138.13">other grammars</entity>. Our study further finds that the complexity of <entity id="C08-1138.14">structure divergence</entity> is much higher than suggested in literature, which imposes a big challenge to <entity id="C08-1138.15">syntactic transformation-based SMT</entity>.
</abstract>

</text>

<text id="P04-3020">
<title>Graph-Based Ranking Algorithms For Sentence Extraction, Applied To Text Summarization</title>
<abstract>
This paper presents an innovative <entity id="P04-3020.1">unsupervised method</entity> for <entity id="P04-3020.2">automatic sentence extraction</entity> using <entity id="P04-3020.3">graph-based ranking algorithms</entity>. We evaluate the <entity id="P04-3020.4">method</entity> in the context of a <entity id="P04-3020.5">text summarization task</entity>, and show that the results obtained compare favorably with previously published results on <entity id="P04-3020.6">established benchmarks</entity>.
</abstract>

</text>

<text id="C90-2017">
<title>Discourse Anaphora</title>
<abstract>
This paper reports on a model that serves <entity id="C90-2017.1">anaphora resolution</entity>. A distinction will be made between <entity id="C90-2017.2">possible antecedents</entity> and <entity id="C90-2017.3">preferred antecedents</entity>. The set of <entity id="C90-2017.4">linguistically possible candidates</entity> will be defined in terms of <entity id="C90-2017.5">compatibility</entity> and <entity id="C90-2017.6">recency</entity>. <entity id="C90-2017.7">Preferred antecedents</entity> are a subset of the <entity id="C90-2017.8">possible antecedents</entity>, selected by the application of <entity id="C90-2017.9">extralinguistic knowledge</entity>. Motivation for the particular design and comparison with <entity id="C90-2017.10">other approaches</entity> are extensive.
</abstract>

</text>

<text id="C88-2123">
<title>A Syntactic Description Of German In A Formalism Designed For Machine Translation</title>
<abstract>
This paper presents a <entity id="C88-2123.1">syntactic description</entity> of a fragment of <entity id="C88-2123.2">German</entity> that has been worked out within the <entity id="C88-2123.3">machine translation project Eurotra</entity>. It represents the <entity id="C88-2123.4">syntactic part</entity> of the <entity id="C88-2123.5">German module</entity> of this <entity id="C88-2123.6">multilingual translation system</entity>. The <entity id="C88-2123.7">linguistic tool</entity> for the following analyses is the so-called <entity id="C88-2123.8">CAT-frame work</entity>. In the first two sections of this paper an introduction of the <entity id="C88-2123.9">formalism</entity> and a <entity id="C88-2123.10">linguistic characterization</entity> of the <entity id="C88-2123.11">framework</entity> is given. The <entity id="C88-2123.12">CAT formalism</entity> as a whole is a <entity id="C88-2123.13">theory of machine translation</entity>, the <entity id="C88-2123.14">syntactic analysis part</entity> which is the subject of this paper is an <entity id="C88-2123.15">LFG-like mapping</entity> of a <entity id="C88-2123.16">constituent structure</entity> onto a <entity id="C88-2123.17">functional structure</entity>. A third section develops principles for a <entity id="C88-2123.18">phrase structure</entity> and a <entity id="C88-2123.19">functional structure</entity> for <entity id="C88-2123.20">German</entity> and the mapping of <entity id="C88-2123.21">phrase structure</entity> onto <entity id="C88-2123.22">functional structure</entity>. In a fourth section a treatment of <entity id="C88-2123.23">unbounded movement phenomena</entity> is sketched. As the <entity id="C88-2123.24">CAT-framework</entity> does not provide any <entity id="C88-2123.25">global mechanisms</entity> I try to give a <entity id="C88-2123.26">local treatment</entity> of this problem.
</abstract>

</text>

<text id="L08-1190">
<title>Ontology-Based XQuerying of XML-Encoded Language Resources on Multiple Annotation Layers</title>
<abstract>
We present an approach for querying collections of <entity id="L08-1190.1">heterogeneous linguistic corpora</entity> that are annotated on <entity id="L08-1190.2">multiple layers</entity> using arbitrary <entity id="L08-1190.3">XML-based markup languages</entity>. An <entity id="L08-1190.4">OWL ontology</entity> provides a homogenising view on the conceptually different <entity id="L08-1190.5">markup languages</entity> so that a common <entity id="L08-1190.6">querying framework</entity> can be established using the method of <entity id="L08-1190.7">ontology-based query expansion</entity>. In addition, we present a <entity id="L08-1190.8">highly flexible web-based graphical interface</entity> that can be used to query <entity id="L08-1190.9">corpora</entity> with regard to several different <entity id="L08-1190.10">linguistic properties</entity> such as, for example, <entity id="L08-1190.11">syntactic tree fragments</entity>. This interface can also be used for <entity id="L08-1190.12">ontology-based querying</entity> of multiple <entity id="L08-1190.13">corpora</entity> simultaneously.
</abstract>

</text>

<text id="E06-1044">
<title>Modelling Semantic Role Pausibility In Human Sentence Processing</title>
<abstract>
We present the <entity id="E06-1044.1">psycholinguistically motivated task</entity> of predicting <entity id="E06-1044.2">human plausibility judgements</entity> for <entity id="E06-1044.3">verb-role-argument triples</entity> and introduce a <entity id="E06-1044.4">probabilistic model</entity> that solves it. We also evaluate our <entity id="E06-1044.5">model</entity> on the related <entity id="E06-1044.6">role-labelling task</entity>, and compare it with a <entity id="E06-1044.7">standard role labeller</entity>. For <entity id="E06-1044.8">both tasks</entity>, our <entity id="E06-1044.9">model</entity> benefits from <entity id="E06-1044.10">class-based smoothing</entity>, which allows it to make correct <entity id="E06-1044.11">argument-specific predictions</entity> despite a severe <entity id="E06-1044.12">sparse data problem</entity>. The <entity id="E06-1044.13">standard labeller</entity> suffers from <entity id="E06-1044.14">sparse data</entity> and a strong reliance on <entity id="E06-1044.15">syntactic cues</entity>, especially in the <entity id="E06-1044.16">prediction task</entity>.
</abstract>

</text>

<text id="C04-1178">
<title>Semiautomatic Extension Of CoreNet Using A Bootstrapping Mechanism On Corpus-Based Co-Occurrences</title>
<abstract>
The paper describes a language-independent approach for <entity id="C04-1178.1">semiautomatic extension</entity> of <entity id="C04-1178.2">lexical-semantic word nets</entity> and evaluates the <entity id="C04-1178.3">method</entity> on <entity id="C04-1178.4">CoreNet, the Korean version of word net</entity>. In a <entity id="C04-1178.5">bootstrapping fashion</entity>, the so-called <entity id="C04-1178.6">'Pendulum Algorithm'</entity> operates on <entity id="C04-1178.7">word sets</entity> obtained by <entity id="C04-1178.8">co-occurrence statistics</entity> on a <entity id="C04-1178.9">large un-annotated corpus</entity> and keeps <entity id="C04-1178.10">error propagation</entity> low by a <entity id="C04-1178.11">verification step</entity>. Results are not sufficient for <entity id="C04-1178.12">automatic extension</entity>, but provide a good <entity id="C04-1178.13">candidate set</entity>. Further improvements are discussed.
</abstract>

</text>

<text id="H94-1066">
<title>Signal Processing For Robust Speech Recognition</title>
<abstract>
This paper describes a series of <entity id="H94-1066.1">cepstral-based compensation procedures</entity> that render the <entity id="H94-1066.2">SPHINX-II system</entity> more robust with respect to <entity id="H94-1066.3">acoustical environment</entity>. The first algorithm, <entity id="H94-1066.4">phone-dependent cepstral compensation</entity>, is similar in concept to the previously-described <entity id="H94-1066.5">MFCDCN method</entity>, except that <entity id="H94-1066.6">cepstral compensation vectors</entity> are selected according to the current <entity id="H94-1066.7">phonetic hypothesis</entity>, rather than on the basis of <entity id="H94-1066.8">SNR or VQ codeword identity</entity>. We also describe two procedures to accomplish adaptation of the <entity id="H94-1066.9">VQ codebook</entity> for new environments, as well as the use of <entity id="H94-1066.10">reduced-bandwidth frequency analysis</entity> to process <entity id="H94-1066.11">telephone-bandwidth speech</entity>. Use of the various <entity id="H94-1066.12">compensation algorithms</entity> in consort produces a <entity id="H94-1066.13">reduction of error rates</entity> for <entity id="H94-1066.14">SPHINX-II</entity> by as much as 40 percent relative to the rate achieved with <entity id="H94-1066.15">cepstral mean normalization</entity> alone, in both <entity id="H94-1066.16">development test sets</entity> and in the context of the <entity id="H94-1066.17">1993 ARPA CSR evaluations</entity>.
</abstract>

</text>

<text id="C04-1182">
<title>Analysis And Detection Of Reading Miscues For Interactive Literacy Tutors</title>
<abstract>
The <entity id="C04-1182.1">Colorado Literacy Tutor (CLT)</entity> is a <entity id="C04-1182.2">technology-based literacy program</entity>, designed on the basis of <entity id="C04-1182.3">cognitive theory</entity> and scientifically motivated reading research, which aims to improve literacy and student achievement in public schools. One of the critical components of the <entity id="C04-1182.4">CLT</entity> is a <entity id="C04-1182.5">speech recognition system</entity> which is used to track the child's progress during <entity id="C04-1182.6">oral reading</entity> and to provide sufficient information to detect <entity id="C04-1182.7">reading miscues</entity>. In this paper, we extend on prior work by examining a novel <entity id="C04-1182.8">labeling</entity> of children's <entity id="C04-1182.9">oral reading audio data</entity> in order to better understand the factors that contribute most significantly to <entity id="C04-1182.10">speech recognition errors</entity>. While these <entity id="C04-1182.11">events</entity> make up nearly 8% of the data, they are shown to account for approximately 30% of the <entity id="C04-1182.12">word errors</entity> in a <entity id="C04-1182.13">state-of-the-art speech recognizer</entity>. Next, we consider the problem of detecting <entity id="C04-1182.14">miscues</entity> during <entity id="C04-1182.15">oral reading</entity>. Using <entity id="C04-1182.16">features</entity> derived from the <entity id="C04-1182.17">speech recognizer</entity>, we demonstrate that 67% of <entity id="C04-1182.18">reading miscues</entity> can be detected at a <entity id="C04-1182.19">false alarm rate</entity> of 3%.
</abstract>

</text>


</doc>
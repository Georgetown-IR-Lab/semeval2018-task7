<?xml version="1.0" encoding="UTF-8"?>
<doc>
<text id="L08-1239">
<title>Automatic extraction of subcategorization frames for Italian</title>
<abstract>
Subcategorization is a <entity id="L08-1239.1">kind</entity> of <entity id="L08-1239.2">knowledge</entity> which can be considered as crucial in several <entity id="L08-1239.3">NLP tasks</entity>, such as <entity id="L08-1239.4">Information Extraction</entity> or <entity id="L08-1239.5">parsing</entity>, but the <entity id="L08-1239.6">collection</entity> of very large <entity id="L08-1239.7">resources</entity> <entity id="L08-1239.8">including</entity> subcategorization <entity id="L08-1239.9">representation</entity> is difficult and <entity id="L08-1239.10">time-consuming</entity>. Various <entity id="L08-1239.11">experiences</entity> show that the <entity id="L08-1239.12">automatic</entity> <entity id="L08-1239.13">extraction</entity> can be a practical and reliable <entity id="L08-1239.14">solution</entity> for acquiring such a <entity id="L08-1239.15">kind</entity> of <entity id="L08-1239.16">knowledge</entity>. The aim of this <entity id="L08-1239.17">paper</entity> is at investigating the <entity id="L08-1239.18">relationships</entity> between subcategorization <entity id="L08-1239.19">frame</entity> <entity id="L08-1239.20">extraction</entity> and the <entity id="L08-1239.21">nature</entity> of<entity id="L08-1239.22">data</entity> from which the <entity id="L08-1239.23">frames</entity> have to be <entity id="L08-1239.24">extracted</entity>, e.g. how much the <entity id="L08-1239.25">task</entity> can be <entity id="L08-1239.26">influenced</entity> by the <entity id="L08-1239.27">richness</entity>/poorness of the annotation. Therefore, we present some <entity id="L08-1239.28">experiments</entity> that <entity id="L08-1239.29">apply</entity> <entity id="L08-1239.30">statistical</entity> subcategorization <entity id="L08-1239.31">extraction methods</entity>, known in <entity id="L08-1239.32">literature</entity>, on an Italian treebank that exploits a rich set of <entity id="L08-1239.33">dependency relations</entity> that can be annotated at different <entity id="L08-1239.34">degrees</entity> of <entity id="L08-1239.35">specificity</entity>. Benefiting of the <entity id="L08-1239.36">availability</entity> of <entity id="L08-1239.37">relation</entity> sets that <entity id="L08-1239.38">implement</entity> different granularity in the <entity id="L08-1239.39">representation</entity> of <entity id="L08-1239.40">relations</entity>, we <entity id="L08-1239.41">evaluate</entity> our <entity id="L08-1239.42">results</entity> with <entity id="L08-1239.43">reference</entity> to previous works in a <entity id="L08-1239.44">cross-linguistic</entity> <entity id="L08-1239.45">perspective</entity>.
</abstract>

</text>

<text id="L08-1342">
<title>Benchmarking Textual Annotation Tools for the Semantic Web</title>
<abstract>
This <entity id="L08-1342.1">paper</entity> investigates the state of the art in <entity id="L08-1342.2">automatic</entity> textual annotation <entity id="L08-1342.3">tools</entity>, and examines the <entity id="L08-1342.4">extent</entity> to which they are ready for use in the <entity id="L08-1342.5">real world</entity>. We define some benchmarking <entity id="L08-1342.6">criteria</entity> for measuring the usability of annotation <entity id="L08-1342.7">tools</entity>, and examine those <entity id="L08-1342.8">factors</entity> which are particularly important for a real <entity id="L08-1342.9">user</entity> to be able to determine which is the most suitable <entity id="L08-1342.10">tool</entity> for their use. We discuss <entity id="L08-1342.11">factors</entity> such as usability, accessibility, interoperability and <entity id="L08-1342.12">scalability</entity>, and <entity id="L08-1342.13">evaluate</entity> a set of annotation <entity id="L08-1342.14">tools</entity> according to these <entity id="L08-1342.15">factors</entity>. Finally, we draw some <entity id="L08-1342.16">conclusions</entity> about the <entity id="L08-1342.17">current</entity> state of <entity id="L08-1342.18">research</entity> in annotation and make some <entity id="L08-1342.19">suggestions</entity> for the future.
</abstract>

</text>

<text id="L08-1483">
<title>Building of a Speech Corpus Optimised for Unit Selection TTS Synthesis</title>
<abstract>
The <entity id="L08-1483.1">paper</entity> <entity id="L08-1483.2">deals</entity> with the <entity id="L08-1483.3">process</entity> of <entity id="L08-1483.4">designing</entity> a phonetically and prosodically rich <entity id="L08-1483.5">speech corpus</entity> for <entity id="L08-1483.6">unit</entity> <entity id="L08-1483.7">selection</entity> <entity id="L08-1483.8">speech synthesis</entity>. The attention is given mainly to the <entity id="L08-1483.9">recording</entity> and <entity id="L08-1483.10">verification</entity> stage of the <entity id="L08-1483.11">process</entity>. In <entity id="L08-1483.12">order</entity> to ensure as <entity id="L08-1483.13">high quality</entity> and <entity id="L08-1483.14">consistency</entity> of the <entity id="L08-1483.15">recordings</entity> as possible, a special <entity id="L08-1483.16">recording</entity> <entity id="L08-1483.17">environment</entity> consisting of a <entity id="L08-1483.18">recording</entity> session <entity id="L08-1483.19">management</entity> and "pluggable" <entity id="L08-1483.20">chain</entity> of checking <entity id="L08-1483.21">modules</entity> was <entity id="L08-1483.22">designed</entity> and utilised. Other stages, namely <entity id="L08-1483.23">text</entity> <entity id="L08-1483.24">collection</entity> (<entity id="L08-1483.25">including</entity>) both phonetically and prosodically balanced <entity id="L08-1483.26">sentence</entity> <entity id="L08-1483.27">selection</entity> and a careful annotation on both orthographic and phonetic <entity id="L08-1483.28">level</entity> are also <entity id="L08-1483.29">mentioned</entity>.
</abstract>

</text>

<text id="L08-1487">
<title>Communicating Unknown Words in Machine Translation</title>
<abstract>
A new <entity id="L08-1487.1">approach</entity> to handle <entity id="L08-1487.2">unknown words</entity> in <entity id="L08-1487.3">machine translation</entity> is presented. The <entity id="L08-1487.4">basic</entity> idea is to find <entity id="L08-1487.5">definitions</entity> for the <entity id="L08-1487.6">unknown words</entity> on the <entity id="L08-1487.7">source language</entity> <entity id="L08-1487.8">side</entity> and <entity id="L08-1487.9">translate</entity> those <entity id="L08-1487.10">definitions</entity> instead. Only monolingual <entity id="L08-1487.11">resources</entity> are <entity id="L08-1487.12">required</entity>, which generally offer a broader <entity id="L08-1487.13">coverage</entity> than bilingual <entity id="L08-1487.14">resources</entity> and are available for a large <entity id="L08-1487.15">number</entity> of <entity id="L08-1487.16">languages</entity>. In <entity id="L08-1487.17">order</entity> to use this in a <entity id="L08-1487.18">machine translation system</entity> <entity id="L08-1487.19">definitions</entity> are <entity id="L08-1487.20">extracted</entity> automatically from online <entity id="L08-1487.21">dictionaries</entity> and <entity id="L08-1487.22">encyclopedias</entity>. The <entity id="L08-1487.23">translated</entity> <entity id="L08-1487.24">definition</entity> is then inserted and clearly marked in the original <entity id="L08-1487.25">hypothesis</entity>. This is shown to lead to significant <entity id="L08-1487.26">improvements</entity> in (subjective) <entity id="L08-1487.27">translation quality</entity>. clear if it will be a positive or a negative <entity id="L08-1487.28">event</entity>. The first <entity id="L08-1487.29">sentence</entity> is relatively understandable, but there is the <entity id="L08-1487.30">possibility</entity> that the <entity id="L08-1487.31">unknown word</entity> might negate the actual <entity id="L08-1487.32">sentence</entity>. A <entity id="L08-1487.33">background</entity> <entity id="L08-1487.34">lexicon</entity> can ameliorate this <entity id="L08-1487.35">situation</entity>, but it will not be possible to have a <entity id="L08-1487.36">lexicon</entity> covering all <entity id="L08-1487.37">words</entity>. The <entity id="L08-1487.38">unknown word</entity> <entity id="L08-1487.39">problem</entity> is especially severe for small portable <entity id="L08-1487.40">translation systems</entity>, as here the <entity id="L08-1487.41">vocabulary</entity> has to be <entity id="L08-1487.42">limited</entity> to be able to put the <entity id="L08-1487.43">translation system</entity> on the portable <entity id="L08-1487.44">device</entity>.
</abstract>

</text>

<text id="L08-1521">
<title>Automatic Rich Annotation of Large Corpus of Conversational transcribed speech: the Chunking Task of the EPAC Project</title>
<abstract>
This <entity id="L08-1521.1">paper</entity> describes the use of the CasSys <entity id="L08-1521.2">platform</entity> in <entity id="L08-1521.3">order</entity> to achieve the <entity id="L08-1521.4">chunking</entity> of conversational <entity id="L08-1521.5">speech</entity> <entity id="L08-1521.6">transcripts</entity> by means of <entity id="L08-1521.7">cascades</entity> of Unitex transducers. Our <entity id="L08-1521.8">system</entity> is involved in the EPAC <entity id="L08-1521.9">project</entity> of the French National agency of <entity id="L08-1521.10">Research</entity> (ANR). The aim of this <entity id="L08-1521.11">project</entity> is to <entity id="L08-1521.12">develop</entity> <entity id="L08-1521.13">robust</entity> <entity id="L08-1521.14">methods</entity> for the annotation of audio/multimedia <entity id="L08-1521.15">document</entity> <entity id="L08-1521.16">collections</entity> which contains conversational <entity id="L08-1521.17">speech</entity> <entity id="L08-1521.18">sequences</entity> such as TV or radio <entity id="L08-1521.19">programs</entity>. At first, this <entity id="L08-1521.20">paper</entity> presents the EPAC <entity id="L08-1521.21">project</entity> and the <entity id="L08-1521.22">adaptation</entity> of a former <entity id="L08-1521.23">chunking</entity> <entity id="L08-1521.24">system</entity> (Romus) which was <entity id="L08-1521.25">developed</entity> in the restricted <entity id="L08-1521.26">framework</entity> of dedicated spoken <entity id="L08-1521.27">man-machine</entity> <entity id="L08-1521.28">dialogue</entity>. Then, it describes the <entity id="L08-1521.29">problems</entity> that are arising <entity id="L08-1521.30">due</entity> to 1) <entity id="L08-1521.31">spontaneous speech</entity> disfluencies and 2) <entity id="L08-1521.32">errors</entity> for the previous stages of <entity id="L08-1521.33">processing</entity> (<entity id="L08-1521.34">automatic speech recognition</entity> and <entity id="L08-1521.35">POS tagging</entity>).
</abstract>

</text>

<text id="L08-1577">
<title>Constructing a Corpus that Indicates Patterns of Modification between Draft and Final Translations by Human Translators</title>
<abstract>
In human <entity id="L08-1577.1">translation</entity>, <entity id="L08-1577.2">translators</entity> first make draft <entity id="L08-1577.3">translations</entity> and then modify and edit them. In the <entity id="L08-1577.4">case</entity> of <entity id="L08-1577.5">experienced</entity> <entity id="L08-1577.6">translators</entity>, this <entity id="L08-1577.7">process</entity> involves the use of wide-ranging <entity id="L08-1577.8">expert</entity> <entity id="L08-1577.9">knowledge</entity>, which has mostly remained implicit so far. Describing the <entity id="L08-1577.10">difference</entity> between draft and final <entity id="L08-1577.11">translations</entity>, therefore, should contribute to making this <entity id="L08-1577.12">knowledge</entity> explicit. If we could clarify the <entity id="L08-1577.13">expert</entity> <entity id="L08-1577.14">knowledge</entity> of <entity id="L08-1577.15">translators</entity>, hopefully in a computationally tractable way, we would be able to contribute to the <entity id="L08-1577.16">automatic</entity> notification of awkward <entity id="L08-1577.17">translations</entity> to assist inexperienced <entity id="L08-1577.18">translators</entity>, <entity id="L08-1577.19">improving</entity> the <entity id="L08-1577.20">quality</entity> of MT <entity id="L08-1577.21">output</entity>, etc. Against this backdrop, we have started <entity id="L08-1577.22">constructing</entity> a <entity id="L08-1577.23">corpus</entity> that indicates <entity id="L08-1577.24">patterns</entity> of <entity id="L08-1577.25">modification</entity> between draft and final <entity id="L08-1577.26">translations</entity> made by human <entity id="L08-1577.27">translators</entity>. This <entity id="L08-1577.28">paper</entity> <entity id="L08-1577.29">reports</entity> on our <entity id="L08-1577.30">progress</entity> to <entity id="L08-1577.31">date</entity>.
</abstract>

</text>

<text id="D08-1104">
<title>Construction of an Idiom Corpus and its Application to Idiom Identification based on WSD Incorporating Idiom-Specific Features</title>
<abstract>
Some <entity id="D08-1104.1">phrases</entity> can be interpreted either idiomatically (figuratively) or literally in <entity id="D08-1104.2">context</entity>, and the precise <entity id="D08-1104.3">identification</entity> of <entity id="D08-1104.4">idioms</entity> is indispensable for full-fledged <entity id="D08-1104.5">natural language processing</entity> (NLP). To this end, we have <entity id="D08-1104.6">constructed</entity> an <entity id="D08-1104.7">idiom</entity> <entity id="D08-1104.8">corpus</entity> for <entity id="D08-1104.9">Japanese</entity>. This <entity id="D08-1104.10">paper</entity> <entity id="D08-1104.11">reports</entity> on the <entity id="D08-1104.12">corpus</entity> and the <entity id="D08-1104.13">results</entity> of an <entity id="D08-1104.14">idiom</entity> <entity id="D08-1104.15">identification</entity> <entity id="D08-1104.16">experiment</entity> using the <entity id="D08-1104.17">corpus</entity>. The <entity id="D08-1104.18">corpus</entity> <entity id="D08-1104.19">targets</entity> 146 ambiguous <entity id="D08-1104.20">idioms</entity>, and consists of 102,846 <entity id="D08-1104.21">sentences</entity>, each of which is annotated with a literal/<entity id="D08-1104.22">idiom</entity> label. For <entity id="D08-1104.23">idiom</entity> <entity id="D08-1104.24">identification</entity>, we <entity id="D08-1104.25">targeted</entity> 90 out of the 146 <entity id="D08-1104.26">idioms</entity> and adopted a <entity id="D08-1104.27">word sense disambiguation</entity> (WSD) <entity id="D08-1104.28">method</entity> using both <entity id="D08-1104.29">common</entity> WSD <entity id="D08-1104.30">features</entity> and <entity id="D08-1104.31">idiom-specific</entity> <entity id="D08-1104.32">features</entity>. The <entity id="D08-1104.33">corpus</entity> and the <entity id="D08-1104.34">experiment</entity> are the largest of their <entity id="D08-1104.35">kind</entity>, as far as we know. As a <entity id="D08-1104.36">result</entity>, we found that a <entity id="D08-1104.37">standard</entity> supervised WSD <entity id="D08-1104.38">method</entity> works well for the <entity id="D08-1104.39">idiom</entity> <entity id="D08-1104.40">identification</entity> and achieved an <entity id="D08-1104.41">accuracy</entity> of 89.25% and 88.86% with/without <entity id="D08-1104.42">idiom-specific</entity> <entity id="D08-1104.43">features</entity> and that the most effective <entity id="D08-1104.44">idiom-specific</entity> <entity id="D08-1104.45">feature</entity> is the one involving the adjacency of <entity id="D08-1104.46">idiom</entity> <entity id="D08-1104.47">constituents</entity>.
</abstract>

</text>

<text id="D08-1112">
<title>An Analysis of Active Learning Strategies for Sequence Labeling Tasks</title>
<abstract><entity id="D08-1112.1">Active learning</entity> is well-suited to many <entity id="D08-1112.2">problems</entity> in <entity id="D08-1112.3">natural language processing</entity>, where unlabeled<entity id="D08-1112.4">data</entity> may be abundant but annotation is slow and expensive. This <entity id="D08-1112.5">paper</entity> aims to shed light on the best active <entity id="D08-1112.6">learning approaches</entity> for <entity id="D08-1112.7">sequence</entity> labeling <entity id="D08-1112.8">tasks</entity> such as <entity id="D08-1112.9">information extraction</entity> and <entity id="D08-1112.10">document</entity> segmentation. We <entity id="D08-1112.11">survey</entity> previously used <entity id="D08-1112.12">query</entity> <entity id="D08-1112.13">selection</entity> <entity id="D08-1112.14">strategies</entity> for <entity id="D08-1112.15">sequence</entity> <entity id="D08-1112.16">models</entity>, and <entity id="D08-1112.17">propose</entity> several novel <entity id="D08-1112.18">algorithms</entity> to address their <entity id="D08-1112.19">shortcomings</entity>. We also conduct a <entity id="D08-1112.20">large-scale</entity> empirical <entity id="D08-1112.21">comparison</entity> using multiple <entity id="D08-1112.22">corpora</entity>, which demonstrates that our <entity id="D08-1112.23">proposed</entity> <entity id="D08-1112.24">methods</entity> <entity id="D08-1112.25">advance</entity> the state of the art.
</abstract>

</text>

<text id="I05-1057">
<title>Two-Phase Biomedical Named Entity Recognition Using A Hybrid Method</title>
<abstract><entity id="I05-1057.1">Abstract</entity>. Biomedical <entity id="I05-1057.2">named</entity> <entity id="I05-1057.3">entity</entity> <entity id="I05-1057.4">recognition</entity> (NER) is a difficult <entity id="I05-1057.5">problem</entity> in biomedical <entity id="I05-1057.6">information processing</entity> <entity id="I05-1057.7">due</entity> to the widespread <entity id="I05-1057.8">ambiguity</entity> of <entity id="I05-1057.9">terms</entity> out of <entity id="I05-1057.10">context</entity> and extensive <entity id="I05-1057.11">lexical</entity> <entity id="I05-1057.12">variations</entity>. This <entity id="I05-1057.13">paper</entity> presents a <entity id="I05-1057.14">two-phase</entity> biomedical NER consisting of <entity id="I05-1057.15">term</entity> <entity id="I05-1057.16">boundary</entity> <entity id="I05-1057.17">detection</entity> and <entity id="I05-1057.18">semantic</entity> labeling. By dividing the <entity id="I05-1057.19">problem</entity>, we can adopt an effective <entity id="I05-1057.20">model</entity> for each <entity id="I05-1057.21">process</entity>. In our <entity id="I05-1057.22">study</entity>, we use two exponential <entity id="I05-1057.23">models</entity>, <entity id="I05-1057.24">conditional random fields</entity> and <entity id="I05-1057.25">maximum entropy</entity>, at each <entity id="I05-1057.26">phase</entity>. Moreover, <entity id="I05-1057.27">results</entity> by this <entity id="I05-1057.28">machine</entity> learning <entity id="I05-1057.29">based</entity> <entity id="I05-1057.30">model</entity> are refined by <entity id="I05-1057.31">rule-based</entity> postprocessing <entity id="I05-1057.32">implemented</entity> using a finite state <entity id="I05-1057.33">method</entity>. <entity id="I05-1057.34">Experiments</entity> show it achieves the <entity id="I05-1057.35">performance</entity> of F-score 71.19% on the JNLPBA 2004 shared <entity id="I05-1057.36">task</entity> of identifying 5 <entity id="I05-1057.37">classes</entity> of biomedical NEs.
</abstract>

</text>

<text id="L08-1115">
<title>Parser Evaluation and the BNC: Evaluating 4 constituency parsers with 3 metrics</title>
<abstract>
We <entity id="L08-1115.1">evaluate</entity> discriminative <entity id="L08-1115.2">parse</entity> reranking and <entity id="L08-1115.3">parser</entity> <entity id="L08-1115.4">self-training</entity> on a new <entity id="L08-1115.5">English</entity> <entity id="L08-1115.6">test set</entity> using four <entity id="L08-1115.7">versions</entity> of the Charniak <entity id="L08-1115.8">parser</entity> and a <entity id="L08-1115.9">variety</entity> of <entity id="L08-1115.10">parser</entity> <entity id="L08-1115.11">evaluation metrics</entity>. The new <entity id="L08-1115.12">test set</entity> consists of 1,000 <entity id="L08-1115.13">hand-corrected</entity> British National <entity id="L08-1115.14">Corpus</entity> <entity id="L08-1115.15">parse trees</entity>. We directly <entity id="L08-1115.16">evaluate</entity> <entity id="L08-1115.17">parser</entity> <entity id="L08-1115.18">output</entity> using both the Parseval and the Leaf Ancestor <entity id="L08-1115.19">metrics</entity>. We also convert the <entity id="L08-1115.20">hand-corrected</entity> and <entity id="L08-1115.21">parser</entity> <entity id="L08-1115.22">output</entity> <entity id="L08-1115.23">phrase structure trees</entity> to <entity id="L08-1115.24">dependency trees</entity> using a state-of-the-art functional <entity id="L08-1115.25">tag</entity> labeller and <entity id="L08-1115.26">constituent-to-dependency</entity> <entity id="L08-1115.27">conversion</entity> <entity id="L08-1115.28">tool</entity>, and then calculate label <entity id="L08-1115.29">accuracy</entity>, unlabelled <entity id="L08-1115.30">attachment</entity> and labelled <entity id="L08-1115.31">attachment</entity> scores over the <entity id="L08-1115.32">dependency structures</entity>. We find that reranking leads to a <entity id="L08-1115.33">performance improvement</entity> on the new <entity id="L08-1115.34">test set</entity> (albeit a modest one). We find that <entity id="L08-1115.35">self-training</entity> using BNC<entity id="L08-1115.36">data</entity> leads to significantly better <entity id="L08-1115.37">results</entity>. However, it is not clear how effective <entity id="L08-1115.38">self-training</entity> is when the <entity id="L08-1115.39">training</entity> material comes from the North American <entity id="L08-1115.40">News</entity> <entity id="L08-1115.41">Corpus</entity>.
</abstract>

</text>

<text id="L08-1121">
<title>Thai Broadcast News Corpus Construction and Evaluation</title>
<abstract>
Large <entity id="L08-1121.1">speech</entity> and <entity id="L08-1121.2">text</entity> <entity id="L08-1121.3">corpora</entity> are crucial to the <entity id="L08-1121.4">development</entity> of a state-of-the-art <entity id="L08-1121.5">speech recognition system</entity>. This <entity id="L08-1121.6">paper</entity> <entity id="L08-1121.7">reports</entity> on the <entity id="L08-1121.8">construction</entity> and <entity id="L08-1121.9">evaluation</entity> of the first Thai <entity id="L08-1121.10">broadcast news</entity> <entity id="L08-1121.11">speech</entity> and <entity id="L08-1121.12">text</entity> <entity id="L08-1121.13">corpora</entity>. <entity id="L08-1121.14">Specifications</entity> and <entity id="L08-1121.15">conventions</entity> used in the <entity id="L08-1121.16">transcription</entity> <entity id="L08-1121.17">process</entity> are described in the <entity id="L08-1121.18">paper</entity>. The <entity id="L08-1121.19">speech corpus</entity> contains about 17 hours of <entity id="L08-1121.20">speech</entity><entity id="L08-1121.21">data</entity> while the <entity id="L08-1121.22">text</entity> <entity id="L08-1121.23">corpus</entity> was transcribed from around 35 hours of television <entity id="L08-1121.24">broadcast news</entity>. The <entity id="L08-1121.25">characteristics</entity> of the <entity id="L08-1121.26">corpus</entity> were analyzed and shown in the <entity id="L08-1121.27">paper</entity>. The <entity id="L08-1121.28">speech corpus</entity> was split according to the <entity id="L08-1121.29">evaluation</entity> <entity id="L08-1121.30">focus</entity> condition used in the DARPA Hub-4 <entity id="L08-1121.31">evaluation</entity>. An 18<entity id="L08-1121.32">k-word</entity> Thai <entity id="L08-1121.33">speech recognition system</entity> was <entity id="L08-1121.34">setup</entity> to <entity id="L08-1121.35">test</entity> with this <entity id="L08-1121.36">speech corpus</entity> as a preliminary <entity id="L08-1121.37">experiment</entity>. <entity id="L08-1121.38">Acoustic model</entity> <entity id="L08-1121.39">adaptations</entity> were <entity id="L08-1121.40">performed</entity> to <entity id="L08-1121.41">improve</entity> the <entity id="L08-1121.42">system</entity> <entity id="L08-1121.43">performance</entity>. The best <entity id="L08-1121.44">system</entity> <entity id="L08-1121.45">yielded</entity> a <entity id="L08-1121.46">word error rate</entity> of about 20% for clean and planned <entity id="L08-1121.47">speech</entity>, and below 30% for the overall condition.
</abstract>

</text>

<text id="L08-1141">
<title>Question Answering on Speech Transcriptions: the QAST evaluation in CLEF</title>
<abstract>
This <entity id="L08-1141.1">paper</entity> <entity id="L08-1141.2">reports</entity> on the QAST track of CLEF aiming to <entity id="L08-1141.3">evaluate</entity> <entity id="L08-1141.4">Question Answering</entity> on <entity id="L08-1141.5">Speech</entity> Transcriptions. Accessing <entity id="L08-1141.6">information</entity> in spoken <entity id="L08-1141.7">documents</entity> <entity id="L08-1141.8">provides</entity> additional <entity id="L08-1141.9">challenges</entity> to those of <entity id="L08-1141.10">text-based</entity> QA, needing to address the <entity id="L08-1141.11">characteristics</entity> of spoken <entity id="L08-1141.12">language</entity>, as well as <entity id="L08-1141.13">errors</entity> in the <entity id="L08-1141.14">case</entity> of <entity id="L08-1141.15">automatic</entity> <entity id="L08-1141.16">transcriptions</entity> of <entity id="L08-1141.17">spontaneous speech</entity>. The <entity id="L08-1141.18">framework</entity> and <entity id="L08-1141.19">results</entity> of the <entity id="L08-1141.20">pilot</entity> QAst <entity id="L08-1141.21">evaluation</entity> held as <entity id="L08-1141.22">part</entity> of CLEF 2007 is described, illustrating some of the additional <entity id="L08-1141.23">challenges</entity> posed by QA in spoken <entity id="L08-1141.24">documents</entity> <entity id="L08-1141.25">relative</entity> to written ones. The <entity id="L08-1141.26">current</entity> plans for future <entity id="L08-1141.27">multiple-language</entity> and <entity id="L08-1141.28">multiple-task</entity> QAst <entity id="L08-1141.29">evaluations</entity> are described.
</abstract>

</text>

<text id="L08-1164">
<title>Ontology Learning and Semantic Annotation: a Necessary Symbiosis</title>
<abstract>
"<entity id="L08-1164.1">Semantic</entity> annotation of <entity id="L08-1164.2">text</entity> <entity id="L08-1164.3">requires</entity> the dynamic merging of linguistically <entity id="L08-1164.4">structured</entity> <entity id="L08-1164.5">information</entity> and a "world <entity id="L08-1164.6">model</entity>", usually represented as a <entity id="L08-1164.7">domain-specific</entity> <entity id="L08-1164.8">ontology</entity>. On the other <entity id="L08-1164.9">hand</entity>, the <entity id="L08-1164.10">process</entity> of engineering a <entity id="L08-1164.11">domain</entity> <entity id="L08-1164.12">ontology</entity> through <entity id="L08-1164.13">semi-automatic</entity> <entity id="L08-1164.14">ontology</entity> learning <entity id="L08-1164.15">system</entity> <entity id="L08-1164.16">requires</entity> the <entity id="L08-1164.17">availability</entity> of a considerable <entity id="L08-1164.18">amount</entity> of semantically annotated <entity id="L08-1164.19">documents</entity>. Facing this <entity id="L08-1164.20">bootstrapping</entity> paradox <entity id="L08-1164.21">requires</entity> an incremental <entity id="L08-1164.22">process</entity> of <entity id="L08-1164.23">annotation-acquisition-annotation</entity>, whereby <entity id="L08-1164.24">domain-specific</entity> <entity id="L08-1164.25">knowledge</entity> is acquired from linguistically-annotated <entity id="L08-1164.26">texts</entity> and then <entity id="L08-1164.27">projected</entity> back onto <entity id="L08-1164.28">texts</entity> for extra <entity id="L08-1164.29">linguistic information</entity> to be annotated and further <entity id="L08-1164.30">knowledge</entity> <entity id="L08-1164.31">layers</entity> to be <entity id="L08-1164.32">extracted</entity>. The presented <entity id="L08-1164.33">methodology</entity> is a <entity id="L08-1164.34">first step</entity> in the <entity id="L08-1164.35">direction</entity> of a full "virtuous" circle where the <entity id="L08-1164.36">semantic</entity> annotation <entity id="L08-1164.37">platform</entity> and the evolving <entity id="L08-1164.38">ontology</entity> interact in symbiosis. As a <entity id="L08-1164.39">case study</entity> we have chosen the <entity id="L08-1164.40">semantic</entity> annotation of <entity id="L08-1164.41">product</entity> catalogues. We <entity id="L08-1164.42">propose</entity> a hybrid <entity id="L08-1164.43">approach</entity>, combining <entity id="L08-1164.44">pattern</entity> <entity id="L08-1164.45">matching</entity> <entity id="L08-1164.46">techniques</entity> to exploit the regular <entity id="L08-1164.47">structure</entity> of <entity id="L08-1164.48">product</entity> <entity id="L08-1164.49">descriptions</entity> in catalogues, and <entity id="L08-1164.50">Natural Language Processing techniques</entity> which are resorted to analyze <entity id="L08-1164.51">natural language</entity> <entity id="L08-1164.52">descriptions</entity>. The <entity id="L08-1164.53">semantic</entity> annotation involves the <entity id="L08-1164.54">access</entity> to the <entity id="L08-1164.55">ontology</entity>, semi-automatically bootstrapped with an <entity id="L08-1164.56">ontology</entity> learning <entity id="L08-1164.57">tool</entity> from annotated <entity id="L08-1164.58">collections</entity> of catalogues. 
</abstract>

</text>

<text id="L08-1091">
<title>Anaphoric Annotation in the ARRAU Corpus</title>
<abstract> Arrau is a new <entity id="L08-1091.1">corpus</entity> annotated for anaphoric <entity id="L08-1091.2">relations</entity>, with <entity id="L08-1091.3">information</entity> about <entity id="L08-1091.4">agreement</entity> and explicit <entity id="L08-1091.5">representation</entity> of multiple antecedents for ambiguous anaphoric <entity id="L08-1091.6">expressions</entity> and <entity id="L08-1091.7">discourse</entity> antecedents for <entity id="L08-1091.8">expressions</entity> which refer to <entity id="L08-1091.9">abstract</entity> <entity id="L08-1091.10">entities</entity> such as <entity id="L08-1091.11">events</entity>, <entity id="L08-1091.12">actions</entity> and plans. The <entity id="L08-1091.13">corpus</entity> contains <entity id="L08-1091.14">texts</entity> from different <entity id="L08-1091.15">genres</entity>: <entity id="L08-1091.16">task-oriented</entity> <entity id="L08-1091.17">dialogues</entity> from the Trains-91 and Trains-93 <entity id="L08-1091.18">corpus</entity>, narratives from the <entity id="L08-1091.19">English</entity> <entity id="L08-1091.20">Pear</entity> Stories <entity id="L08-1091.21">corpus</entity>, <entity id="L08-1091.22">newspaper</entity> articles from the <entity id="L08-1091.23">Wall Street Journal</entity> <entity id="L08-1091.24">portion</entity> of the <entity id="L08-1091.25">Penn Treebank</entity>, and mixed <entity id="L08-1091.26">text</entity> from the Gnome <entity id="L08-1091.27">corpus</entity>.
</abstract>

</text>

<text id="E06-1014">
<title>Improving Probabilistic Latent Semantic Analysis With Principal Component Analysis</title>
<abstract><entity id="E06-1014.1">Probabilistic Latent Semantic Analysis</entity> (PLSA) <entity id="E06-1014.2">models</entity> have been shown to <entity id="E06-1014.3">provide</entity> a better <entity id="E06-1014.4">model</entity> for capturing polysemy and synonymy than <entity id="E06-1014.5">Latent Semantic Analysis</entity> (LSA). However, the <entity id="E06-1014.6">parameters</entity> of a PLSA <entity id="E06-1014.7">model</entity> are <entity id="E06-1014.8">trained</entity> using the <entity id="E06-1014.9">Expectation</entity> <entity id="E06-1014.10">Maximization</entity> (EM) <entity id="E06-1014.11">algorithm</entity>, and as a <entity id="E06-1014.12">result</entity>, the <entity id="E06-1014.13">trained</entity> <entity id="E06-1014.14">model</entity> is dependent on the initialization values so that <entity id="E06-1014.15">performance</entity> can be highly <entity id="E06-1014.16">variable</entity>. In this <entity id="E06-1014.17">paper</entity> we present a <entity id="E06-1014.18">method</entity> for using LSA <entity id="E06-1014.19">analysis</entity> to initialize a PLSA <entity id="E06-1014.20">model</entity>. We also investigated the <entity id="E06-1014.21">performance</entity> of our <entity id="E06-1014.22">method</entity> for the <entity id="E06-1014.23">tasks</entity> of <entity id="E06-1014.24">text</entity> segmentation and <entity id="E06-1014.25">retrieval</entity> on <entity id="E06-1014.26">personal-size</entity> <entity id="E06-1014.27">corpora</entity>, and present <entity id="E06-1014.28">results</entity> demonstrating the efficacy of our <entity id="E06-1014.29">proposed</entity> <entity id="E06-1014.30">approach</entity>.
</abstract>

</text>

<text id="E06-1020">
<title>Improved Lexical Alignment By Combining Multiple Reified Alignments</title>
<abstract>
We describe a <entity id="E06-1020.1">word alignment</entity> <entity id="E06-1020.2">platform</entity> which ensures <entity id="E06-1020.3">text</entity> <entity id="E06-1020.4">pre-processing</entity> (to-kenization, <entity id="E06-1020.5">POS-tagging</entity>, lemmatization, <entity id="E06-1020.6">chunking</entity>, <entity id="E06-1020.7">sentence</entity> <entity id="E06-1020.8">alignment</entity>) as <entity id="E06-1020.9">required</entity> by an accurate <entity id="E06-1020.10">word alignment</entity>. The <entity id="E06-1020.11">platform</entity> combines two different <entity id="E06-1020.12">methods</entity>, producing distinct <entity id="E06-1020.13">alignments</entity>. The <entity id="E06-1020.14">basic</entity> <entity id="E06-1020.15">word</entity> aligners are described in some <entity id="E06-1020.16">details</entity> and are individually <entity id="E06-1020.17">evaluated</entity>. The union of the <entity id="E06-1020.18">individual</entity> <entity id="E06-1020.19">alignments</entity> is subject to a filtering postprocessing <entity id="E06-1020.20">phase</entity>. Two different filtering <entity id="E06-1020.21">methods</entity> are also presented. The <entity id="E06-1020.22">evaluation</entity> shows that the combined <entity id="E06-1020.23">word alignment</entity> contains 10.75% less <entity id="E06-1020.24">errors</entity> than the best <entity id="E06-1020.25">individual</entity> aligner.
</abstract>

</text>

<text id="N03-1035">
<title>Toward A Task-Based Gold Standard For Evaluation Of NP Chunks And Technical Terms</title>
<abstract>
We <entity id="N03-1035.1">propose</entity> a <entity id="N03-1035.2">gold standard</entity> for <entity id="N03-1035.3">evaluating</entity> two <entity id="N03-1035.4">types</entity> of <entity id="N03-1035.5">information extraction</entity> <entity id="N03-1035.6">output</entity> -- <entity id="N03-1035.7">noun phrase</entity> (NP) <entity id="N03-1035.8">chunks</entity> ( Abney 1991 ; Ramshaw and Marcus 1995 ) and technical <entity id="N03-1035.9">terms</entity> ( Justeson and Katz 1995 ; Daille 2000 ; Jacquemin 2002 ). The <entity id="N03-1035.10">gold standard</entity> is built around the <entity id="N03-1035.11">notion</entity> that since different <entity id="N03-1035.12">semantic</entity> and <entity id="N03-1035.13">syntactic</entity> <entity id="N03-1035.14">variants</entity> of <entity id="N03-1035.15">terms</entity> are arguably correct, a fully satisfactory <entity id="N03-1035.16">assessment</entity> of the <entity id="N03-1035.17">quality</entity> of the <entity id="N03-1035.18">output</entity> must <entity id="N03-1035.19">include</entity> <entity id="N03-1035.20">task-based</entity> <entity id="N03-1035.21">evaluation</entity>. We conducted an <entity id="N03-1035.22">experiment</entity> that assessed subjects' <entity id="N03-1035.23">choice</entity> of <entity id="N03-1035.24">index</entity> <entity id="N03-1035.25">terms</entity> in an <entity id="N03-1035.26">information access</entity> <entity id="N03-1035.27">task</entity>. Subjects showed significant <entity id="N03-1035.28">preference</entity> for <entity id="N03-1035.29">index</entity> <entity id="N03-1035.30">terms</entity> that are longer, as measured by <entity id="N03-1035.31">number</entity> of <entity id="N03-1035.32">words</entity>, and more <entity id="N03-1035.33">complex</entity>, as measured by <entity id="N03-1035.34">number</entity> of <entity id="N03-1035.35">prepositions</entity>. These <entity id="N03-1035.36">terms</entity>, which were identified by a human indexer, serve as the <entity id="N03-1035.37">gold standard</entity>. The <entity id="N03-1035.38">experimental</entity> <entity id="N03-1035.39">protocol</entity> is a reliable and rigorous <entity id="N03-1035.40">method</entity> for <entity id="N03-1035.41">evaluating</entity> the <entity id="N03-1035.42">quality</entity> of a set of <entity id="N03-1035.43">terms</entity>. An important <entity id="N03-1035.44">advantage</entity> of this <entity id="N03-1035.45">task-based</entity> <entity id="N03-1035.46">evaluation</entity> is that a set of <entity id="N03-1035.47">index</entity> <entity id="N03-1035.48">terms</entity> which is different than the <entity id="N03-1035.49">gold standard</entity> can 'win' by <entity id="N03-1035.50">providing</entity> better <entity id="N03-1035.51">information access</entity> than the <entity id="N03-1035.52">gold standard</entity> itself does. And although the <entity id="N03-1035.53">individual</entity> human subject <entity id="N03-1035.54">experiments</entity> are <entity id="N03-1035.55">time</entity> consuming, the <entity id="N03-1035.56">experimental</entity> <entity id="N03-1035.57">interface</entity>, <entity id="N03-1035.58">test</entity> materials and<entity id="N03-1035.59">data</entity> <entity id="N03-1035.60">analysis</entity> <entity id="N03-1035.61">programs</entity> are completely re-usable.
</abstract>

</text>

<text id="N06-1008">
<title>Acquiring Inference Rules With Temporal Constraints By Using Japanese Coordinated Sentences And Noun-Verb Co-Occurrences</title>
<abstract>
"This <entity id="N06-1008.1">paper</entity> shows that <entity id="N06-1008.2">inference</entity> <entity id="N06-1008.3">rules</entity> with temporal <entity id="N06-1008.4">constraints</entity> can be acquired by using <entity id="N06-1008.5">verb-verb</entity> co-occurrences in <entity id="N06-1008.6">Japanese</entity> coordinated <entity id="N06-1008.7">sentences</entity> and <entity id="N06-1008.8">verb-noun</entity> cooccurrences. For <entity id="N06-1008.9">example</entity>, our unsuper-vised <entity id="N06-1008.10">acquisition</entity> <entity id="N06-1008.11">method</entity> could obtain the <entity id="N06-1008.12">inference</entity> <entity id="N06-1008.13">rule</entity> "If someone enforces a law, usually someone enacts the law at the same <entity id="N06-1008.14">time</entity> as or before the enforcing of the law" since the <entity id="N06-1008.15">verbs</entity> "enact" and "enforce" frequently co-occurred in coordinated <entity id="N06-1008.16">sentences</entity> and the <entity id="N06-1008.17">verbs</entity> also frequently co-occurred with the <entity id="N06-1008.18">noun</entity> "law". We also show that the <entity id="N06-1008.19">accuracy</entity> of the <entity id="N06-1008.20">acquisition</entity> is <entity id="N06-1008.21">improved</entity> by using the <entity id="N06-1008.22">occurrence</entity> <entity id="N06-1008.23">frequency</entity> of a single <entity id="N06-1008.24">verb</entity>, which we assume indicates how generic the <entity id="N06-1008.25">meaning</entity> of the <entity id="N06-1008.26">verb</entity> is.
</abstract>

</text>

<text id="N06-2046">
<title>Improved Affinity Graph Based Multi-Document Summarization</title>
<abstract>
This <entity id="N06-2046.1">paper</entity> describes an affinity graph <entity id="N06-2046.2">based</entity> <entity id="N06-2046.3">approach</entity> to <entity id="N06-2046.4">multi-document summarization</entity>. We incorporate a diffusion <entity id="N06-2046.5">process</entity> to acquire <entity id="N06-2046.6">semantic</entity> <entity id="N06-2046.7">relationships</entity> between <entity id="N06-2046.8">sentences</entity>, and then <entity id="N06-2046.9">compute</entity> <entity id="N06-2046.10">information</entity> <entity id="N06-2046.11">richness</entity> of <entity id="N06-2046.12">sentences</entity> by a graph <entity id="N06-2046.13">rank</entity> <entity id="N06-2046.14">algorithm</entity> on differentiated <entity id="N06-2046.15">in-tra-document</entity> <entity id="N06-2046.16">links</entity> and <entity id="N06-2046.17">inter-document</entity> <entity id="N06-2046.18">links</entity> between <entity id="N06-2046.19">sentences</entity>. A greedy <entity id="N06-2046.20">algorithm</entity> is employed to impose <entity id="N06-2046.21">diversity</entity> penalty on <entity id="N06-2046.22">sentences</entity> and the <entity id="N06-2046.23">sentences</entity> with both high <entity id="N06-2046.24">information</entity> <entity id="N06-2046.25">richness</entity> and high <entity id="N06-2046.26">information</entity> <entity id="N06-2046.27">novelty</entity> are chosen into the <entity id="N06-2046.28">summary</entity>. <entity id="N06-2046.29">Experimental</entity> <entity id="N06-2046.30">results</entity> on <entity id="N06-2046.31">task</entity> 2 of DUC 2002 and <entity id="N06-2046.32">task</entity> 2 of DUC 2004 demonstrate that the <entity id="N06-2046.33">proposed</entity> <entity id="N06-2046.34">approach</entity> outperforms existing state-of-the-art <entity id="N06-2046.35">systems</entity>.
</abstract>

</text>

<text id="N06-3009">
<title>A Hybrid Approach To Biomedical Named Entity Recognition And Semantic Role Labeling</title>
<abstract>
In this <entity id="N06-3009.1">paper</entity>, we describe our hybrid <entity id="N06-3009.2">approach</entity> to two key NLP <entity id="N06-3009.3">technologies</entity>: biomedical <entity id="N06-3009.4">named</entity> <entity id="N06-3009.5">entity</entity> <entity id="N06-3009.6">recognition</entity> (Bio-NER) and (Bio-SRL). In Bio-NER, our <entity id="N06-3009.7">system</entity> successfully integrates <entity id="N06-3009.8">linguistic features</entity> into the CRF <entity id="N06-3009.9">framework</entity>. In <entity id="N06-3009.10">addition</entity>, we employ web <entity id="N06-3009.11">lexicons</entity> and <entity id="N06-3009.12">template-based</entity> <entity id="N06-3009.13">post-processing</entity> to further boost its <entity id="N06-3009.14">performance</entity>. Through these broad <entity id="N06-3009.15">linguistic features</entity> and the <entity id="N06-3009.16">nature</entity> of CRF, our <entity id="N06-3009.17">system</entity> outperforms state-of-the-art <entity id="N06-3009.18">machine-learning-based systems</entity>, especially in the <entity id="N06-3009.19">recognition</entity> of <entity id="N06-3009.20">protein</entity> <entity id="N06-3009.21">names</entity> (F=78.5%). In Bio-SRL, first, we <entity id="N06-3009.22">construct</entity> a proposition <entity id="N06-3009.23">bank</entity> on top of the popular biomedical GENIA treebank following the PropBank annotation <entity id="N06-3009.24">scheme</entity>. We only annotate the <entity id="N06-3009.25">predicate-argument structures</entity> (PAS's) of thirty frequently used biomedical <entity id="N06-3009.26">verbs</entity> (predicates) and their corresponding <entity id="N06-3009.27">arguments</entity>. Second, we use our proposition <entity id="N06-3009.28">bank</entity> to <entity id="N06-3009.29">train</entity> a biomedical SRL <entity id="N06-3009.30">system</entity>, which uses a <entity id="N06-3009.31">maximum entropy</entity> (ME) <entity id="N06-3009.32">machine-learning</entity> <entity id="N06-3009.33">model</entity>. Thirdly, we automatically <entity id="N06-3009.34">generate</entity> <entity id="N06-3009.35">argument-type</entity> <entity id="N06-3009.36">templates</entity>, which can be used to <entity id="N06-3009.37">improve</entity> <entity id="N06-3009.38">classification</entity> of biomedical <entity id="N06-3009.39">argument</entity> <entity id="N06-3009.40">roles</entity>. Our <entity id="N06-3009.41">experimental</entity> <entity id="N06-3009.42">results</entity> show that a newswire <entity id="N06-3009.43">English</entity> SRL <entity id="N06-3009.44">system</entity> that achieves an F-score of 86.29% in the newswire <entity id="N06-3009.45">English</entity> <entity id="N06-3009.46">domain</entity> can maintain an F-score of 64.64% when ported to the biomedical <entity id="N06-3009.47">domain</entity>. By using our annotated biomedical <entity id="N06-3009.48">corpus</entity>, we can <entity id="N06-3009.49">increase</entity> that F-score by 22.9%. Adding automatically <entity id="N06-3009.50">generated</entity> <entity id="N06-3009.51">template</entity> <entity id="N06-3009.52">features</entity> further <entity id="N06-3009.53">increases</entity> overall F-score by 0.47% and <entity id="N06-3009.54">adjunct</entity> (AM) F-score by 1.57%, respectively.
</abstract>

</text>

<text id="A92-1024">
<title>Automatic Extraction Of Facts From Press Releases To Generate News Stories</title>
<abstract>
While complete <entity id="A92-1024.1">understanding</entity> of arbitrary <entity id="A92-1024.2">input</entity> <entity id="A92-1024.3">text</entity> remains in the future, it is currently possible to <entity id="A92-1024.4">construct</entity> <entity id="A92-1024.5">natural language processing systems</entity> that <entity id="A92-1024.6">provide</entity> a <entity id="A92-1024.7">partial</entity> <entity id="A92-1024.8">understanding</entity> of <entity id="A92-1024.9">text</entity> with limited <entity id="A92-1024.10">accuracy</entity>. Moreover, such <entity id="A92-1024.11">systems</entity> can <entity id="A92-1024.12">provide</entity> <entity id="A92-1024.13">cost-effective</entity> <entity id="A92-1024.14">solutions</entity> to commercially-significant business <entity id="A92-1024.15">problems</entity>. This <entity id="A92-1024.16">paper</entity> describes one such <entity id="A92-1024.17">system</entity>: JASPER. JASPER is a fact <entity id="A92-1024.18">extraction system</entity> recently <entity id="A92-1024.19">developed</entity> and deployed by Carnegie Group for Reuters Ltd. JASPER uses a <entity id="A92-1024.20">template-driven</entity> <entity id="A92-1024.21">approach</entity>, <entity id="A92-1024.22">partial</entity> <entity id="A92-1024.23">understanding</entity> <entity id="A92-1024.24">techniques</entity>, and heuristic <entity id="A92-1024.25">procedures</entity> to <entity id="A92-1024.26">extract</entity> certain key pieces of <entity id="A92-1024.27">information</entity> from a limited range of <entity id="A92-1024.28">text</entity>.. We believe that many significant business <entity id="A92-1024.29">problems</entity> can be <entity id="A92-1024.30">solved</entity> by fact <entity id="A92-1024.31">extraction</entity> <entity id="A92-1024.32">applications</entity> which involve locating and <entity id="A92-1024.33">extracting</entity> specific, predefined <entity id="A92-1024.34">types</entity> of <entity id="A92-1024.35">information</entity> from a limited range of <entity id="A92-1024.36">text</entity> The <entity id="A92-1024.37">information</entity> <entity id="A92-1024.38">extracted</entity> by such <entity id="A92-1024.39">systems</entity> can be used in a <entity id="A92-1024.40">variety</entity> of ways, such as filling in values in a <entity id="A92-1024.41">database</entity>, <entity id="A92-1024.42">generating</entity> <entity id="A92-1024.43">summaries</entity> of the <entity id="A92-1024.44">input</entity> <entity id="A92-1024.45">text</entity>, serving as a <entity id="A92-1024.46">part</entity> of the <entity id="A92-1024.47">knowledge</entity> in an <entity id="A92-1024.48">expert system</entity>, or feeding into another <entity id="A92-1024.49">program</entity> which <entity id="A92-1024.50">bases</entity> <entity id="A92-1024.51">decisions</entity> on it. We expect to <entity id="A92-1024.52">develop</entity> many such <entity id="A92-1024.53">applications</entity> in the future using similar <entity id="A92-1024.54">techniques</entity>.
</abstract>

</text>

<text id="A94-1011">
<title>Exploiting Sophisticated Representations For Document Retrieval</title>
<abstract>
"The use of NLP <entity id="A94-1011.1">techniques</entity> for <entity id="A94-1011.2">document classification</entity> has not produced significant <entity id="A94-1011.3">improvements</entity> in <entity id="A94-1011.4">performance</entity> within the <entity id="A94-1011.5">standard</entity> <entity id="A94-1011.6">term</entity> <entity id="A94-1011.7">weighting</entity> <entity id="A94-1011.8">statistical</entity> <entity id="A94-1011.9">assignment</entity> <entity id="A94-1011.10">paradigm</entity> ( Fagan 1987 ; <entity id="A94-1011.11">Lewis</entity>, 1992b c; Buckley, 1993 ). This perplexing fact needs both an <entity id="A94-1011.12">explanation</entity> and a <entity id="A94-1011.13">solution</entity> if the power of recently <entity id="A94-1011.14">developed</entity> NLP <entity id="A94-1011.15">techniques</entity> are to be successfully <entity id="A94-1011.16">applied</entity> in IR. A novel <entity id="A94-1011.17">method</entity> for adding linguistic annotation to <entity id="A94-1011.18">corpora</entity> is presented which involves using a <entity id="A94-1011.19">statistical</entity> POS tagger in <entity id="A94-1011.20">conjunction</entity> with unsupervised <entity id="A94-1011.21">structure</entity> finding <entity id="A94-1011.22">methods</entity> to derive <entity id="A94-1011.23">notions</entity> of "<entity id="A94-1011.24">noun</entity> group", "<entity id="A94-1011.25">verb</entity> group", and so on which is inherently extensible to more sophisticated annotation, and does not <entity id="A94-1011.26">require</entity> a pre-tagged <entity id="A94-1011.27">corpus</entity> to fit. One of the distinguishing <entity id="A94-1011.28">features</entity> of a more linguistically sophisticated <entity id="A94-1011.29">representation</entity> of <entity id="A94-1011.30">documents</entity> over a <entity id="A94-1011.31">word</entity> set <entity id="A94-1011.32">based</entity> <entity id="A94-1011.33">representation</entity> of them is that linguistically sophisticated <entity id="A94-1011.34">units</entity> are more frequently individually good <entity id="A94-1011.35">predictors</entity> of <entity id="A94-1011.36">document</entity> descriptors (<entity id="A94-1011.37">keywords</entity>) than single <entity id="A94-1011.38">words</entity> are. This leads us to consider the <entity id="A94-1011.39">assignment</entity> of descriptors from <entity id="A94-1011.40">individual</entity> <entity id="A94-1011.41">phrases</entity> rather than from the weighted <entity id="A94-1011.42">sum</entity> of a <entity id="A94-1011.43">word</entity> set <entity id="A94-1011.44">representation</entity>. We investigate how sets of individually <entity id="A94-1011.45">high-precision</entity> <entity id="A94-1011.46">rules</entity> can <entity id="A94-1011.47">result</entity> in a low <entity id="A94-1011.48">precision</entity> when used together, and <entity id="A94-1011.49">develop</entity> some <entity id="A94-1011.50">theory</entity> about these probably-correct <entity id="A94-1011.51">rules</entity>. We then proceed to repeat <entity id="A94-1011.52">results</entity> which show that <entity id="A94-1011.53">standard</entity> <entity id="A94-1011.54">statistical models</entity> are not particularly suitable for exploiting linguistically sophisticated <entity id="A94-1011.55">representations</entity>, and show that a statistically fitted <entity id="A94-1011.56">rule-based model</entity> <entity id="A94-1011.57">provides</entity> significantly <entity id="A94-1011.58">improved</entity> <entity id="A94-1011.59">performance</entity> for sophisticated <entity id="A94-1011.60">representations</entity>. It therefore shows that <entity id="A94-1011.61">statistical</entity> <entity id="A94-1011.62">systems</entity> can exploit sophisticated <entity id="A94-1011.63">representations</entity> of <entity id="A94-1011.64">documents</entity>, and lends some <entity id="A94-1011.65">support</entity> to the use of more linguistically sophisticated <entity id="A94-1011.66">representations</entity> for <entity id="A94-1011.67">document classification</entity>. This <entity id="A94-1011.68">paper</entity> <entity id="A94-1011.69">reports</entity> on work done for the LRE <entity id="A94-1011.70">project</entity> SlSTA, "
</abstract>

</text>

<text id="H89-2053">
<title>An Evaluation Of Lexicalization In Parsing</title>
<abstract>
In this <entity id="H89-2053.1">paper</entity>, we <entity id="H89-2053.2">evaluate</entity> a two-pass <entity id="H89-2053.3">parsing</entity> <entity id="H89-2053.4">strategy</entity> <entity id="H89-2053.5">proposed</entity> for the so-called 'lexicalized' grammar. In 'lexicalized' grammars ( Schabes, Abeille and Joshi, 1988 ), each elementary <entity id="H89-2053.6">structure</entity> is systematically associated with a <entity id="H89-2053.7">lexical item</entity> <entity id="H89-2053.8">called</entity> A general two-pass <entity id="H89-2053.9">parsing</entity> <entity id="H89-2053.10">strategy</entity> for 'lexicalized' grammars follows naturally. In the first stage, the <entity id="H89-2053.11">parser</entity> selects a set of elementary <entity id="H89-2053.12">structures</entity> associated with the <entity id="H89-2053.13">lexical items</entity> in the <entity id="H89-2053.14">input</entity> <entity id="H89-2053.15">sentence</entity>, and in the second stage the <entity id="H89-2053.16">sentence</entity> is parsed with <entity id="H89-2053.17">respect</entity> to this set. We <entity id="H89-2053.18">evaluate</entity> this <entity id="H89-2053.19">strategy</entity> with <entity id="H89-2053.20">respect</entity> to two <entity id="H89-2053.21">characteristics</entity>. First, the <entity id="H89-2053.22">amount</entity> of filtering on the entire grammar is <entity id="H89-2053.23">evaluated</entity>: once the first pass is <entity id="H89-2053.24">performed</entity>, the <entity id="H89-2053.25">parser</entity> uses only a subset of the grammar. Second, we <entity id="H89-2053.26">evaluate</entity> the use of non-local <entity id="H89-2053.27">information</entity>: the <entity id="H89-2053.28">structures</entity> selected during the first pass encode the morphological value (and therefore the position in the <entity id="H89-2053.29">string</entity>) of their anchor; this enables the <entity id="H89-2053.30">parser</entity> to use non-local <entity id="H89-2053.31">information</entity> to guide its <entity id="H89-2053.32">search</entity>.We take Lexicalized <entity id="H89-2053.33">Tree</entity> Adjoining Grammars as an <entity id="H89-2053.34">instance</entity> of lexicalized grammar. We illustrate the <entity id="H89-2053.35">organization</entity> of the grammar. Then we show how a general <entity id="H89-2053.36">Earley-type</entity> <entity id="H89-2053.37">TAG</entity> <entity id="H89-2053.38">parser</entity> ( Schabes and Joshi, 1988 ) can take <entity id="H89-2053.39">advantage</entity> of lexicalization. Empirical<entity id="H89-2053.40">data</entity> show that the filtering of the grammar and the non-local <entity id="H89-2053.41">information</entity> <entity id="H89-2053.42">provided</entity> by the two-pass <entity id="H89-2053.43">strategy</entity> <entity id="H89-2053.44">improve</entity> the <entity id="H89-2053.45">performance</entity> of the <entity id="H89-2053.46">parser</entity>.
</abstract>

</text>

<text id="H91-1051">
<title>Context Dependent Modeling Of Phones In Continuous Speech Using Decision Trees</title>
<abstract>
In a <entity id="H91-1051.1">continuous speech recognition system</entity> it is important to <entity id="H91-1051.2">model</entity> the <entity id="H91-1051.3">context</entity> dependent <entity id="H91-1051.4">variations</entity> in the <entity id="H91-1051.5">pronunciations</entity> of <entity id="H91-1051.6">words</entity>. In this <entity id="H91-1051.7">paper</entity> we present an <entity id="H91-1051.8">automatic</entity> <entity id="H91-1051.9">method</entity> for <entity id="H91-1051.10">modeling</entity> phonological <entity id="H91-1051.11">variation</entity> using <entity id="H91-1051.12">decision trees</entity>. For each <entity id="H91-1051.13">phone</entity> we <entity id="H91-1051.14">construct</entity> a <entity id="H91-1051.15">decision tree</entity> that specifies the acoustic <entity id="H91-1051.16">realization</entity> of the <entity id="H91-1051.17">phone</entity> as a <entity id="H91-1051.18">function</entity> of the <entity id="H91-1051.19">context</entity> in which it appears. Several thousand <entity id="H91-1051.20">sentences</entity> from a <entity id="H91-1051.21">natural language</entity> <entity id="H91-1051.22">corpus</entity> spoken by several talkers are used to <entity id="H91-1051.23">construct</entity> these <entity id="H91-1051.24">decision trees</entity>. <entity id="H91-1051.25">Experimental</entity> <entity id="H91-1051.26">results</entity> on a 5000-<entity id="H91-1051.27">word</entity> <entity id="H91-1051.28">vocabulary</entity> <entity id="H91-1051.29">natural language</entity> <entity id="H91-1051.30">speech recognition</entity> <entity id="H91-1051.31">task</entity> are presented.
</abstract>

</text>

<text id="H91-1055">
<title>Experience With A Stack Decoder-Based HMM CSR And Back-Off N-Gram Language Models</title>
<abstract>
Stochastic <entity id="H91-1055.1">language models</entity> are more useful than non-stochastic <entity id="H91-1055.2">models</entity> because they contribute more <entity id="H91-1055.3">information</entity> than a <entity id="H91-1055.4">simple</entity> acceptance or rejection of a <entity id="H91-1055.5">word</entity> <entity id="H91-1055.6">sequence</entity>. Back-ofF N-<entity id="H91-1055.7">gram language models</entity>[ll] are an effective <entity id="H91-1055.8">class</entity> of <entity id="H91-1055.9">word</entity> <entity id="H91-1055.10">based</entity> stochastic <entity id="H91-1055.11">language model</entity>. The first <entity id="H91-1055.12">part</entity> of this <entity id="H91-1055.13">paper</entity> describes our <entity id="H91-1055.14">experiences</entity> using the back-off <entity id="H91-1055.15">language models</entity> in our <entity id="H91-1055.16">time-synchronous</entity> <entity id="H91-1055.17">decoder</entity> CSR. A bigram back-off <entity id="H91-1055.18">language model</entity> was chosen for the <entity id="H91-1055.19">language model</entity> to be used in the informal ATIS CSR baseline <entity id="H91-1055.20">evaluation</entity> <entity id="H91-1055.21">test</entity>[13, 21].The stack <entity id="H91-1055.22">decoder</entity>[2, 8, 24] is a promising <entity id="H91-1055.23">control</entity> <entity id="H91-1055.24">structure</entity> for a <entity id="H91-1055.25">speech understanding system</entity> because it can combine <entity id="H91-1055.26">constraints</entity> from both the <entity id="H91-1055.27">acoustic model</entity> and a long <entity id="H91-1055.28">span</entity> <entity id="H91-1055.29">language model</entity> (such as a <entity id="H91-1055.30">natural language processor</entity> (NLP)) into a single integrated <entity id="H91-1055.31">search</entity>[17]. A copy of the Lincoln <entity id="H91-1055.32">time-synchronous</entity> HMM CSR has been converted to a stack <entity id="H91-1055.33">decoder</entity> controlled <entity id="H91-1055.34">search</entity> with stochastic <entity id="H91-1055.35">language models</entity>. The second <entity id="H91-1055.36">part</entity> of this <entity id="H91-1055.37">paper</entity> describes our <entity id="H91-1055.38">experiences</entity> with our <entity id="H91-1055.39">prototype</entity> stack <entity id="H91-1055.40">decoder</entity> CSR using no grammar, the <entity id="H91-1055.41">word-pair</entity> grammar, and N-<entity id="H91-1055.42">gram</entity> back-off <entity id="H91-1055.43">language models</entity>.
</abstract>

</text>

<text id="H93-1049">
<title>Hypothesizing Word Association From Untagged Text</title>
<abstract>
This <entity id="H93-1049.1">paper</entity> <entity id="H93-1049.2">reports</entity> a new <entity id="H93-1049.3">method</entity> for suggesting <entity id="H93-1049.4">word</entity> <entity id="H93-1049.5">associations</entity>, <entity id="H93-1049.6">based</entity> on a greedy <entity id="H93-1049.7">algorithm</entity> that employs Chi-square <entity id="H93-1049.8">statistics</entity> on joint <entity id="H93-1049.9">frequencies</entity> of <entity id="H93-1049.10">pairs</entity> of <entity id="H93-1049.11">word</entity> groups compared against chance <entity id="H93-1049.12">co-occurrence</entity>. The <entity id="H93-1049.13">benefits</entity> of this new <entity id="H93-1049.14">approach</entity> are: 1) we can consider even low <entity id="H93-1049.15">frequency</entity> <entity id="H93-1049.16">words</entity> and <entity id="H93-1049.17">word pairs</entity>, and 2) <entity id="H93-1049.18">word</entity> groups and <entity id="H93-1049.19">word</entity> <entity id="H93-1049.20">associations</entity> can be automatically <entity id="H93-1049.21">generated</entity>. The <entity id="H93-1049.22">method</entity> <entity id="H93-1049.23">provided</entity> 87% <entity id="H93-1049.24">accuracy</entity> in hypothesizing <entity id="H93-1049.25">word</entity> <entity id="H93-1049.26">associations</entity> for unobserved <entity id="H93-1049.27">combinations</entity> of <entity id="H93-1049.28">words</entity> in <entity id="H93-1049.29">Japanese</entity> <entity id="H93-1049.30">text</entity>.
</abstract>

</text>

<text id="H93-1068">
<title>Perceived Prosodic Boundaries And Their Phonetic Correlates</title>
<abstract>
This <entity id="H93-1068.1">paper</entity> addresses two <entity id="H93-1068.2">main</entity> <entity id="H93-1068.3">questions</entity>: (a) Can <entity id="H93-1068.4">listeners</entity> assign values of perceived <entity id="H93-1068.5">boundary</entity> <entity id="H93-1068.6">strength</entity> to the juncture between any two <entity id="H93-1068.7">words</entity>? (b) If so, what is the <entity id="H93-1068.8">relationship</entity> between these values and various (<entity id="H93-1068.9">combinations</entity> of) suprasegmental <entity id="H93-1068.10">features</entity>. Three speakers read a set of twenty <entity id="H93-1068.11">utterances</entity> of varying <entity id="H93-1068.12">length</entity> and <entity id="H93-1068.13">complexity</entity>. A panel of nineteen <entity id="H93-1068.14">listeners</entity> assigned <entity id="H93-1068.15">boundary</entity> <entity id="H93-1068.16">strength</entity> values to each of the 175 <entity id="H93-1068.17">word</entity> <entity id="H93-1068.18">boundaries</entity> in the material. Then the <entity id="H93-1068.19">correlation</entity> was established between the <entity id="H93-1068.20">variable</entity> <entity id="H93-1068.21">strength</entity> of the perceived <entity id="H93-1068.22">boundaries</entity> and three prosodie <entity id="H93-1068.23">variables</entity>: melodic discontinuity, declination reset and pause. The <entity id="H93-1068.24">results</entity> show that speakers may differ in their <entity id="H93-1068.25">strategies</entity> of prosodie <entity id="H93-1068.26">boundary</entity> marking and <entity id="H93-1068.27">listeners</entity> agree in the perceptual <entity id="H93-1068.28">weight</entity> they attribute to the prosodie <entity id="H93-1068.29">cues</entity>.
</abstract>

</text>

<text id="H93-1103">
<title>WORDNET: A Lexical Database For English</title>
<abstract>
Work under this <entity id="H93-1103.1">grant</entity> is intended to <entity id="H93-1103.2">provide</entity> <entity id="H93-1103.3">lexical resources</entity> for <entity id="H93-1103.4">research</entity> on <entity id="H93-1103.5">natural languages</entity>. The principal <entity id="H93-1103.6">product</entity> is WordNet, a <entity id="H93-1103.7">lexical database</entity> for <entity id="H93-1103.8">English</entity> whose <entity id="H93-1103.9">organization</entity> is inspired by <entity id="H93-1103.10">current</entity> psycholinguistic <entity id="H93-1103.11">theories</entity> of human <entity id="H93-1103.12">lexical knowledge</entity>. Lexicalized <entity id="H93-1103.13">concepts</entity> are organized by <entity id="H93-1103.14">semantic relations</entity> for <entity id="H93-1103.15">nouns</entity>, <entity id="H93-1103.16">verbs</entity>, <entity id="H93-1103.17">adjectives</entity>, and <entity id="H93-1103.18">adverbs</entity>. The principal <entity id="H93-1103.19">goal</entity> of the <entity id="H93-1103.20">project</entity> is to upgrade WordNet and make it available to interested <entity id="H93-1103.21">users</entity>. A secondary <entity id="H93-1103.22">goal</entity> is to explore <entity id="H93-1103.23">practical applications</entity> of WordNet; its possible use in the <entity id="H93-1103.24">resolution</entity> of <entity id="H93-1103.25">word</entity> senses in <entity id="H93-1103.26">context</entity> (<entity id="H93-1103.27">semantic</entity> <entity id="H93-1103.28">disambiguation</entity>) is viewed as a necessary precursor for many other <entity id="H93-1103.29">applications</entity>.
</abstract>

</text>

<text id="H94-1031">
<title>Issues And Methodology For Template Design For Information Extraction</title>
<abstract>
The <entity id="H94-1031.1">goal</entity> of <entity id="H94-1031.2">Information Extraction</entity> <entity id="H94-1031.3">tasks</entity> is to identify, categorize, classify, relate, and normalize specific <entity id="H94-1031.4">information</entity> of interest found in free <entity id="H94-1031.5">text</entity>, and to make that <entity id="H94-1031.6">information</entity> available to a back-end<entity id="H94-1031.7">data</entity> <entity id="H94-1031.8">base</entity>,<entity id="H94-1031.9">data</entity> <entity id="H94-1031.10">fusion</entity>, or other <entity id="H94-1031.11">application</entity>. A<entity id="H94-1031.12">data</entity> <entity id="H94-1031.13">structure</entity> referred to as a <entity id="H94-1031.14">template</entity> (desiderata)
</abstract>

</text>

<text id="A00-1012">
<title>Experiments On Sentence Boundary Detection</title>
<abstract>
This <entity id="A00-1012.1">paper</entity> explores the <entity id="A00-1012.2">problem</entity> of identifying <entity id="A00-1012.3">sentence</entity> <entity id="A00-1012.4">boundaries</entity> in the <entity id="A00-1012.5">transcriptions</entity> produced by automatic <entity id="A00-1012.6">speech recognition systems</entity>. An <entity id="A00-1012.7">experiment</entity> which determines the <entity id="A00-1012.8">level</entity> of human <entity id="A00-1012.9">performance</entity> for this <entity id="A00-1012.10">task</entity> is described as well as a <entity id="A00-1012.11">memory-based</entity> <entity id="A00-1012.12">computational</entity> <entity id="A00-1012.13">approach</entity> to the <entity id="A00-1012.14">problem</entity>.
</abstract>

</text>

<text id="H05-1116">
<title>Multi-Perspective Question Answering Using The OpQA Corpus</title>
<abstract>
We investigate <entity id="H05-1116.1">techniques</entity> to <entity id="H05-1116.2">support</entity> the answering of <entity id="H05-1116.3">opinion-based</entity> <entity id="H05-1116.4">questions</entity>. We first present the OpQA <entity id="H05-1116.5">corpus</entity> of <entity id="H05-1116.6">opinion</entity> <entity id="H05-1116.7">questions</entity> and answers. Using the <entity id="H05-1116.8">corpus</entity>, we compare and <entity id="H05-1116.9">contrast</entity> the <entity id="H05-1116.10">properties</entity> of fact and <entity id="H05-1116.11">opinion</entity> <entity id="H05-1116.12">questions</entity> and answers. <entity id="H05-1116.13">Based</entity> on the disparate <entity id="H05-1116.14">characteristics</entity> of <entity id="H05-1116.15">opinion</entity> vs. fact answers, we argue that traditional fact-based QA <entity id="H05-1116.16">approaches</entity> may have <entity id="H05-1116.17">difficulty</entity> in an MPQA setting without <entity id="H05-1116.18">modification</entity>. As an initial <entity id="H05-1116.19">step</entity> towards the <entity id="H05-1116.20">development</entity> of MPQA <entity id="H05-1116.21">systems</entity>, we investigate the use of <entity id="H05-1116.22">machine learning</entity> and <entity id="H05-1116.23">rule-based</entity> subjectivity and <entity id="H05-1116.24">opinion</entity> <entity id="H05-1116.25">source</entity> filters and show that they can be used to guide MPQA <entity id="H05-1116.26">systems</entity>.
</abstract>

</text>

<text id="X96-1021">
<title>Oleada: User-Centered TIPSTER Technology For Language Instruction</title>
<abstract>
TIPSTER is an <entity id="X96-1021.1">AREA</entity> sponsored <entity id="X96-1021.2">program</entity> that seeks to <entity id="X96-1021.3">develop</entity> <entity id="X96-1021.4">methods</entity> and <entity id="X96-1021.5">tools</entity> that <entity id="X96-1021.6">support</entity> <entity id="X96-1021.7">analysts</entity> in their <entity id="X96-1021.8">efforts</entity> to filter, <entity id="X96-1021.9">process</entity>, and analyze ever <entity id="X96-1021.10">increasing</entity> <entity id="X96-1021.11">quantities</entity> of <entity id="X96-1021.12">text-based</entity> <entity id="X96-1021.13">information</entity>. To this end, government sponsors, contractors, and <entity id="X96-1021.14">developers</entity> are working to <entity id="X96-1021.15">design</entity> an <entity id="X96-1021.16">architecture</entity> <entity id="X96-1021.17">specification</entity> that makes it possible for <entity id="X96-1021.18">natural language processing techniques</entity> and <entity id="X96-1021.19">tools</entity>, from a <entity id="X96-1021.20">variety</entity> <entity id="X96-1021.21">sources</entity>, to be integrated, shared, and configured by end-users. The <entity id="X96-1021.22">Computing</entity> <entity id="X96-1021.23">Research</entity> <entity id="X96-1021.24">Laboratory</entity> (CRL) is a longtime contributor to TIPSTER. A significant <entity id="X96-1021.25">portion</entity> of CRL's <entity id="X96-1021.26">research</entity> involves work on a <entity id="X96-1021.27">variety</entity> of <entity id="X96-1021.28">natural language processing</entity> <entity id="X96-1021.29">problems</entity>, <entity id="X96-1021.30">human-computer</entity> <entity id="X96-1021.31">interaction</entity>, and <entity id="X96-1021.32">problems</entity> associated with getting <entity id="X96-1021.33">technology</entity> into the <entity id="X96-1021.34">hands</entity> of end-users. CRL is using TIPSTER <entity id="X96-1021.35">technology</entity> to <entity id="X96-1021.36">develop</entity> OLEADA, which is an integrated set of <entity id="X96-1021.37">computer</entity> <entity id="X96-1021.38">tools</entity> <entity id="X96-1021.39">designed</entity> to <entity id="X96-1021.40">support</entity> <entity id="X96-1021.41">language</entity> learners, and instructors. Further, OLEADA has been <entity id="X96-1021.42">developed</entity> using a <entity id="X96-1021.43">task-oriented</entity> <entity id="X96-1021.44">user-centered</entity> <entity id="X96-1021.45">design</entity> <entity id="X96-1021.46">methodology</entity>. This <entity id="X96-1021.47">paper</entity> describes the <entity id="X96-1021.48">methodology</entity> used to <entity id="X96-1021.49">develop</entity> OLEADA and the <entity id="X96-1021.50">current</entity> <entity id="X96-1021.51">system</entity>'s <entity id="X96-1021.52">capabilities</entity>.
</abstract>

</text>

<text id="X98-1007">
<title>The Cornell TIPSTER Phase III Project</title>
<abstract>
The overall <entity id="X98-1007.1">objective</entity> of the Cornell <entity id="X98-1007.2">University</entity> TIPSTER <entity id="X98-1007.3">Project</entity> was to <entity id="X98-1007.4">improve</entity> <entity id="X98-1007.5">end-user</entity> <entity id="X98-1007.6">efficiency</entity> in <entity id="X98-1007.7">information retrieval systems</entity> by reducing the <entity id="X98-1007.8">amount</entity> of <entity id="X98-1007.9">text</entity> that the <entity id="X98-1007.10">user</entity> must <entity id="X98-1007.11">process</entity> [1]. The <entity id="X98-1007.12">project</entity> <entity id="X98-1007.13">focuses</entity> on high <entity id="X98-1007.14">precision</entity> IR, near-duplicate <entity id="X98-1007.15">detection</entity> and <entity id="X98-1007.16">context-dependent</entity> <entity id="X98-1007.17">summarization</entity>. The two <entity id="X98-1007.18">main</entity> foundations of the <entity id="X98-1007.19">research</entity> are the latest <entity id="X98-1007.20">version</entity> of the Smart <entity id="X98-1007.21">system</entity> for <entity id="X98-1007.22">information Retrieval</entity> and the Empire <entity id="X98-1007.23">system</entity> for <entity id="X98-1007.24">natural language processing</entity>. Smart is an <entity id="X98-1007.25">implementation</entity> of the <entity id="X98-1007.26">vector-space</entity> <entity id="X98-1007.27">model</entity> of <entity id="X98-1007.28">information retrieval</entity> (IR). Its earlier <entity id="X98-1007.29">purpose</entity> was to <entity id="X98-1007.30">provide</entity> a <entity id="X98-1007.31">framework</entity> to conduct IR <entity id="X98-1007.32">research</entity> but <entity id="X98-1007.33">current</entity> <entity id="X98-1007.34">developments</entity> will make the <entity id="X98-1007.35">system</entity> easier to use by <entity id="X98-1007.36">non-researcher</entity>. Empire is a <entity id="X98-1007.37">research-oriented</entity> <entity id="X98-1007.38">system</entity> that uses <entity id="X98-1007.39">machine</entity> learning <entity id="X98-1007.40">methods</entity> to quickly <entity id="X98-1007.41">perform</entity> <entity id="X98-1007.42">partial</entity> <entity id="X98-1007.43">parsing</entity> of <entity id="X98-1007.44">sentences</entity>. Cornell's integrated <entity id="X98-1007.45">approach</entity> uses both <entity id="X98-1007.46">statistical</entity> and linguistic <entity id="X98-1007.47">sources</entity> to first identify <entity id="X98-1007.48">relationships</entity> among important <entity id="X98-1007.49">terms</entity> in the <entity id="X98-1007.50">query</entity> or in the <entity id="X98-1007.51">text</entity>. The integrated <entity id="X98-1007.52">system</entity> then uses the <entity id="X98-1007.53">extracted</entity> <entity id="X98-1007.54">relationships</entity> to (1) discard or reorder retrieved <entity id="X98-1007.55">texts</entity> (for <entity id="X98-1007.56">high-precision</entity> IR); (2) locate redundant <entity id="X98-1007.57">information</entity> (for near-duplicate <entity id="X98-1007.58">document</entity> <entity id="X98-1007.59">detection</entity>); or (3) <entity id="X98-1007.60">generate</entity> <entity id="X98-1007.61">summaries</entity>. A more detailed technical <entity id="X98-1007.62">description</entity> about the <entity id="X98-1007.63">research</entity> can be found in the Cornell <entity id="X98-1007.64">University</entity> technical <entity id="X98-1007.65">paper</entity> [2].
</abstract>

</text>

<text id="W94-0108">
<title>The Automatic Construction Of A Symbolic Parser Via Statistical Techniques</title>
<abstract>
We <entity id="W94-0108.1">report</entity> on the <entity id="W94-0108.2">development</entity> of a <entity id="W94-0108.3">robust</entity> <entity id="W94-0108.4">parsing</entity> <entity id="W94-0108.5">device</entity> which aims to <entity id="W94-0108.6">provide</entity> a <entity id="W94-0108.7">partial</entity> <entity id="W94-0108.8">explanation</entity> for child <entity id="W94-0108.9">language</entity> <entity id="W94-0108.10">acquisition</entity> and <entity id="W94-0108.11">help</entity> in the <entity id="W94-0108.12">construction</entity> of better <entity id="W94-0108.13">natural language processing systems</entity>. The <entity id="W94-0108.14">backbone</entity> of the new <entity id="W94-0108.15">approach</entity> is the <entity id="W94-0108.16">synthesis</entity> of <entity id="W94-0108.17">statistical</entity> and symbolic <entity id="W94-0108.18">approaches</entity> to <entity id="W94-0108.19">natural language</entity>.
</abstract>

</text>

<text id="W96-0302">
<title>How Language Structures Concepts - An Outline</title>
<abstract>
For the past three decades, the <entity id="W96-0302.1">mainstream</entity> of <entity id="W96-0302.2">linguistics</entity> has <entity id="W96-0302.3">focused</entity> its <entity id="W96-0302.4">research</entity> <entity id="W96-0302.5">agenda</entity> on the formal <entity id="W96-0302.6">aspects</entity> of <entity id="W96-0302.7">language</entity>, primarily <entity id="W96-0302.8">syntax</entity>. By <entity id="W96-0302.9">contrast</entity>, the more recent tradition of cognitive <entity id="W96-0302.10">linguistics</entity> <entity id="W96-0302.11">centers</entity> its <entity id="W96-0302.12">research</entity> directly within the <entity id="W96-0302.13">semantic</entity> stratum of <entity id="W96-0302.14">language</entity> in <entity id="W96-0302.15">order</entity> to observe how <entity id="W96-0302.16">languages</entity> organize meaning and <entity id="W96-0302.17">structure</entity> conception, and it examines the more formal stratum of <entity id="W96-0302.18">language</entity> for its <entity id="W96-0302.19">role</entity> in <entity id="W96-0302.20">supporting</entity> these <entity id="W96-0302.21">semantic</entity> <entity id="W96-0302.22">functions</entity>.
</abstract>

</text>

<text id="W97-0110">
<title>Corpus Based Statistical Generalization Tree In Rule Optimization</title>
<abstract>
A <entity id="W97-0110.1">corpus-based</entity> <entity id="W97-0110.2">statistical</entity> <entity id="W97-0110.3">Generalization</entity> <entity id="W97-0110.4">Tree</entity> <entity id="W97-0110.5">model</entity> is described to achieve <entity id="W97-0110.6">rule</entity> <entity id="W97-0110.7">optimization</entity> for the information <entity id="W97-0110.8">extraction task</entity>. First, the <entity id="W97-0110.9">user</entity> creates specific <entity id="W97-0110.10">rules</entity> for the <entity id="W97-0110.11">target</entity> <entity id="W97-0110.12">information</entity> from the <entity id="W97-0110.13">sample</entity> articles through a <entity id="W97-0110.14">training</entity> <entity id="W97-0110.15">interface</entity>. Second, WordNet is <entity id="W97-0110.16">applied</entity> to generalize <entity id="W97-0110.17">noun</entity> <entity id="W97-0110.18">entities</entity> in the specific <entity id="W97-0110.19">rules</entity>. The <entity id="W97-0110.20">degree</entity> of <entity id="W97-0110.21">generalization</entity> is adjusted to fit the <entity id="W97-0110.22">user</entity>'s needs by use of the <entity id="W97-0110.23">statistical</entity> <entity id="W97-0110.24">Generalization</entity> <entity id="W97-0110.25">Tree</entity> <entity id="W97-0110.26">model</entity>. Finally, the optimally <entity id="W97-0110.27">generalized</entity> <entity id="W97-0110.28">rules</entity> are <entity id="W97-0110.29">applied</entity> to scan new <entity id="W97-0110.30">information</entity>. The <entity id="W97-0110.31">results</entity> of <entity id="W97-0110.32">experiments</entity> demonstrate the <entity id="W97-0110.33">applicability</entity> of our <entity id="W97-0110.34">Generalization</entity> <entity id="W97-0110.35">Tree</entity> <entity id="W97-0110.36">method</entity>.
</abstract>

</text>

<text id="W97-0605">
<title>Automatic Lexicon Enhancement By Means Of Corpus Tagging</title>
<abstract>
Using specialised <entity id="W97-0605.1">text</entity> <entity id="W97-0605.2">corpus</entity> to automatically enhance a general <entity id="W97-0605.3">lexicon</entity> is the aim of this <entity id="W97-0605.4">study</entity>. Indeed, having <entity id="W97-0605.5">lexicons</entity> which offer maximal cover on a specific <entity id="W97-0605.6">topic</entity> is an important <entity id="W97-0605.7">benefit</entity> in many <entity id="W97-0605.8">applications</entity> of <entity id="W97-0605.9">Automatic</entity> <entity id="W97-0605.10">Speech</entity> and <entity id="W97-0605.11">Natural Language Processing</entity>. The <entity id="W97-0605.12">enhancement</entity> of these <entity id="W97-0605.13">lexicons</entity> can be made <entity id="W97-0605.14">automatic</entity> as big <entity id="W97-0605.15">corpora</entity> of specialised <entity id="W97-0605.16">texts</entity> are available. A <entity id="W97-0605.17">syntactic</entity> <entity id="W97-0605.18">tagging</entity> <entity id="W97-0605.19">process</entity>, <entity id="W97-0605.20">based</entity> on 3-<entity id="W97-0605.21">class</entity> and 3-<entity id="W97-0605.22">gram language models</entity>, allows us to automatically allocate possible <entity id="W97-0605.23">syntactic categories</entity> to the Out-Of-<entity id="W97-0605.24">Vocabulary</entity> (OOV) <entity id="W97-0605.25">words</entity> which are found in the <entity id="W97-0605.26">corpus</entity> <entity id="W97-0605.27">processed</entity>. These OOV <entity id="W97-0605.28">words</entity> generally occur several <entity id="W97-0605.29">times</entity> in the <entity id="W97-0605.30">corpus</entity>, and a <entity id="W97-0605.31">number</entity> of these <entity id="W97-0605.32">occurrences</entity> can be important. By taking into account all the <entity id="W97-0605.33">occurrences</entity> of an OOV <entity id="W97-0605.34">word</entity> in a given <entity id="W97-0605.35">text</entity> as a whole, we <entity id="W97-0605.36">propose</entity> here a <entity id="W97-0605.37">method</entity> for automatically <entity id="W97-0605.38">extracting</entity> a specialised <entity id="W97-0605.39">lexicon</entity> from a <entity id="W97-0605.40">text</entity> <entity id="W97-0605.41">corpus</entity> which is <entity id="W97-0605.42">representative</entity> of a specific <entity id="W97-0605.43">topic</entity>.
</abstract>

</text>

<text id="W97-1107">
<title>Stochastic Phonological Grammars And Acceptability</title>
<abstract>
In foundational works of generative <entity id="W97-1107.1">phonology</entity> it is <entity id="W97-1107.2">claimed</entity> that subjects can reliably discriminate between possible but non-occurring <entity id="W97-1107.3">words</entity> and <entity id="W97-1107.4">words</entity> that could not be <entity id="W97-1107.5">English</entity>. In this <entity id="W97-1107.6">paper</entity> we examine the use of a probabilistic phonological <entity id="W97-1107.7">parser</entity> for <entity id="W97-1107.8">words</entity> to <entity id="W97-1107.9">model</entity> experimentally-obtained judgements of the acceptability of a set of nonsense <entity id="W97-1107.10">words</entity>. We compared various <entity id="W97-1107.11">methods</entity> of scoring the goodness of the <entity id="W97-1107.12">parse</entity> as a <entity id="W97-1107.13">predictor</entity> of acceptability. We found that the <entity id="W97-1107.14">probability</entity> of the worst <entity id="W97-1107.15">part</entity> is not the best score of acceptability, indicating that classical generative <entity id="W97-1107.16">phonology</entity> and Optimality <entity id="W97-1107.17">Theory</entity> miss an important fact, as these <entity id="W97-1107.18">approaches</entity> do not recognise a <entity id="W97-1107.19">mechanism</entity> by which the <entity id="W97-1107.20">frequency</entity> of well-formed <entity id="W97-1107.21">parts</entity> may ameliorate the unacceptability of <entity id="W97-1107.22">low-frequency</entity> <entity id="W97-1107.23">parts</entity>. We argue that probabilistic generative grammars are demonstrably a more psychologically realistic <entity id="W97-1107.24">model</entity> of phonological <entity id="W97-1107.25">competence</entity> than <entity id="W97-1107.26">standard</entity> generative <entity id="W97-1107.27">phonology</entity> or Optimality <entity id="W97-1107.28">Theory</entity>.
</abstract>

</text>

<text id="W97-1302">
<title>Constraints And Defaults Of Zero Pronouns In Japanese Instruction Manuals</title>
<abstract>
In this <entity id="W97-1302.1">paper</entity>, we <entity id="W97-1302.2">propose</entity> a <entity id="W97-1302.3">method</entity> for <entity id="W97-1302.4">anaphora resolution</entity> of zero subjects in <entity id="W97-1302.5">Japanese</entity> <entity id="W97-1302.6">manual</entity> <entity id="W97-1302.7">sentences</entity> <entity id="W97-1302.8">based</entity> on both the <entity id="W97-1302.9">nature</entity> of <entity id="W97-1302.10">language</entity> <entity id="W97-1302.11">expressions</entity> and the <entity id="W97-1302.12">ontology</entity> of ordinary <entity id="W97-1302.13">instruction</entity> <entity id="W97-1302.14">manuals</entity>. In <entity id="W97-1302.15">instruction</entity> <entity id="W97-1302.16">manuals</entity> written in <entity id="W97-1302.17">Japanese</entity>, zero subjects often introduce <entity id="W97-1302.18">ambiguity</entity> into <entity id="W97-1302.19">sentences</entity>. In <entity id="W97-1302.20">order</entity> to resolve them, we consider the <entity id="W97-1302.21">property</entity> of several <entity id="W97-1302.22">types</entity> of <entity id="W97-1302.23">expressions</entity> <entity id="W97-1302.24">including</entity> some <entity id="W97-1302.25">forms</entity> of <entity id="W97-1302.26">verbal</entity> <entity id="W97-1302.27">phrases</entity> and some conjunctives of <entity id="W97-1302.28">clauses</entity>, and so on. As the <entity id="W97-1302.29">result</entity>, we have a set of <entity id="W97-1302.30">constraints</entity> and <entity id="W97-1302.31">defaults</entity> for zero subject <entity id="W97-1302.32">resolution</entity>. We examine the <entity id="W97-1302.33">precision</entity> of the <entity id="W97-1302.34">constraints</entity> and <entity id="W97-1302.35">defaults</entity> with real <entity id="W97-1302.36">manual</entity> <entity id="W97-1302.37">sentences</entity>, and we have the <entity id="W97-1302.38">result</entity> that they make a good estimate with <entity id="W97-1302.39">precision</entity> of over 80%.
</abstract>

</text>

<text id="W98-0318">
<title>Automatic Disambiguation Of Discourse Particles</title>
<abstract>
In <entity id="W98-0318.1">spite</entity> of their important quantitative <entity id="W98-0318.2">role</entity>, <entity id="W98-0318.3">discourse</entity> <entity id="W98-0318.4">particles</entity> have so far been neglected in <entity id="W98-0318.5">automatic</entity> <entity id="W98-0318.6">speech processing</entity> for two <entity id="W98-0318.7">reasons</entity>: <entity id="W98-0318.8">Firstly</entity> it is not clear what they may contribute to the aims of <entity id="W98-0318.9">automatic</entity> <entity id="W98-0318.10">speech processing</entity>, and secondly their <entity id="W98-0318.11">functions</entity> seem to vary so much that it seems difficult to identify the <entity id="W98-0318.12">information</entity> relevant to such aims. The <entity id="W98-0318.13">approach</entity> presented here therefore attempts to <entity id="W98-0318.14">provide</entity> <entity id="W98-0318.15">automatic</entity> means to distinguishing the different readings of <entity id="W98-0318.16">discourse</entity> <entity id="W98-0318.17">particles</entity> and to filtering out the <entity id="W98-0318.18">information</entity> which can be useful for <entity id="W98-0318.19">speech understanding systems</entity>, employing positional <entity id="W98-0318.20">information</entity> and their <entity id="W98-0318.21">role</entity> within a <entity id="W98-0318.22">dialogue</entity> <entity id="W98-0318.23">model</entity> of the respective <entity id="W98-0318.24">domain</entity>, two <entity id="W98-0318.25">types</entity> of <entity id="W98-0318.26">information</entity> which are especially easy to obtain. First <entity id="W98-0318.27">results</entity> indicate that <entity id="W98-0318.28">discourse</entity> <entity id="W98-0318.29">particles</entity> can indeed be automatically disambiguated on the <entity id="W98-0318.30">basis</entity> of the <entity id="W98-0318.31">model</entity> <entity id="W98-0318.32">proposed</entity>.
</abstract>

</text>

<text id="W98-1224">
<title>Do Not Forget: Full Memory In Memory-Based Learning Of Word Pronunciation</title>
<abstract><entity id="W98-1224.1">Memory-based</entity> learning, keeping full <entity id="W98-1224.2">memory</entity> of learning material, appears a viable <entity id="W98-1224.3">approach</entity> to learning <entity id="W98-1224.4">NLP tasks</entity>, and is often superior in generalisation <entity id="W98-1224.5">accuracy</entity> to eager <entity id="W98-1224.6">learning approaches</entity> that <entity id="W98-1224.7">abstract</entity> from learning material. Here we investigate three
</abstract>

</text>

<text id="W98-1411">
<title>Experiments Using Stochastic Search For Text Planning</title>
<abstract> Marcu has characterised an important and difficult <entity id="W98-1411.1">problem</entity> in <entity id="W98-1411.2">text</entity> planning: given a set of facts to convey and a set of rhetorical <entity id="W98-1411.3">relations</entity> that can be used to <entity id="W98-1411.4">link</entity> them together, how can one arrange this material so as to <entity id="W98-1411.5">yield</entity> the best possible <entity id="W98-1411.6">text</entity>? We describe <entity id="W98-1411.7">experiments</entity> with a <entity id="W98-1411.8">number</entity> of heuristic <entity id="W98-1411.9">search</entity> <entity id="W98-1411.10">methods</entity> for this <entity id="W98-1411.11">task</entity>.
</abstract>

</text>

<text id="W99-0113">
<title>Logical Structure And Discourse Anaphora Resolution</title>
<abstract>
Working within the Dynamic <entity id="W99-0113.1">Quantifier</entity> <entity id="W99-0113.2">Logic</entity> (DQL) <entity id="W99-0113.3">framework</entity> ( <entity id="W99-0113.4">van</entity> <entity id="W99-0113.5">den</entity> Berg 1992,1996a,b), we <entity id="W99-0113.6">claim</entity> in this <entity id="W99-0113.7">paper</entity> that in every <entity id="W99-0113.8">language</entity> the <entity id="W99-0113.9">translation</entity> into a logical <entity id="W99-0113.10">language</entity> will be such that the <entity id="W99-0113.11">preference</entity> <entity id="W99-0113.12">ordering</entity> of possible <entity id="W99-0113.13">discourse</entity> <entity id="W99-0113.14">referents</entity> for an anaphor in a <entity id="W99-0113.15">sentence</entity> can be explained in <entity id="W99-0113.16">terms</entity> of the scopal <entity id="W99-0113.17">order</entity> of the <entity id="W99-0113.18">expressions</entity> in the antecedent that introduce the <entity id="W99-0113.19">discourse</entity> <entity id="W99-0113.20">referents</entity>. Since the <entity id="W99-0113.21">scope</entity> of <entity id="W99-0113.22">terms</entity> is derived from <entity id="W99-0113.23">arguments</entity> independent of any <entity id="W99-0113.24">discourse</entity> <entity id="W99-0113.25">theory</entity>, our account explains <entity id="W99-0113.26">discourse</entity> <entity id="W99-0113.27">anaphora resolution</entity> in <entity id="W99-0113.28">terms</entity> of general <entity id="W99-0113.29">principles</entity> of <entity id="W99-0113.30">utterance</entity> <entity id="W99-0113.31">semantics</entity>, from which the <entity id="W99-0113.32">predictions</entity> of <entity id="W99-0113.33">centering</entity> <entity id="W99-0113.34">theory</entity> follow. When combined with the powerful <entity id="W99-0113.35">discourse</entity> <entity id="W99-0113.36">structural</entity> <entity id="W99-0113.37">framework</entity> of the Linguistic <entity id="W99-0113.38">Discourse Model</entity> ( Polanyi (1985, 1986, 1988, 1996) Polanyi and Scha (1984), Scha and Polanyi (1988), Prfist, H., R. Scha and M. H. <entity id="W99-0113.39">van</entity> <entity id="W99-0113.40">den</entity> Berg, 1994 ; Polanyi, L. and M. H. <entity id="W99-0113.41">van</entity> <entity id="W99-0113.42">den</entity> Berg 1996 ; <entity id="W99-0113.43">van</entity> <entity id="W99-0113.44">den</entity> Berg, M. H. 1996b), we <entity id="W99-0113.45">provide</entity> a unified account of <entity id="W99-0113.46">discourse</entity> <entity id="W99-0113.47">anaphora resolution</entity>.
</abstract>

</text>

<text id="W99-0207">
<title>Corpus-Based Anaphora Resolution Towards Antecedent Preference</title>
<abstract>
In this <entity id="W99-0207.1">paper</entity> we <entity id="W99-0207.2">propose</entity> a <entity id="W99-0207.3">corpus-based approach</entity> to <entity id="W99-0207.4">anaphora resolution</entity> combining a <entity id="W99-0207.5">machine</entity> learning <entity id="W99-0207.6">method</entity> and <entity id="W99-0207.7">statistical</entity> <entity id="W99-0207.8">information</entity>. First, a <entity id="W99-0207.9">decision tree</entity> <entity id="W99-0207.10">trained</entity> on an annotated <entity id="W99-0207.11">corpus</entity> determines the coreference <entity id="W99-0207.12">relation</entity> of a given anaphor and antecedent <entity id="W99-0207.13">candidates</entity> and is utilized as a filter in <entity id="W99-0207.14">order</entity> to reduce the <entity id="W99-0207.15">number</entity> of potential <entity id="W99-0207.16">candidates</entity>. In the second <entity id="W99-0207.17">step</entity>, <entity id="W99-0207.18">preference</entity> <entity id="W99-0207.19">selection</entity> is achieved by taking into account the <entity id="W99-0207.20">frequency</entity> <entity id="W99-0207.21">information</entity> of coreferential and non-referential <entity id="W99-0207.22">pairs</entity> <entity id="W99-0207.23">tagged</entity> in the <entity id="W99-0207.24">training corpus</entity> as well as <entity id="W99-0207.25">distance</entity> <entity id="W99-0207.26">features</entity> within the <entity id="W99-0207.27">current</entity> <entity id="W99-0207.28">discourse</entity>. Preliminary <entity id="W99-0207.29">experiments</entity> <entity id="W99-0207.30">concerning</entity> the <entity id="W99-0207.31">resolution</entity> of <entity id="W99-0207.32">Japanese</entity> pronouns in <entity id="W99-0207.33">spoken-language</entity> <entity id="W99-0207.34">dialogs</entity> <entity id="W99-0207.35">result</entity> in a <entity id="W99-0207.36">success</entity> <entity id="W99-0207.37">rate</entity> of 80.6%.
</abstract>

</text>

<text id="H05-1046">
<title>Disambiguating Toponyms In News</title>
<abstract>
This <entity id="H05-1046.1">research</entity> is aimed at the <entity id="H05-1046.2">problem</entity> of <entity id="H05-1046.3">disambiguating</entity> toponyms (<entity id="H05-1046.4">place names</entity>) in <entity id="H05-1046.5">terms</entity> of a <entity id="H05-1046.6">classification</entity> derived by merging <entity id="H05-1046.7">information</entity> from two publicly available <entity id="H05-1046.8">gazetteers</entity>. To establish the <entity id="H05-1046.9">difficulty</entity> of the <entity id="H05-1046.10">problem</entity>, we measured the <entity id="H05-1046.11">degree</entity> of <entity id="H05-1046.12">ambiguity</entity>, with <entity id="H05-1046.13">respect</entity> to a <entity id="H05-1046.14">gazetteer</entity>, for toponyms in <entity id="H05-1046.15">news</entity>. We found that 67.82% of the toponyms found in a <entity id="H05-1046.16">corpus</entity> that were ambiguous in a <entity id="H05-1046.17">gazetteer</entity> <entity id="H05-1046.18">lacked</entity> a local discriminator in the <entity id="H05-1046.19">text</entity>. Given the <entity id="H05-1046.20">scarcity</entity> of human-annotated data, our <entity id="H05-1046.21">method</entity> used unsuper-vised <entity id="H05-1046.22">machine</entity> learning to <entity id="H05-1046.23">develop</entity> <entity id="H05-1046.24">disambiguation</entity> <entity id="H05-1046.25">rules</entity>. Toponyms were automatically <entity id="H05-1046.26">tagged</entity> with <entity id="H05-1046.27">information</entity> about them found in a <entity id="H05-1046.28">gazetteer</entity>. A toponym that was ambiguous in the <entity id="H05-1046.29">gazetteer</entity> was automatically disambiguated <entity id="H05-1046.30">based</entity> on <entity id="H05-1046.31">preference</entity> heuristics. This automatically <entity id="H05-1046.32">tagged</entity><entity id="H05-1046.33">data</entity> was used to <entity id="H05-1046.34">train</entity> a <entity id="H05-1046.35">machine</entity> learner, which disambigu-ated toponyms in a human-annotated <entity id="H05-1046.36">news</entity> <entity id="H05-1046.37">corpus</entity> at 78.5% <entity id="H05-1046.38">accuracy</entity>.
</abstract>

</text>

<text id="H05-1065">
<title>Disambiguation Of Morphological Structure Using A PCFG</title>
<abstract>
German has a productive <entity id="H05-1065.1">morphology</entity> and allows the <entity id="H05-1065.2">creation</entity> of <entity id="H05-1065.3">complex</entity> <entity id="H05-1065.4">words</entity> which are often highly ambiguous. This <entity id="H05-1065.5">paper</entity> <entity id="H05-1065.6">reports</entity> on the <entity id="H05-1065.7">development</entity> of a head-lexicalized PCFG for the <entity id="H05-1065.8">disambiguation</entity> of German <entity id="H05-1065.9">morphological analyses</entity>. The grammar is <entity id="H05-1065.10">trained</entity> on unla-beled<entity id="H05-1065.11">data</entity> using the Inside-Outside <entity id="H05-1065.12">algorithm</entity>. The <entity id="H05-1065.13">parser</entity> achieves a <entity id="H05-1065.14">precision</entity> of more than 68% on difficult <entity id="H05-1065.15">test</entity> data, which is 23% more than the baseline obtained by randomly choosing one of the simplest <entity id="H05-1065.16">analyses</entity>. Remarkable is the fact that <entity id="H05-1065.17">precision</entity> drops to 52% without lexicalization.
</abstract>

</text>

<text id="W00-0501">
<title>"When Is An Embedded MT System "Good Enough" For Filtering?"</title>
<abstract>
This <entity id="W00-0501.1">paper</entity> <entity id="W00-0501.2">proposes</entity> an end-to-end <entity id="W00-0501.3">process</entity> <entity id="W00-0501.4">analysis</entity> <entity id="W00-0501.5">template</entity> with replicable measures to <entity id="W00-0501.6">evaluate</entity> the filtering <entity id="W00-0501.7">performance</entity> of a Scan-<entity id="W00-0501.8">OCR-MT system</entity>. Preliminary <entity id="W00-0501.9">results</entity></abstract>

</text>

<text id="W00-0715">
<title>Experiments On Unsupervised Learning For Extracting Relevant Fragments From Spoken Dialog Corpus</title>
<abstract>
In this <entity id="W00-0715.1">paper</entity> are described <entity id="W00-0715.2">experiments</entity> on unsupervised <entity id="W00-0715.3">learning</entity> of the <entity id="W00-0715.4">domain</entity> <entity id="W00-0715.5">lexicon</entity> and relevant <entity id="W00-0715.6">phrase</entity> <entity id="W00-0715.7">fragments</entity> from a <entity id="W00-0715.8">dialog</entity> <entity id="W00-0715.9">corpus</entity>. Suggested <entity id="W00-0715.10">approach</entity> is <entity id="W00-0715.11">based</entity> on using <entity id="W00-0715.12">domain</entity> independent <entity id="W00-0715.13">words</entity> for <entity id="W00-0715.14">chunking</entity> and using semantical predictional power of such <entity id="W00-0715.15">words</entity> for <entity id="W00-0715.16">clustering</entity> and <entity id="W00-0715.17">automatic</entity> <entity id="W00-0715.18">extraction</entity> <entity id="W00-0715.19">phrase</entity> <entity id="W00-0715.20">fragments</entity> relevant to <entity id="W00-0715.21">dialog</entity> <entity id="W00-0715.22">topics</entity>.
</abstract>

</text>

<text id="W01-0814">
<title>A Paraphrase-Based Exploration Of Cohesiveness Criteria</title>
<abstract>
This <entity id="W01-0814.1">paper</entity> <entity id="W01-0814.2">proposes</entity> an empirical <entity id="W01-0814.3">approach</entity> to the <entity id="W01-0814.4">development</entity> of a <entity id="W01-0814.5">computational model</entity> for assessing <entity id="W01-0814.6">texts</entity> according to cohesiveness. We argue that the NLG <entity id="W01-0814.7">technologies</entity> for the <entity id="W01-0814.8">generation</entity> of <entity id="W01-0814.9">structural</entity> paraphrases can be used to efficiently create what we <entity id="W01-0814.10">call</entity> a <entity id="W01-0814.11">cohesion-variant</entity> <entity id="W01-0814.12">parallel corpus</entity>, which would serve as a good <entity id="W01-0814.13">resource</entity> for empirical <entity id="W01-0814.14">acquisition</entity> of cohesiveness <entity id="W01-0814.15">criteria</entity>. We also present our <entity id="W01-0814.16">pilot</entity> <entity id="W01-0814.17">case study</entity>, in which we took a particular <entity id="W01-0814.18">type</entity> of paraphrasing that separates a <entity id="W01-0814.19">relative</entity> <entity id="W01-0814.20">clause</entity> from a <entity id="W01-0814.21">sentence</entity>. We have so far created a <entity id="W01-0814.22">cohesion-variant</entity> <entity id="W01-0814.23">parallel corpus</entity> containing 499 cohesive <entity id="W01-0814.24">instances</entity> and 841 incohesive <entity id="W01-0814.25">instances</entity>. <entity id="W01-0814.26">Based</entity> on this <entity id="W01-0814.27">corpus</entity>, we conducted a preliminary <entity id="W01-0814.28">experiment</entity> on <entity id="W01-0814.29">cohesion</entity> <entity id="W01-0814.30">evaluation</entity>, obtaining encouraging <entity id="W01-0814.31">results</entity>.
</abstract>

</text>

<text id="W01-1509">
<title>Tools And Resources For Tree Adjoining Grammars</title>
<abstract>
This <entity id="W01-1509.1">paper</entity> presents a workbench for <entity id="W01-1509.2">Tree</entity> Adjoining Grammars that we are currently <entity id="W01-1509.3">developing</entity>. This workbench <entity id="W01-1509.4">includes</entity> several <entity id="W01-1509.5">tools</entity> and <entity id="W01-1509.6">resources</entity> <entity id="W01-1509.7">based</entity> on the <entity id="W01-1509.8">markup</entity> <entity id="W01-1509.9">language</entity> XML, used as a convenient <entity id="W01-1509.10">language</entity> to <entity id="W01-1509.11">format</entity> and exchange <entity id="W01-1509.12">linguistic resources</entity>.
</abstract>

</text>

<text id="W02-1037">
<title>Processing Comparable Corpora With Bilingual Suffix Trees</title>
<abstract>
We introduce Bilingual <entity id="W02-1037.1">Suffix</entity> Trees (BST), a<entity id="W02-1037.2">data</entity> <entity id="W02-1037.3">structure</entity> that is suitable for exploiting comparable <entity id="W02-1037.4">corpora</entity>. We discuss <entity id="W02-1037.5">algorithms</entity> that use BSTs in <entity id="W02-1037.6">order</entity> to create <entity id="W02-1037.7">parallel corpora</entity> and learn <entity id="W02-1037.8">translations</entity> of unseen <entity id="W02-1037.9">words</entity> from comparable <entity id="W02-1037.10">corpora</entity>. Starting with a small bilingual <entity id="W02-1037.11">dictionary</entity> that was derived automatically from a <entity id="W02-1037.12">corpus</entity> of 5.000 parallel <entity id="W02-1037.13">sentences</entity>, we have automatically <entity id="W02-1037.14">extracted</entity> a <entity id="W02-1037.15">corpus</entity> of 33.926 parallel <entity id="W02-1037.16">phrases</entity> of <entity id="W02-1037.17">size</entity> greater than 3, and learned 9 new <entity id="W02-1037.18">word</entity> <entity id="W02-1037.19">translations</entity> from a comparable <entity id="W02-1037.20">corpus</entity> of 1.3M <entity id="W02-1037.21">words</entity> (100.000 <entity id="W02-1037.22">sentences</entity>).
</abstract>

</text>

<text id="W02-2006">
<title>Bootstrapping A Multilingual Part-Of-Speech Tagger In One Person-Day</title>
<abstract>
This <entity id="W02-2006.1">paper</entity> presents a <entity id="W02-2006.2">method</entity> for <entity id="W02-2006.3">bootstrapping</entity> a fine-grained, <entity id="W02-2006.4">broad-coverage</entity> <entity id="W02-2006.5">part-of-speech</entity> (POS) tagger in a new <entity id="W02-2006.6">language</entity> using only one person-day of<entity id="W02-2006.7">data</entity> <entity id="W02-2006.8">acquisition</entity> <entity id="W02-2006.9">effort</entity>. It <entity id="W02-2006.10">requires</entity> only three <entity id="W02-2006.11">resources</entity>, which are currently readily available in 60-100 world <entity id="W02-2006.12">languages</entity>: (1) an online or hard-copy pocket-sized bilingual <entity id="W02-2006.13">dictionary</entity>, (2) a <entity id="W02-2006.14">basic</entity> <entity id="W02-2006.15">library</entity> <entity id="W02-2006.16">reference</entity> grammar, and (3) <entity id="W02-2006.17">access</entity> to an existing monolingual <entity id="W02-2006.18">text</entity> <entity id="W02-2006.19">corpus</entity> in the <entity id="W02-2006.20">language</entity>. The <entity id="W02-2006.21">algorithm</entity> begins by inducing initial <entity id="W02-2006.22">lexical</entity> POS <entity id="W02-2006.23">distributions</entity> from <entity id="W02-2006.24">English</entity> <entity id="W02-2006.25">translations</entity> in a bilingual <entity id="W02-2006.26">dictionary</entity> without <entity id="W02-2006.27">POS tags</entity>. It handles irregular, regular and semi-regular <entity id="W02-2006.28">morphology</entity> through a <entity id="W02-2006.29">robust</entity> <entity id="W02-2006.30">generative model</entity> using weighted Levenshtein <entity id="W02-2006.31">alignments</entity>. Unsupervised <entity id="W02-2006.32">induction</entity> ofgrammatical <entity id="W02-2006.33">gender</entity> is <entity id="W02-2006.34">performed</entity> via global <entity id="W02-2006.35">modeling</entity> <entity id="W02-2006.36">ofcontext-window</entity> <entity id="W02-2006.37">feature</entity> <entity id="W02-2006.38">agreement</entity>. Using a <entity id="W02-2006.39">combination</entity> of these and other <entity id="W02-2006.40">evidence</entity> <entity id="W02-2006.41">sources</entity>, interactive <entity id="W02-2006.42">training</entity> of <entity id="W02-2006.43">context</entity> and <entity id="W02-2006.44">lexical</entity> prior <entity id="W02-2006.45">models</entity> are accomplished for fine-grained <entity id="W02-2006.46">POS tag</entity> <entity id="W02-2006.47">spaces</entity>. <entity id="W02-2006.48">Experiments</entity> show high <entity id="W02-2006.49">accuracy</entity>, fine-grained <entity id="W02-2006.50">tag</entity> <entity id="W02-2006.51">resolution</entity> with minimal new human <entity id="W02-2006.52">effort</entity>.
</abstract>

</text>

<text id="W03-0308">
<title>TREQ-AL: A Word Alignment System With Limited Language Resources</title>
<abstract>
We <entity id="W03-0308.1">provide</entity> a rather informal <entity id="W03-0308.2">presentation</entity> of a <entity id="W03-0308.3">prototype</entity> <entity id="W03-0308.4">system</entity> for <entity id="W03-0308.5">word alignment</entity> <entity id="W03-0308.6">based</entity> on our previous <entity id="W03-0308.7">translation</entity> <entity id="W03-0308.8">equivalence</entity> <entity id="W03-0308.9">approach</entity>, discuss the <entity id="W03-0308.10">problems</entity> encountered in the <entity id="W03-0308.11">shared-task</entity> on <entity id="W03-0308.12">word-aligning</entity> of a parallel Romanian-<entity id="W03-0308.13">English</entity> <entity id="W03-0308.14">text</entity>, present the preliminary <entity id="W03-0308.15">evaluation results</entity> and suggest further ways of <entity id="W03-0308.16">improving</entity> the <entity id="W03-0308.17">alignment</entity> <entity id="W03-0308.18">accuracy</entity>.
</abstract>

</text>

<text id="W03-0602">
<title>Words And Pictures In The News</title>
<abstract>
We discuss the <entity id="W03-0602.1">properties</entity> of a <entity id="W03-0602.2">collection</entity> of <entity id="W03-0602.3">news</entity> photos and captions, collected from the Associated Press and Reuters. Captions have a <entity id="W03-0602.4">vocabulary</entity> dominated by <entity id="W03-0602.5">proper names</entity>. We have <entity id="W03-0602.6">implemented</entity> various <entity id="W03-0602.7">text</entity> <entity id="W03-0602.8">clustering</entity> <entity id="W03-0602.9">algorithms</entity> to organize these <entity id="W03-0602.10">items</entity> by <entity id="W03-0602.11">topic</entity>, as well as an iconic matcher that identifies articles that share a picture. We have found that the special <entity id="W03-0602.12">structure</entity> of captions allows us to <entity id="W03-0602.13">extract</entity> some <entity id="W03-0602.14">names</entity> of people actually portrayed in the image quite reliably, using a <entity id="W03-0602.15">simple</entity> <entity id="W03-0602.16">syntactic analysis</entity>. We have been able to build a directory of face images of <entity id="W03-0602.17">individuals</entity> from this <entity id="W03-0602.18">collection</entity>.
</abstract>

</text>

<text id="W03-0605">
<title>An Architecture For Word Learning Using Bidirectional Multimodal Structural Alignment</title>
<abstract>
"<entity id="W03-0605.1">Learning</entity> of new <entity id="W03-0605.2">words</entity> is assisted by <entity id="W03-0605.3">contextual information</entity>. This <entity id="W03-0605.4">context</entity> can come in several <entity id="W03-0605.5">forms</entity>, <entity id="W03-0605.6">including</entity> <entity id="W03-0605.7">observations</entity> in non-linguistic <entity id="W03-0605.8">semantic</entity> <entity id="W03-0605.9">domains</entity>, as well as the linguistic <entity id="W03-0605.10">context</entity> in which the new <entity id="W03-0605.11">word</entity> was presented. We <entity id="W03-0605.12">outline</entity> a general <entity id="W03-0605.13">architecture</entity> for <entity id="W03-0605.14">word</entity> <entity id="W03-0605.15">learning</entity>, in which <entity id="W03-0605.16">structural</entity> <entity id="W03-0605.17">alignment</entity> coordinates this <entity id="W03-0605.18">contextual information</entity> in <entity id="W03-0605.19">order</entity> to restrict the possible <entity id="W03-0605.20">interpretations</entity> of <entity id="W03-0605.21">unknown words</entity>. We identify spatial <entity id="W03-0605.22">relations</entity> as an applicable <entity id="W03-0605.23">semantic</entity> <entity id="W03-0605.24">domain</entity>, and describe a <entity id="W03-0605.25">system-in-progress</entity> for <entity id="W03-0605.26">implementing</entity> the general <entity id="W03-0605.27">architecture</entity> using <entity id="W03-0605.28">video</entity> <entity id="W03-0605.29">sequences</entity> as our non-linguistic <entity id="W03-0605.30">input</entity>. For <entity id="W03-0605.31">example</entity>, when the complete <entity id="W03-0605.32">system</entity> is presented with "The bird dove to the rock," with a <entity id="W03-0605.33">video</entity> <entity id="W03-0605.34">sequence</entity> of a bird flying from a <entity id="W03-0605.35">tree</entity> to a rock, and with the meanings for all the <entity id="W03-0605.36">words</entity> except the <entity id="W03-0605.37">preposition</entity> "to," the <entity id="W03-0605.38">system</entity> will register the unknown "to" with the corresponding <entity id="W03-0605.39">aspect</entity> of the bird's trajectory. "
</abstract>

</text>

<text id="W03-0901">
<title>A Knowledge-Driven Approach To Text Meaning Processing</title>
<abstract>
"Our <entity id="W03-0901.1">goal</entity> is to be able to answer <entity id="W03-0901.2">questions</entity> about <entity id="W03-0901.3">text</entity> that go beyond facts explicitly stated in the <entity id="W03-0901.4">text</entity>, a <entity id="W03-0901.5">task</entity> which inherently <entity id="W03-0901.6">requires</entity> <entity id="W03-0901.7">extracting</entity> a "deep" <entity id="W03-0901.8">level</entity> of meaning from that <entity id="W03-0901.9">text</entity>. Our <entity id="W03-0901.10">approach</entity> treats meaning <entity id="W03-0901.11">processing</entity> fundamentally as a <entity id="W03-0901.12">modeling</entity> activity, in which a <entity id="W03-0901.13">knowledge base</entity> of <entity id="W03-0901.14">common-sense</entity> <entity id="W03-0901.15">expectations</entity> guides <entity id="W03-0901.16">interpretation</entity> of <entity id="W03-0901.17">text</entity>, and <entity id="W03-0901.18">text</entity> suggests which <entity id="W03-0901.19">parts</entity> of the <entity id="W03-0901.20">knowledge base</entity> might be relevant. In this <entity id="W03-0901.21">paper</entity>, we describe our ongoing <entity id="W03-0901.22">investigations</entity> to <entity id="W03-0901.23">develop</entity> this <entity id="W03-0901.24">approach</entity> into a usable <entity id="W03-0901.25">method</entity> for meaning <entity id="W03-0901.26">processing</entity>. "
</abstract>

</text>

<text id="W03-1205">
<title>An Evolutionary Approach For Improving The Quality Of Automatic Summaries</title>
<abstract><entity id="W03-1205.1">Automatic</entity> <entity id="W03-1205.2">text</entity> <entity id="W03-1205.3">extraction</entity> <entity id="W03-1205.4">techniques</entity> have proved <entity id="W03-1205.5">robust</entity>, but very often their <entity id="W03-1205.6">summaries</entity> are not coherent. In this <entity id="W03-1205.7">paper</entity>, we <entity id="W03-1205.8">propose</entity> a new <entity id="W03-1205.9">extraction method</entity> which uses local <entity id="W03-1205.10">coherence</entity> as a means to <entity id="W03-1205.11">improve</entity> the overall <entity id="W03-1205.12">quality</entity> of <entity id="W03-1205.13">automatic</entity> <entity id="W03-1205.14">summaries</entity>. Two <entity id="W03-1205.15">algorithms</entity> for <entity id="W03-1205.16">sentence</entity> <entity id="W03-1205.17">selection</entity> are <entity id="W03-1205.18">proposed</entity> and <entity id="W03-1205.19">evaluated</entity> on scientific <entity id="W03-1205.20">documents</entity>. <entity id="W03-1205.21">Evaluation</entity> showed that the <entity id="W03-1205.22">method</entity> ameliorates the <entity id="W03-1205.23">quality</entity> of <entity id="W03-1205.24">summaries</entity>, noticeable <entity id="W03-1205.25">improvements</entity> being obtained for longer <entity id="W03-1205.26">summaries</entity> produced by an <entity id="W03-1205.27">algorithm</entity> which selects <entity id="W03-1205.28">sentences</entity> using an evolutionary <entity id="W03-1205.29">algorithm</entity>.
</abstract>

</text>

<text id="W03-1302">
<title>Unsupervised Monolingual And Bilingual Word-Sense Disambiguation Of Medical Documents Using UMLS</title>
<abstract>
This <entity id="W03-1302.1">paper</entity> describes <entity id="W03-1302.2">techniques</entity> for unsupervised <entity id="W03-1302.3">word sense disambiguation</entity> of <entity id="W03-1302.4">English</entity> and German medical <entity id="W03-1302.5">documents</entity> using UMLS. We present both monolingual <entity id="W03-1302.6">techniques</entity> which rely only on the <entity id="W03-1302.7">structure</entity> of UMLS, and bilingual <entity id="W03-1302.8">techniques</entity> which also rely on the <entity id="W03-1302.9">availability</entity> of <entity id="W03-1302.10">parallel corpora</entity>. The best <entity id="W03-1302.11">results</entity> are obtained using <entity id="W03-1302.12">relations</entity> between <entity id="W03-1302.13">terms</entity> given by UMLS, a <entity id="W03-1302.14">method</entity> which achieves 74% <entity id="W03-1302.15">precision</entity>, 66% <entity id="W03-1302.16">coverage</entity> for <entity id="W03-1302.17">English</entity> and 79% <entity id="W03-1302.18">precision</entity>, 73% <entity id="W03-1302.19">coverage</entity> for German on <entity id="W03-1302.20">evaluation</entity> <entity id="W03-1302.21">corpora</entity> and over 83% <entity id="W03-1302.22">coverage</entity> over the whole <entity id="W03-1302.23">corpus</entity>. The <entity id="W03-1302.24">success</entity> of this <entity id="W03-1302.25">technique</entity> for German shows that a <entity id="W03-1302.26">lexical resource</entity> giving <entity id="W03-1302.27">relations</entity> between <entity id="W03-1302.28">concepts</entity> used to <entity id="W03-1302.29">index</entity> an <entity id="W03-1302.30">English</entity> <entity id="W03-1302.31">document</entity> <entity id="W03-1302.32">collection</entity> can be used for <entity id="W03-1302.33">high quality</entity> <entity id="W03-1302.34">disambiguation</entity> in another <entity id="W03-1302.35">language</entity>.
</abstract>

</text>

<text id="W08-0303">
<title>Discriminative Word Alignment via Alignment Matrix Modeling</title>
<abstract>
In this <entity id="W08-0303.1">paper</entity> a new discriminative <entity id="W08-0303.2">word alignment</entity> <entity id="W08-0303.3">method</entity> is presented. This <entity id="W08-0303.4">approach</entity> <entity id="W08-0303.5">models</entity> directly the <entity id="W08-0303.6">alignment</entity> <entity id="W08-0303.7">matrix</entity> by a <entity id="W08-0303.8">conditional random field</entity> (CRF) and so no <entity id="W08-0303.9">restrictions</entity> to the <entity id="W08-0303.10">alignments</entity> have to be made. Furthermore, it is easy to add <entity id="W08-0303.11">features</entity> and so all available <entity id="W08-0303.12">information</entity> can be used. Since the <entity id="W08-0303.13">structure</entity> of the CRFs can get <entity id="W08-0303.14">complex</entity>, the <entity id="W08-0303.15">inference</entity> can only be done approximately and the <entity id="W08-0303.16">standard</entity> <entity id="W08-0303.17">algorithms</entity> had to be <entity id="W08-0303.18">adapted</entity>. In <entity id="W08-0303.19">addition</entity>, different <entity id="W08-0303.20">methods</entity> to <entity id="W08-0303.21">train</entity> the <entity id="W08-0303.22">model</entity> have been <entity id="W08-0303.23">developed</entity>. Using this <entity id="W08-0303.24">approach</entity> the <entity id="W08-0303.25">alignment</entity> <entity id="W08-0303.26">quality</entity> could be <entity id="W08-0303.27">improved</entity> by up to 23 percent for 3 different <entity id="W08-0303.28">language pairs</entity> compared to a <entity id="W08-0303.29">combination</entity> of both IBM4-<entity id="W08-0303.30">alignments</entity>. Furthermore the <entity id="W08-0303.31">word alignment</entity> was used to <entity id="W08-0303.32">generate</entity> new <entity id="W08-0303.33">phrase</entity> <entity id="W08-0303.34">tables</entity>. These could <entity id="W08-0303.35">improve</entity> the <entity id="W08-0303.36">translation quality</entity> significantly.
</abstract>

</text>

<text id="W08-0304">
<title>Regularization and Search for Minimum Error Rate Training</title>
<abstract><entity id="W08-0304.1">Minimum error rate training</entity> (MERT) is a widely used learning <entity id="W08-0304.2">procedure</entity> for <entity id="W08-0304.3">statistical machine translation</entity> <entity id="W08-0304.4">models</entity>. We <entity id="W08-0304.5">contrast</entity> three <entity id="W08-0304.6">search strategies</entity> for MERT: Powell 's <entity id="W08-0304.7">method</entity>, the <entity id="W08-0304.8">variant</entity> of coordinate descent found in the Moses MERT <entity id="W08-0304.9">utility</entity>, and a novel stochastic <entity id="W08-0304.10">method</entity>. It is shown that the stochastic <entity id="W08-0304.11">method</entity> obtains <entity id="W08-0304.12">test set</entity> <entity id="W08-0304.13">gains</entity> of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a <entity id="W08-0304.14">method</entity> for regularizing the MERT <entity id="W08-0304.15">objective</entity> that achieves statistically significant <entity id="W08-0304.16">gains</entity> when combined with both Powell 's <entity id="W08-0304.17">method</entity> and coordinate descent.
</abstract>

</text>

<text id="W08-0411">
<title>Syntax-Driven Learning of Sub-Sentential Translation Equivalents and Translation Rules from Parsed Parallel Corpora</title>
<abstract>
We describe a <entity id="W08-0411.1">multi-step</entity> <entity id="W08-0411.2">process</entity> for automatically learning reliable sub-sentential <entity id="W08-0411.3">syntactic</entity> <entity id="W08-0411.4">phrases</entity> that are <entity id="W08-0411.5">translation</entity> equivalents of each other and <entity id="W08-0411.6">syntactic</entity> <entity id="W08-0411.7">translation</entity> <entity id="W08-0411.8">rules</entity> between two <entity id="W08-0411.9">languages</entity>. The <entity id="W08-0411.10">input</entity> to the <entity id="W08-0411.11">process</entity> is a <entity id="W08-0411.12">corpus</entity> of parallel <entity id="W08-0411.13">sentences</entity>, <entity id="W08-0411.14">word-aligned</entity> and annotated with <entity id="W08-0411.15">phrase-structure</entity> <entity id="W08-0411.16">parse trees</entity>. We first <entity id="W08-0411.17">apply</entity> a newly <entity id="W08-0411.18">developed</entity> <entity id="W08-0411.19">algorithm</entity> for aligning <entity id="W08-0411.20">parse-tree</entity> <entity id="W08-0411.21">nodes</entity> between the two parallel <entity id="W08-0411.22">trees</entity>. Next, we <entity id="W08-0411.23">extract</entity> all aligned sub-sentential <entity id="W08-0411.24">syntactic</entity> <entity id="W08-0411.25">constituents</entity> from the parallel <entity id="W08-0411.26">sentences</entity>, and create a <entity id="W08-0411.27">syntax-based</entity> <entity id="W08-0411.28">phrase-table</entity>. Finally, we treat the <entity id="W08-0411.29">node</entity> <entity id="W08-0411.30">alignments</entity> as <entity id="W08-0411.31">tree</entity> <entity id="W08-0411.32">decomposition</entity> points and <entity id="W08-0411.33">extract</entity> from the <entity id="W08-0411.34">corpus</entity> all possible synchronous parallel <entity id="W08-0411.35">tree</entity> <entity id="W08-0411.36">fragments</entity>. These are then converted into synchronous <entity id="W08-0411.37">context-free</entity> <entity id="W08-0411.38">rules</entity>. We describe the <entity id="W08-0411.39">approach</entity> and analyze its <entity id="W08-0411.40">application</entity> to <entity id="W08-0411.41">Chinese-</entity><entity id="W08-0411.42">English</entity> parallel<entity id="W08-0411.43">data</entity>.
</abstract>

</text>

<text id="J81-1001">
<title>Determining Verb Phrase Referents In Dialogs</title>
<abstract>
"This <entity id="J81-1001.1">paper</entity> discusses two <entity id="J81-1001.2">problems</entity> central to the <entity id="J81-1001.3">interpretation</entity> of <entity id="J81-1001.4">utterances</entity>: determining the <entity id="J81-1001.5">relationship</entity> between <entity id="J81-1001.6">actions</entity> described in an <entity id="J81-1001.7">utterance</entity> and <entity id="J81-1001.8">events</entity> in the world, and inferring the "state of the world" from <entity id="J81-1001.9">utterances</entity>. <entity id="J81-1001.10">Knowledge</entity> of the <entity id="J81-1001.11">language</entity>, <entity id="J81-1001.12">knowledge</entity> about the general subject being discussed, and <entity id="J81-1001.13">knowledge</entity> about the <entity id="J81-1001.14">current</entity> <entity id="J81-1001.15">situation</entity> are all necessary for this. The <entity id="J81-1001.16">problem</entity> of determining an <entity id="J81-1001.17">action</entity> referred to by a <entity id="J81-1001.18">verb</entity> <entity id="J81-1001.19">phrase</entity> is analogous to the <entity id="J81-1001.20">problem</entity> of determining the <entity id="J81-1001.21">object</entity> referred to by a <entity id="J81-1001.22">noun phrase</entity>.This <entity id="J81-1001.23">paper</entity> presents an <entity id="J81-1001.24">approach</entity> to the <entity id="J81-1001.25">problem</entity> of determining <entity id="J81-1001.26">verb</entity> <entity id="J81-1001.27">phrase</entity> <entity id="J81-1001.28">referents</entity> in which <entity id="J81-1001.29">knowledge</entity> about <entity id="J81-1001.30">language</entity>, the subject <entity id="J81-1001.31">area</entity>, and the <entity id="J81-1001.32">dialog</entity> itself is combined to interpret such <entity id="J81-1001.33">references</entity>. Presented and discussed are the <entity id="J81-1001.34">kinds</entity> of <entity id="J81-1001.35">knowledge</entity> necessary for interpreting <entity id="J81-1001.36">references</entity> to <entity id="J81-1001.37">actions</entity>, as well as <entity id="J81-1001.38">algorithms</entity> for using that <entity id="J81-1001.39">knowledge</entity> in interpreting <entity id="J81-1001.40">dialog</entity> <entity id="J81-1001.41">utterances</entity> about ongoing <entity id="J81-1001.42">tasks</entity> and for drawing <entity id="J81-1001.43">inferences</entity> about the <entity id="J81-1001.44">task</entity> <entity id="J81-1001.45">situation</entity> that are <entity id="J81-1001.46">based</entity> on a given <entity id="J81-1001.47">interpretation</entity>."
</abstract>

</text>

<text id="J87-1005">
<title>An Algorithm For Generating Quantifier Scopings</title>
<abstract>
The <entity id="J87-1005.1">syntactic structure</entity> of a <entity id="J87-1005.2">sentence</entity> often manifests quite clearly the <entity id="J87-1005.3">predicate-argument structure</entity> and <entity id="J87-1005.4">relations</entity> of grammatical subordination. But <entity id="J87-1005.5">scope</entity> <entity id="J87-1005.6">dependencies</entity> are not so transparent. As a <entity id="J87-1005.7">result</entity>, many <entity id="J87-1005.8">systems</entity> for representing the <entity id="J87-1005.9">semantics</entity> of <entity id="J87-1005.10">sentences</entity> have ignored scoping or <entity id="J87-1005.11">generated</entity> scopings with <entity id="J87-1005.12">mechanisms</entity> that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow.This <entity id="J87-1005.13">paper</entity> presents, along with proofs of some of its important <entity id="J87-1005.14">properties</entity>, an <entity id="J87-1005.15">algorithm</entity> that <entity id="J87-1005.16">generates</entity> scoped <entity id="J87-1005.17">semantic</entity> <entity id="J87-1005.18">forms</entity> from unscoped <entity id="J87-1005.19">expressions</entity> encoding <entity id="J87-1005.20">predicate-argument structure</entity>. The <entity id="J87-1005.21">algorithm</entity> is not profligate as are those <entity id="J87-1005.22">based</entity> on <entity id="J87-1005.23">permutation</entity> of <entity id="J87-1005.24">quantifiers</entity>, and it can <entity id="J87-1005.25">provide</entity> a solid foundation for <entity id="J87-1005.26">computational</entity> <entity id="J87-1005.27">solutions</entity> where <entity id="J87-1005.28">completeness</entity> is sacrificed for <entity id="J87-1005.29">efficiency</entity> and heuristic efficacy.
</abstract>

</text>

<text id="J93-1008">
<title>The Problem Of Logical-Form Equivalence</title>
<abstract>
"This <entity id="J93-1008.1">note</entity> discusses the <entity id="J93-1008.2">problem</entity> of <entity id="J93-1008.3">logical-form</entity> <entity id="J93-1008.4">equivalence</entity>, a <entity id="J93-1008.5">problem</entity> in <entity id="J93-1008.6">natural language generation</entity> first <entity id="J93-1008.7">noted</entity> in print by Douglas Appelt (1987). Appelt describes the <entity id="J93-1008.8">problem</entity>, which we first came across in pursuing joint work on the GeneSys <entity id="J93-1008.9">natural language generation system</entity>, as a <entity id="J93-1008.10">problem</entity> following from the <entity id="J93-1008.11">goal</entity> of eliminating the need for a strategic <entity id="J93-1008.12">generation</entity> <entity id="J93-1008.13">component</entity> to possess detailed grammatical <entity id="J93-1008.14">knowledge</entity>. In a <entity id="J93-1008.15">paper</entity> describing the GeneSys tactical <entity id="J93-1008.16">generation</entity> <entity id="J93-1008.17">component</entity> ( Shieber 1988 ), I <entity id="J93-1008.18">claimed</entity> that the <entity id="J93-1008.19">problem</entity> was "AI-complete," in the <entity id="J93-1008.20">sense</entity> that its <entity id="J93-1008.21">resolution</entity> would involve a <entity id="J93-1008.22">solution</entity> to the general <entity id="J93-1008.23">knowledge representation</entity> <entity id="J93-1008.24">problem</entity>. Since the publication of these two <entity id="J93-1008.25">papers</entity>, several <entity id="J93-1008.26">researchers</entity> have <entity id="J93-1008.27">claimed</entity> to have <entity id="J93-1008.28">solved</entity> the <entity id="J93-1008.29">problem</entity> of <entity id="J93-1008.30">logical-form</entity> <entity id="J93-1008.31">equivalence</entity>. In this <entity id="J93-1008.32">paper</entity>, I <entity id="J93-1008.33">review</entity> the <entity id="J93-1008.34">problem</entity> and attempt to highlight certain salient <entity id="J93-1008.35">aspects</entity> of it that have been lost in the pursuing of <entity id="J93-1008.36">solutions</entity>, in <entity id="J93-1008.37">order</entity> to reconcile the apparently contradictory <entity id="J93-1008.38">claims</entity> of the <entity id="J93-1008.39">problem</entity>'s intractability and its <entity id="J93-1008.40">resolution</entity>. "
</abstract>

</text>

<text id="J96-4006">
<title>Integrating General-Purpose And Corpus-Based Verb Classification</title>
<abstract>
A long-standing debate in the <entity id="J96-4006.1">computational</entity> linguistic <entity id="J96-4006.2">community</entity> is about the <entity id="J96-4006.3">generality</entity> of <entity id="J96-4006.4">lexical</entity> <entity id="J96-4006.5">taxonomies</entity>. Many <entity id="J96-4006.6">linguists</entity> ( Nirenburg 1995 ; Hirst 1995 ) stress that <entity id="J96-4006.7">taxonomies</entity> that are not <entity id="J96-4006.8">language</entity> neutral, at least at the intermediate and high <entity id="J96-4006.9">level</entity>, have little hope of <entity id="J96-4006.10">success</entity>. On the other <entity id="J96-4006.11">hand</entity>, <entity id="J96-4006.12">lexicon</entity> builders who have <entity id="J96-4006.13">experience</entity> of <entity id="J96-4006.14">designing</entity> <entity id="J96-4006.15">taxonomies</entity> for real <entity id="J96-4006.16">applications</entity> <entity id="J96-4006.17">claim</entity> that in sublanguages there exist very <entity id="J96-4006.18">domain-dependent</entity> <entity id="J96-4006.19">similarity</entity> <entity id="J96-4006.20">relations</entity>. Given our <entity id="J96-4006.21">experience</entity> and <entity id="J96-4006.22">results</entity>, we are inclined to take the second position, but we are indeed sensitive to the theoretical <entity id="J96-4006.23">motivations</entity> of the first. The <entity id="J96-4006.24">problem</entity> is that the <entity id="J96-4006.25">similarity</entity> <entity id="J96-4006.26">relations</entity> suggested by the thematic <entity id="J96-4006.27">structures</entity> of wordscompositional
</abstract>

</text>

<text id="J00-2004">
<title>Models Of Translational Equivalence Among Words</title>
<abstract><entity id="J00-2004.1">Parallel texts</entity> (bitexts) have <entity id="J00-2004.2">properties</entity> that distinguish them from other <entity id="J00-2004.3">kinds</entity> of parallel<entity id="J00-2004.4">data</entity>. First, most <entity id="J00-2004.5">words</entity> <entity id="J00-2004.6">translate</entity> to only one other <entity id="J00-2004.7">word</entity>. Second, bitext <entity id="J00-2004.8">correspondence</entity> is typically only <entity id="J00-2004.9">partial</entity>
many <entity id="J00-2004.10">words</entity> in each <entity id="J00-2004.11">text</entity> have no clear equivalent in the other <entity id="J00-2004.12">text</entity>. This article presents <entity id="J00-2004.13">methods</entity> for <entity id="J00-2004.14">biasing</entity> <entity id="J00-2004.15">statistical translation models</entity> to reflect these <entity id="J00-2004.16">properties</entity>. <entity id="J00-2004.17">Evaluation</entity> with <entity id="J00-2004.18">respect</entity> to independent human <entity id="J00-2004.19">judgments</entity> has confirmed that <entity id="J00-2004.20">translation models</entity> <entity id="J00-2004.21">biased</entity> in this <entity id="J00-2004.22">fashion</entity> are significantly more accurate than a baseline <entity id="J00-2004.23">knowledge-free</entity> <entity id="J00-2004.24">model</entity>. This article also shows how a <entity id="J00-2004.25">statistical translation model</entity> can take <entity id="J00-2004.26">advantage</entity> of preexisting <entity id="J00-2004.27">knowledge</entity> that might be available about particular <entity id="J00-2004.28">language pairs</entity>. Even the simplest <entity id="J00-2004.29">kinds</entity> of <entity id="J00-2004.30">language-specific</entity> <entity id="J00-2004.31">knowledge</entity>, such as the <entity id="J00-2004.32">distinction</entity> between <entity id="J00-2004.33">content words</entity> and <entity id="J00-2004.34">function words</entity>, are shown to reliably boost <entity id="J00-2004.35">translation model</entity> <entity id="J00-2004.36">performance</entity> on some <entity id="J00-2004.37">tasks</entity>. <entity id="J00-2004.38">Statistical models</entity> that reflect <entity id="J00-2004.39">knowledge</entity> about the <entity id="J00-2004.40">model</entity> <entity id="J00-2004.41">domain</entity> combine the best of both the rationalist and empiricist <entity id="J00-2004.42">paradigms</entity>.
</abstract>

</text>

<text id="J02-4007">
<title>Human Variation And Lexical Choice</title>
<abstract>
Much <entity id="J02-4007.1">natural language processing</entity> <entity id="J02-4007.2">research</entity> implicitly assumes that <entity id="J02-4007.3">word meanings</entity> are fixed in a <entity id="J02-4007.4">language</entity> <entity id="J02-4007.5">community</entity>, but in fact there is good <entity id="J02-4007.6">evidence</entity> that different people probably associate slightly different meanings with <entity id="J02-4007.7">words</entity>. We summarize some <entity id="J02-4007.8">evidence</entity> for this <entity id="J02-4007.9">claim</entity> from the <entity id="J02-4007.10">literature</entity> and from an ongoing <entity id="J02-4007.11">research project</entity>, and discuss its <entity id="J02-4007.12">implications</entity> for <entity id="J02-4007.13">natural language generation</entity>, especially for <entity id="J02-4007.14">lexical choice</entity>, that is, choosing appropriate <entity id="J02-4007.15">words</entity> for a <entity id="J02-4007.16">generated</entity> <entity id="J02-4007.17">text</entity>.
</abstract>

</text>

<text id="J04-2003">
<title>Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information</title>
<abstract>
In <entity id="J04-2003.1">statistical machine translation</entity>, <entity id="J04-2003.2">correspondences</entity> between the <entity id="J04-2003.3">words</entity> in the <entity id="J04-2003.4">source</entity> and the <entity id="J04-2003.5">target language</entity> are learned from <entity id="J04-2003.6">parallel corpora</entity>, and often little or no <entity id="J04-2003.7">linguistic knowledge</entity> is used to <entity id="J04-2003.8">structure</entity> the underlying <entity id="J04-2003.9">models</entity>. In particular, existing <entity id="J04-2003.10">statistical</entity> <entity id="J04-2003.11">systems</entity> for <entity id="J04-2003.12">machine translation</entity> often treat different inflected <entity id="J04-2003.13">forms</entity> of the same <entity id="J04-2003.14">lemma</entity> as if they were independent of one another. The bilingual <entity id="J04-2003.15">training</entity><entity id="J04-2003.16">data</entity> can be better exploited by explicitly taking into account the interdependencies of related inflected <entity id="J04-2003.17">forms</entity>. We <entity id="J04-2003.18">propose</entity> the <entity id="J04-2003.19">construction</entity> of hierarchical <entity id="J04-2003.20">lexicon</entity> <entity id="J04-2003.21">models</entity> on the <entity id="J04-2003.22">basis</entity> ofequivalence <entity id="J04-2003.23">classes</entity> ofwords. In <entity id="J04-2003.24">addition</entity>, we introduce <entity id="J04-2003.25">sentence-level</entity> restructuring <entity id="J04-2003.26">transformations</entity> which aim at the assimilation ofword <entity id="J04-2003.27">order</entity> in related <entity id="J04-2003.28">sentences</entity>. We have systematically investigated the <entity id="J04-2003.29">amount</entity> ofbilingual <entity id="J04-2003.30">training</entity><entity id="J04-2003.31">data</entity> <entity id="J04-2003.32">required</entity> to maintain an acceptable <entity id="J04-2003.33">quality</entity> ofmachine <entity id="J04-2003.34">translation</entity>. The <entity id="J04-2003.35">combination</entity> ofthe suggested <entity id="J04-2003.36">methods</entity> for <entity id="J04-2003.37">improving</entity> <entity id="J04-2003.38">translation quality</entity> in <entity id="J04-2003.39">frameworks</entity> with scarce <entity id="J04-2003.40">resources</entity> has been successfully <entity id="J04-2003.41">tested</entity>: We were able to reduce the <entity id="J04-2003.42">amount</entity> of bilingual <entity id="J04-2003.43">training</entity><entity id="J04-2003.44">data</entity> to less than 10% of the original <entity id="J04-2003.45">corpus</entity>, while losing only1.6% in <entity id="J04-2003.46">translation quality</entity>. The <entity id="J04-2003.47">improvement</entity> of the <entity id="J04-2003.48">translation results</entity> is demonstrated on two German-<entity id="J04-2003.49">English</entity> <entity id="J04-2003.50">corpora</entity> taken from the Verbmobil <entity id="J04-2003.51">task</entity> and the Nespole! <entity id="J04-2003.52">task</entity>.
</abstract>

</text>

<text id="J04-2005">
<title>"Comments On "Incremental Construction And Maintenance Of Minimal Finite-State Automata," By Rafael C. Carrasco And Mikel L. Forcada "</title>
<abstract>
"In a recent article, Carrasco and Forcada (June 2002 ) presented two <entity id="J04-2005.1">algorithms</entity>: one for incremental <entity id="J04-2005.2">addition</entity> of <entity id="J04-2005.3">strings</entity> to the <entity id="J04-2005.4">language</entity> of a minimal, deterministic, cyclic automaton, and one for incremental <entity id="J04-2005.5">removal</entity> of <entity id="J04-2005.6">strings</entity> from the automaton. The first <entity id="J04-2005.7">algorithm</entity> is a <entity id="J04-2005.8">generalization</entity> of the "<entity id="J04-2005.9">algorithm</entity> for unsorted data"
the second of the two incremental <entity id="J04-2005.10">algorithms</entity> for <entity id="J04-2005.11">construction</entity> of minimal, deterministic, acyclic automata presented in Daciuk (2000). We show that the other <entity id="J04-2005.12">algorithm</entity> in the older article the "<entity id="J04-2005.13">algorithm</entity> for sorted data" can be <entity id="J04-2005.14">generalized</entity> in a similar way. The new <entity id="J04-2005.15">algorithm</entity> is faster than the <entity id="J04-2005.16">algorithm</entity> for <entity id="J04-2005.17">addition</entity> ofstrings presented in Carrasco and Forcada 's article, as it handles each state only once."
</abstract>

</text>

<text id="W08-0106">
<title>Semantic negotiation in dialogue: the mechanisms of alignment</title>
<abstract>
A key <entity id="W08-0106.1">problem</entity> for <entity id="W08-0106.2">models</entity> of <entity id="W08-0106.3">dialogue</entity> is to explain how <entity id="W08-0106.4">semantic</entity> co-ordination in <entity id="W08-0106.5">dialogue</entity> is achieved and sustained. This <entity id="W08-0106.6">paper</entity> presents <entity id="W08-0106.7">findings</entity> from a <entity id="W08-0106.8">series</entity> of Maze <entity id="W08-0106.9">Task</entity> <entity id="W08-0106.10">experiments</entity> which are not readily explained by the primary co-ordination <entity id="W08-0106.11">mechanisms</entity> of existing <entity id="W08-0106.12">models</entity>. It demonstrates that <entity id="W08-0106.13">alignment</entity> in <entity id="W08-0106.14">dialogue</entity> is not simply an <entity id="W08-0106.15">outcome</entity> of successful <entity id="W08-0106.16">interaction</entity>, but a communicative <entity id="W08-0106.17">resource</entity> exploited by <entity id="W08-0106.18">interlocutors</entity> in converging on a <entity id="W08-0106.19">semantic</entity> <entity id="W08-0106.20">model</entity>. We argue this suggests <entity id="W08-0106.21">mechanisms</entity> of co-ordination in <entity id="W08-0106.22">dialogue</entity> which are of <entity id="W08-0106.23">relevance</entity> for a general account of how <entity id="W08-0106.24">semantic</entity> co-ordination is achieved.
</abstract>

</text>

<text id="P98-1061">
<title>A Structure-sharing Parser for Lexicalized Grammars</title>
<abstract>
In <entity id="P98-1061.1">wide-coverage</entity> lexicalized grammars many of the elementary <entity id="P98-1061.2">structures</entity> have <entity id="P98-1061.3">substructures</entity> in <entity id="P98-1061.4">common</entity>. This means that in conventional <entity id="P98-1061.5">parsing</entity> <entity id="P98-1061.6">algorithms</entity> some of the <entity id="P98-1061.7">computation</entity> associated with different <entity id="P98-1061.8">structures</entity> is duplicated. In this <entity id="P98-1061.9">paper</entity> we describe a precompilation <entity id="P98-1061.10">technique</entity> for such grammars which allows some of this <entity id="P98-1061.11">computation</entity> to be shared. In our <entity id="P98-1061.12">approach</entity> the elementary <entity id="P98-1061.13">structures</entity> of the grammar are transformed into finite state automata which can be merged and minimised using <entity id="P98-1061.14">standard</entity> <entity id="P98-1061.15">algorithms</entity>, and then parsed using an automaton-based <entity id="P98-1061.16">parser</entity>. We present <entity id="P98-1061.17">algorithms</entity> for <entity id="P98-1061.18">constructing</entity> automata from elementary <entity id="P98-1061.19">structures</entity>, merging and minimising them, and <entity id="P98-1061.20">string</entity> <entity id="P98-1061.21">recognition</entity> and <entity id="P98-1061.22">parse</entity> <entity id="P98-1061.23">recovery</entity> with the <entity id="P98-1061.24">resulting</entity> grammar.
</abstract>

</text>

<text id="P98-2236">
<title>Automatic Construction of Frame Representations for Spontaneous Speech in Unrestricted Domains</title>
<abstract>
This <entity id="P98-2236.1">paper</entity> presents a <entity id="P98-2236.2">system</entity> which automatically <entity id="P98-2236.3">generates</entity> shallow <entity id="P98-2236.4">semantic</entity> <entity id="P98-2236.5">frame</entity> <entity id="P98-2236.6">structures</entity> for conversational <entity id="P98-2236.7">speech</entity> in unrestricted <entity id="P98-2236.8">domains</entity>. We argue that such shallow <entity id="P98-2236.9">semantic representations</entity> can indeed be <entity id="P98-2236.10">generated</entity> with a <entity id="P98-2236.11">minimum</entity> <entity id="P98-2236.12">amount</entity> of <entity id="P98-2236.13">linguistic knowledge</entity> <entity id="P98-2236.14">engineering</entity> and without having to explicitly <entity id="P98-2236.15">construct</entity> a <entity id="P98-2236.16">semantic knowledge</entity> <entity id="P98-2236.17">base</entity>. The <entity id="P98-2236.18">system</entity> is <entity id="P98-2236.19">designed</entity> to be <entity id="P98-2236.20">robust</entity> to <entity id="P98-2236.21">deal</entity> with the <entity id="P98-2236.22">problems</entity> of <entity id="P98-2236.23">speech</entity> dysfluencies, ungrammaticalities, and imperfect <entity id="P98-2236.24">speech recognition</entity>. Initial <entity id="P98-2236.25">results</entity> on <entity id="P98-2236.26">speech</entity> <entity id="P98-2236.27">transcripts</entity> are promising in that correct <entity id="P98-2236.28">mappings</entity> could be identified in 21% of the <entity id="P98-2236.29">clauses</entity> of a <entity id="P98-2236.30">test set</entity> (resp. 44% of this <entity id="P98-2236.31">test set</entity> where ungrammatical or <entity id="P98-2236.32">verb-less</entity> <entity id="P98-2236.33">clauses</entity> were removed).
</abstract>

</text>

<text id="W08-1127">
<title>The GREC Challenge 2008 : Overview and Evaluation Results</title>
<abstract>
The grec <entity id="W08-1127.1">Task</entity> at reg'08 <entity id="W08-1127.2">required</entity> participating <entity id="W08-1127.3">systems</entity> to select coreference <entity id="W08-1127.4">chains</entity> to the <entity id="W08-1127.5">main</entity> subject of short encyclopaedic <entity id="W08-1127.6">texts</entity> collected from Wikipedia. Three teams submitted a total of 6 <entity id="W08-1127.7">systems</entity>, and we additionally created four <entity id="W08-1127.8">baseline systems</entity>. <entity id="W08-1127.9">Systems</entity> were <entity id="W08-1127.10">tested</entity> automatically using a range of existing intrinsic <entity id="W08-1127.11">metrics</entity>. We also <entity id="W08-1127.12">evaluated</entity> <entity id="W08-1127.13">systems</entity> extrinsically by <entity id="W08-1127.14">applying</entity> <entity id="W08-1127.15">coreference resolution</entity> <entity id="W08-1127.16">tools</entity> to the <entity id="W08-1127.17">outputs</entity> and measuring the <entity id="W08-1127.18">success</entity> of the <entity id="W08-1127.19">tools</entity>. In <entity id="W08-1127.20">addition</entity>, <entity id="W08-1127.21">systems</entity> were <entity id="W08-1127.22">tested</entity> in a reading/<entity id="W08-1127.23">comprehension</entity> <entity id="W08-1127.24">experiment</entity> involving human subjects. This <entity id="W08-1127.25">report</entity> describes the grec <entity id="W08-1127.26">Task</entity> and the <entity id="W08-1127.27">evaluation methods</entity>, gives brief <entity id="W08-1127.28">descriptions</entity> of the participating <entity id="W08-1127.29">systems</entity>, and presents the <entity id="W08-1127.30">evaluation results</entity>.
</abstract>

</text>

<text id="I08-1045">
<title>A Hybrid Feature Set based Maximum Entropy Hindi Named Entity Recognition</title>
<abstract>
We describe our <entity id="I08-1045.1">effort</entity> in <entity id="I08-1045.2">developing</entity> a <entity id="I08-1045.3">Named</entity> <entity id="I08-1045.4">Entity</entity> <entity id="I08-1045.5">Recognition</entity> (NER) <entity id="I08-1045.6">system</entity> for Hindi using <entity id="I08-1045.7">Maximum Entropy</entity> (Max-Ent) <entity id="I08-1045.8">approach</entity>. We <entity id="I08-1045.9">developed</entity> a NER annotated <entity id="I08-1045.10">corpora</entity> for the <entity id="I08-1045.11">purpose</entity>. We have tried to identify the most relevant <entity id="I08-1045.12">features</entity> for Hindi NER <entity id="I08-1045.13">task</entity> to enable us to <entity id="I08-1045.14">develop</entity> an efficient NER from the limited <entity id="I08-1045.15">corpora</entity> <entity id="I08-1045.16">developed</entity>. Apart from the orthographic and <entity id="I08-1045.17">collocation</entity> <entity id="I08-1045.18">features</entity>, we have <entity id="I08-1045.19">experimented</entity> on the <entity id="I08-1045.20">efficiency</entity> of using <entity id="I08-1045.21">gazetteer</entity> <entity id="I08-1045.22">lists</entity> as <entity id="I08-1045.23">features</entity>. We also worked on <entity id="I08-1045.24">semi-automatic</entity> <entity id="I08-1045.25">induction</entity> of <entity id="I08-1045.26">context</entity> <entity id="I08-1045.27">patterns</entity> and <entity id="I08-1045.28">experimented</entity> with using these as <entity id="I08-1045.29">features</entity> of the MaxEnt <entity id="I08-1045.30">method</entity>. We have <entity id="I08-1045.31">evaluated</entity> the <entity id="I08-1045.32">performance</entity> of the <entity id="I08-1045.33">system</entity> against a blind <entity id="I08-1045.34">test set</entity> having 4 <entity id="I08-1045.35">classes</entity> - Person, <entity id="I08-1045.36">Organization</entity>, <entity id="I08-1045.37">Location</entity> and <entity id="I08-1045.38">Date</entity>. Our <entity id="I08-1045.39">system</entity> achieved a f-value of 81.52%.
</abstract>

</text>

<text id="W04-0841">
<title>SVM Classification Of FrameNet Semantic Roles</title>
<abstract>
A <entity id="W04-0841.1">Support</entity> <entity id="W04-0841.2">Vector</entity> Machines (SVM) <entity id="W04-0841.3">classifier</entity> of FrameNet <entity id="W04-0841.4">semantic roles</entity> was <entity id="W04-0841.5">implemented</entity> <entity id="W04-0841.6">based</entity> on a set of new and previously used <entity id="W04-0841.7">syntactic</entity> and <entity id="W04-0841.8">semantic features</entity>. At Senseval 3, the <entity id="W04-0841.9">system</entity> achieved a <entity id="W04-0841.10">precision</entity> of 0.807 for the restricted <entity id="W04-0841.11">test</entity> and 0.898 for the non-restricted <entity id="W04-0841.12">test</entity>.
</abstract>

</text>

<text id="W04-2316">
<title>Acknowledgment Use With Synthesized And Recorded Prompts</title>
<abstract>
"Acknowledgments, e.g., "yeah" and "uh-huh," are ubiquitous in human <entity id="W04-2316.1">conversation</entity> but are rarer in <entity id="W04-2316.2">human-computer</entity> <entity id="W04-2316.3">interaction</entity>. What <entity id="W04-2316.4">interface</entity> <entity id="W04-2316.5">factors</entity> might contribute to this <entity id="W04-2316.6">difference</entity>? Using a <entity id="W04-2316.7">simple</entity> <entity id="W04-2316.8">spoken-language</entity> <entity id="W04-2316.9">interface</entity> that responded to acknowledgments, we compared subjects' use of acknowledgments when the <entity id="W04-2316.10">interface</entity> used <entity id="W04-2316.11">recorded</entity> <entity id="W04-2316.12">speech</entity> with that seen when the <entity id="W04-2316.13">interface</entity> used synthesized <entity id="W04-2316.14">speech</entity>. Contrary to our <entity id="W04-2316.15">hypothesis</entity>, we saw a drop in the <entity id="W04-2316.16">numbers</entity> of subjects using acknowledgments: subjects appeared to interpret the recorded-voice <entity id="W04-2316.17">interface</entity> as signalling a more limited <entity id="W04-2316.18">interface</entity>. These <entity id="W04-2316.19">results</entity> were consistent for both Mexican Spanish and American <entity id="W04-2316.20">English</entity> <entity id="W04-2316.21">versions</entity> of the <entity id="W04-2316.22">interface</entity>. "
</abstract>

</text>

<text id="W04-3010">
<title>Speech Recognition Models Of The Interdependence Among Syntax, Prosody, And Segmental Acoustics</title>
<abstract>
This <entity id="W04-3010.1">paper</entity> describes <entity id="W04-3010.2">results</entity> from several dozen <entity id="W04-3010.3">experimental</entity> <entity id="W04-3010.4">systems</entity>, and draws <entity id="W04-3010.5">conclusions</entity> about the <entity id="W04-3010.6">ability</entity> of <entity id="W04-3010.7">speech recognition</entity> <entity id="W04-3010.8">models</entity> to represent the <entity id="W04-3010.9">relationship</entity> among <entity id="W04-3010.10">syntax</entity>, <entity id="W04-3010.11">prosody</entity>, and segmental acoustics. <entity id="W04-3010.12">Prosody-dependent</entity> allophone <entity id="W04-3010.13">modeling</entity> can reduce the <entity id="W04-3010.14">word error rate</entity> (WER) of a <entity id="W04-3010.15">speech</entity> recognizer, but only if both the <entity id="W04-3010.16">language model</entity> and the <entity id="W04-3010.17">acoustic model</entity> encode explicit <entity id="W04-3010.18">dependence</entity> on <entity id="W04-3010.19">prosody</entity>. <entity id="W04-3010.20">Word error rate</entity> is <entity id="W04-3010.21">improved</entity> mainly because the observed <entity id="W04-3010.22">prosody</entity> is linguistically unlikely to co-occur with any incorrect <entity id="W04-3010.23">word</entity> <entity id="W04-3010.24">string</entity>. Additional <entity id="W04-3010.25">improvements</entity>, in both <entity id="W04-3010.26">perplexity</entity> and WER, can be obtained using a semi-factored <entity id="W04-3010.27">language model</entity>, in which the <entity id="W04-3010.28">relationship</entity> between <entity id="W04-3010.29">prosody</entity> and the <entity id="W04-3010.30">word</entity> <entity id="W04-3010.31">sequence</entity> is at least partly mediated by <entity id="W04-3010.32">syntactic</entity> <entity id="W04-3010.33">tags</entity>. Careful <entity id="W04-3010.34">analysis</entity> of the <entity id="W04-3010.35">relationship</entity> between <entity id="W04-3010.36">prosody</entity> and <entity id="W04-3010.37">syntax</entity> indicates that <entity id="W04-3010.38">syntactic</entity> <entity id="W04-3010.39">phrase</entity> <entity id="W04-3010.40">boundaries</entity> are the most important <entity id="W04-3010.41">cue</entity> for prosodic <entity id="W04-3010.42">phrase</entity> <entity id="W04-3010.43">boundary</entity> <entity id="W04-3010.44">recognition</entity>, while <entity id="W04-3010.45">part of speech</entity> is the most important <entity id="W04-3010.46">cue</entity> for locating <entity id="W04-3010.47">pitch</entity> accents, but that neither of these <entity id="W04-3010.48">cues</entity> is entirely sufficient for either <entity id="W04-3010.49">classification task</entity>. <entity id="W04-3010.50">Experiments</entity> to <entity id="W04-3010.51">port</entity> this <entity id="W04-3010.52">system</entity> from Radio <entity id="W04-3010.53">News</entity> to the Switchboard <entity id="W04-3010.54">corpus</entity> are currently under way, but preliminary <entity id="W04-3010.55">results</entity> suggest that the <entity id="W04-3010.56">prosody</entity> of Switchboard is profoundly different from the <entity id="W04-3010.57">prosody</entity> of Radio <entity id="W04-3010.58">News</entity>.
</abstract>

</text>

<text id="W05-0803">
<title>Parsing Word-Aligned Parallel Corpora In A Grammar Induction Context</title>
<abstract>
We present an Earley-style <entity id="W05-0803.1">dynamic programming algorithm</entity> for <entity id="W05-0803.2">parsing</entity> <entity id="W05-0803.3">sentence</entity> <entity id="W05-0803.4">pairs</entity> from a <entity id="W05-0803.5">parallel corpus</entity> simultaneously, <entity id="W05-0803.6">building</entity> up two <entity id="W05-0803.7">phrase structure trees</entity> and a <entity id="W05-0803.8">correspondence</entity> <entity id="W05-0803.9">mapping</entity> between the <entity id="W05-0803.10">nodes</entity>. The intended use of the <entity id="W05-0803.11">algorithm</entity> is in <entity id="W05-0803.12">bootstrapping</entity> grammars for less <entity id="W05-0803.13">studied</entity> <entity id="W05-0803.14">languages</entity> by using implicit grammatical <entity id="W05-0803.15">information</entity> in <entity id="W05-0803.16">parallel corpora</entity>. Therefore, we presuppose a given (<entity id="W05-0803.17">statistical</entity>) <entity id="W05-0803.18">word alignment</entity> underlying in the synchronous <entity id="W05-0803.19">parsing</entity> <entity id="W05-0803.20">task</entity>; this leads to a significant <entity id="W05-0803.21">reduction</entity> of the <entity id="W05-0803.22">parsing</entity> <entity id="W05-0803.23">complexity</entity>. The theoretical <entity id="W05-0803.24">complexity</entity> <entity id="W05-0803.25">results</entity> are corroborated by a quantitative <entity id="W05-0803.26">evaluation</entity> in which we ran an <entity id="W05-0803.27">implementation</entity> of the <entity id="W05-0803.28">algorithm</entity> on a <entity id="W05-0803.29">suite</entity> of <entity id="W05-0803.30">test</entity> <entity id="W05-0803.31">sentences</entity> from the Europarl <entity id="W05-0803.32">parallel corpus</entity>.
</abstract>

</text>

<text id="W05-0831">
<title>Novel Reordering Approaches In Phrase-Based Statistical Machine Translation</title>
<abstract>
This <entity id="W05-0831.1">paper</entity> presents novel <entity id="W05-0831.2">approaches</entity> to reordering in <entity id="W05-0831.3">phrase-based</entity> <entity id="W05-0831.4">statistical machine translation</entity>. We <entity id="W05-0831.5">perform</entity> consistent reordering of <entity id="W05-0831.6">source sentences</entity> in <entity id="W05-0831.7">training</entity> and estimate a <entity id="W05-0831.8">statistical translation model</entity>. Using this <entity id="W05-0831.9">model</entity>, we follow a <entity id="W05-0831.10">phrase-based</entity> monotonic <entity id="W05-0831.11">machine translation</entity> <entity id="W05-0831.12">approach</entity>, for which we <entity id="W05-0831.13">develop</entity> an efficient and flexible reordering <entity id="W05-0831.14">framework</entity> that allows to easily introduce different reordering <entity id="W05-0831.15">constraints</entity>. In <entity id="W05-0831.16">translation</entity>, we <entity id="W05-0831.17">apply</entity> <entity id="W05-0831.18">source sentence</entity> reordering on <entity id="W05-0831.19">word</entity> <entity id="W05-0831.20">level</entity> and use a reordering automaton as <entity id="W05-0831.21">input</entity>. We show how to <entity id="W05-0831.22">compute</entity> reordering automata on-demand using IBM or ITG <entity id="W05-0831.23">constraints</entity>, and also introduce two new <entity id="W05-0831.24">types</entity> ofreordering <entity id="W05-0831.25">constraints</entity>. We further add <entity id="W05-0831.26">weights</entity> to the reordering automata. We present detailed <entity id="W05-0831.27">experimental</entity> <entity id="W05-0831.28">results</entity> and show that reordering significantly <entity id="W05-0831.29">improves</entity> <entity id="W05-0831.30">translation quality</entity>.
</abstract>

</text>

<text id="W06-1520">
<title>Handling Unlike Coordinated Phrases In TAG By Mixing Syntactic Category And Grammatical Function</title>
<abstract><entity id="W06-1520.1">Coordination</entity> of <entity id="W06-1520.2">phrases</entity> of different <entity id="W06-1520.3">syntactic categories</entity> has posed a <entity id="W06-1520.4">problem</entity> for generative <entity id="W06-1520.5">systems</entity> <entity id="W06-1520.6">based</entity> only on <entity id="W06-1520.7">syntactic categories</entity>. Although some prefer to treat them as exceptional <entity id="W06-1520.8">cases</entity> that should <entity id="W06-1520.9">require</entity> some extra <entity id="W06-1520.10">mechanism</entity> (as for elliptical <entity id="W06-1520.11">constructions</entity>), or to allow for unrestricted <entity id="W06-1520.12">cross-category</entity> <entity id="W06-1520.13">coordination</entity>, they can be naturally derived in a grammatic functional generative <entity id="W06-1520.14">approach</entity>. In this <entity id="W06-1520.15">paper</entity> we explore the ideia on how mixing <entity id="W06-1520.16">syntactic categories</entity> and <entity id="W06-1520.17">grammatical functions</entity> in the label set of a <entity id="W06-1520.18">Tree Adjoining Grammar</entity> allows us to <entity id="W06-1520.19">develop</entity> grammars that elegantly handle both the <entity id="W06-1520.20">cases</entity> of same- and <entity id="W06-1520.21">cross-category</entity> <entity id="W06-1520.22">coordination</entity> in an uniform way.
</abstract>

</text>

<text id="W03-1902">
<title>From Concrete To Virtual Annotation Mark-Up Language: The Case Of COMMOn-REFs</title>
<abstract>
This work presents the<entity id="W03-1902.1">data</entity> <entity id="W03-1902.2">model</entity> we adopted for annotating coreference. Our<entity id="W03-1902.3">data</entity> <entity id="W03-1902.4">model</entity> <entity id="W03-1902.5">includes</entity> different <entity id="W03-1902.6">levels</entity> of annotation, such as <entity id="W03-1902.7">part-of-speech</entity>, <entity id="W03-1902.8">syntax</entity> and <entity id="W03-1902.9">discourse</entity>. We compare our encoding <entity id="W03-1902.10">schemes</entity> to the <entity id="W03-1902.11">abstract</entity> XML encoding being <entity id="W03-1902.12">proposed</entity> as <entity id="W03-1902.13">standard</entity>. We also present our <entity id="W03-1902.14">tool</entity> for <entity id="W03-1902.15">coreference resolution</entity> that handles our<entity id="W03-1902.16">data</entity> <entity id="W03-1902.17">model</entity>.
</abstract>

</text>

<text id="W06-3112">
<title>Contextual Bitext-Derived Paraphrases In Automatic MT Evaluation</title>
<abstract>
In this <entity id="W06-3112.1">paper</entity> we present a novel <entity id="W06-3112.2">method</entity> for deriving paraphrases during <entity id="W06-3112.3">automatic</entity> <entity id="W06-3112.4">MT evaluation</entity> using only the <entity id="W06-3112.5">source</entity> and <entity id="W06-3112.6">reference</entity> <entity id="W06-3112.7">texts</entity>, which are necessary for the <entity id="W06-3112.8">evaluation</entity>, and <entity id="W06-3112.9">word</entity> and <entity id="W06-3112.10">phrase</entity> <entity id="W06-3112.11">alignment</entity> <entity id="W06-3112.12">software</entity>. Using <entity id="W06-3112.13">target language</entity> paraphrases produced through <entity id="W06-3112.14">word</entity> and <entity id="W06-3112.15">phrase</entity> <entity id="W06-3112.16">alignment</entity> a <entity id="W06-3112.17">number</entity> of <entity id="W06-3112.18">alternative</entity> <entity id="W06-3112.19">reference</entity> <entity id="W06-3112.20">sentences</entity> are <entity id="W06-3112.21">constructed</entity> automatically for each <entity id="W06-3112.22">candidate</entity> <entity id="W06-3112.23">translation</entity>. The <entity id="W06-3112.24">method</entity> produces <entity id="W06-3112.25">lexical</entity> and <entity id="W06-3112.26">low-level</entity> <entity id="W06-3112.27">syntactic</entity> paraphrases that are relevant to the <entity id="W06-3112.28">domain</entity> in <entity id="W06-3112.29">hand</entity>, does not use external <entity id="W06-3112.30">knowledge</entity> <entity id="W06-3112.31">resources</entity>, and can be combined with a <entity id="W06-3112.32">variety</entity> of <entity id="W06-3112.33">automatic</entity> <entity id="W06-3112.34">MT evaluation</entity> <entity id="W06-3112.35">system</entity>.
</abstract>

</text>

<text id="W06-3502">
<title>Backbone Extraction And Pruning For Speeding Up A Deep Parser For Dialogue Systems</title>
<abstract>
In this <entity id="W06-3502.1">paper</entity> we discuss <entity id="W06-3502.2">issues</entity> related to <entity id="W06-3502.3">speeding</entity> up <entity id="W06-3502.4">parsing</entity> with <entity id="W06-3502.5">wide-coverage</entity> <entity id="W06-3502.6">unification</entity> grammars. We demonstrate that state-of-the-art optimisation <entity id="W06-3502.7">techniques</entity> <entity id="W06-3502.8">based</entity> on <entity id="W06-3502.9">backbone</entity> <entity id="W06-3502.10">parsing</entity> before <entity id="W06-3502.11">unification</entity> do not <entity id="W06-3502.12">provide</entity> a general <entity id="W06-3502.13">solution</entity>, because they depend on specific <entity id="W06-3502.14">properties</entity> of the grammar <entity id="W06-3502.15">formalism</entity> that do not hold for all <entity id="W06-3502.16">unification</entity> <entity id="W06-3502.17">based</entity> grammars. As an <entity id="W06-3502.18">alternative</entity>, we describe an optimisation <entity id="W06-3502.19">technique</entity> that combines <entity id="W06-3502.20">ambiguity</entity> packing at the <entity id="W06-3502.21">constituent structure</entity> <entity id="W06-3502.22">level</entity> with pruning <entity id="W06-3502.23">based</entity> on local <entity id="W06-3502.24">features</entity>.
</abstract>

</text>

<text id="W04-3212">
<title>Calibrating Features For Semantic Role Labeling</title>
<abstract>
This <entity id="W04-3212.1">paper</entity> takes a critical look at the <entity id="W04-3212.2">features</entity> used in the <entity id="W04-3212.3">semantic role</entity> <entity id="W04-3212.4">tagging</entity> <entity id="W04-3212.5">literature</entity> and show that the <entity id="W04-3212.6">information</entity> in the <entity id="W04-3212.7">input</entity>, generally a <entity id="W04-3212.8">syntactic</entity> <entity id="W04-3212.9">parse tree</entity>, has yet to be fully exploited. We <entity id="W04-3212.10">propose</entity> an additional set of <entity id="W04-3212.11">features</entity> and our <entity id="W04-3212.12">experiments</entity> show that these <entity id="W04-3212.13">features</entity> lead to fairly significant <entity id="W04-3212.14">improvements</entity> in the <entity id="W04-3212.15">tasks</entity> we <entity id="W04-3212.16">performed</entity>. We further show that different <entity id="W04-3212.17">features</entity> are needed for different sub-tasks. Finally, we show that by using a <entity id="W04-3212.18">Maximum Entropy</entity> <entity id="W04-3212.19">classifier</entity> and fewer <entity id="W04-3212.20">features</entity>, we achieved <entity id="W04-3212.21">results</entity> comparable with the best previously <entity id="W04-3212.22">reported</entity> <entity id="W04-3212.23">results</entity> obtained with SVM <entity id="W04-3212.24">models</entity>. We believe this is a clear indication that <entity id="W04-3212.25">developing</entity> <entity id="W04-3212.26">features</entity> that capture the right <entity id="W04-3212.27">kind</entity> of <entity id="W04-3212.28">information</entity> is crucial to <entity id="W04-3212.29">advancing</entity> the state-of-the-art in <entity id="W04-3212.30">semantic analysis</entity>.
</abstract>

</text>

<text id="W04-3241">
<title>The Entropy Rate Principle As A Predictor Of Processing Effort: An Evaluation Against Eye-Tracking Data</title>
<abstract>
This <entity id="W04-3241.1">paper</entity> <entity id="W04-3241.2">provides</entity> <entity id="W04-3241.3">evidence</entity> for Genzel and Char-niak's (2002) <entity id="W04-3241.4">entropy</entity> <entity id="W04-3241.5">rate</entity> <entity id="W04-3241.6">principle</entity>, which predicts that the <entity id="W04-3241.7">entropy</entity> of a <entity id="W04-3241.8">sentence</entity> <entity id="W04-3241.9">increases</entity> with its position in the <entity id="W04-3241.10">text</entity>. We show that this <entity id="W04-3241.11">principle</entity> holds for <entity id="W04-3241.12">individual</entity> <entity id="W04-3241.13">sentences</entity> (not just for averages), but we also find that the <entity id="W04-3241.14">entropy</entity> <entity id="W04-3241.15">rate</entity> <entity id="W04-3241.16">effect</entity> is partly an artifact of <entity id="W04-3241.17">sentence</entity> <entity id="W04-3241.18">length</entity>, which also correlates with <entity id="W04-3241.19">sentence</entity> position. Secondly, we <entity id="W04-3241.20">evaluate</entity> a set of <entity id="W04-3241.21">predictions</entity> that the <entity id="W04-3241.22">entropy</entity> <entity id="W04-3241.23">rate</entity> <entity id="W04-3241.24">principle</entity> makes for <entity id="W04-3241.25">human language</entity> <entity id="W04-3241.26">processing</entity>; using a <entity id="W04-3241.27">corpus</entity> of eye-tracking data, we show that <entity id="W04-3241.28">entropy</entity> and <entity id="W04-3241.29">processing</entity> <entity id="W04-3241.30">effort</entity> are correlated, and that <entity id="W04-3241.31">processing</entity> <entity id="W04-3241.32">effort</entity> is constant throughout a <entity id="W04-3241.33">text</entity>.
</abstract>

</text>

<text id="M98-1028">
<title>MUC-7 Named Entity Task Definition (Version 3.5)</title>
<abstract>
"The <entity id="M98-1028.1">Named</entity> <entity id="M98-1028.2">Entity</entity> <entity id="M98-1028.3">task</entity> consists of three subtasks (<entity id="M98-1028.4">entity</entity> <entity id="M98-1028.5">names</entity>, temporal <entity id="M98-1028.6">expressions</entity>, <entity id="M98-1028.7">number</entity> <entity id="M98-1028.8">expressions</entity>). The <entity id="M98-1028.9">expressions</entity> to be annotated are "unique <entity id="M98-1028.10">identifiers</entity>" of <entity id="M98-1028.11">entities</entity> (<entity id="M98-1028.12">organizations</entity>, persons, <entity id="M98-1028.13">locations</entity>), <entity id="M98-1028.14">times</entity> (<entity id="M98-1028.15">dates</entity>, <entity id="M98-1028.16">times</entity>), and <entity id="M98-1028.17">quantities</entity> (monetary values, <entity id="M98-1028.18">percentages</entity>). For many <entity id="M98-1028.19">text processing</entity> <entity id="M98-1028.20">systems</entity>, such <entity id="M98-1028.21">identifiers</entity> are recognized primarily using local <entity id="M98-1028.22">pattern-matching</entity> <entity id="M98-1028.23">techniques</entity>. The TEI (<entity id="M98-1028.24">Text</entity> Encoding <entity id="M98-1028.25">Initiative</entity>) <entity id="M98-1028.26">Guidelines</entity> for Electronic <entity id="M98-1028.27">Text</entity> Encoding and Interchange cover such <entity id="M98-1028.28">identifiers</entity> (plus <entity id="M98-1028.29">abbreviations</entity>) together in <entity id="M98-1028.30">section</entity> 6.4 and explain that the <entity id="M98-1028.31">identifiers</entity> comprise "textual <entity id="M98-1028.32">features</entity> which it is often convenient to distinguish from their surrounding <entity id="M98-1028.33">text</entity>. <entity id="M98-1028.34">Names</entity>, <entity id="M98-1028.35">dates</entity> and <entity id="M98-1028.36">numbers</entity> are likely to be of particular <entity id="M98-1028.37">importance</entity> to the scholar treating a <entity id="M98-1028.38">text</entity> as <entity id="M98-1028.39">source</entity> for a <entity id="M98-1028.40">database</entity>; distinguishing such <entity id="M98-1028.41">items</entity> from the surrounding <entity id="M98-1028.42">text</entity> is however equally important to the scholar primarily interested in lexis." The <entity id="M98-1028.43">task</entity> is to identify all <entity id="M98-1028.44">instances</entity> of the three <entity id="M98-1028.45">types</entity> of <entity id="M98-1028.46">expressions</entity> in each <entity id="M98-1028.47">text</entity> in the <entity id="M98-1028.48">test set</entity> and to subcategorize the <entity id="M98-1028.49">expressions</entity>. The original <entity id="M98-1028.50">texts</entity> contain some SGML <entity id="M98-1028.51">tags</entity> already; the <entity id="M98-1028.52">Named</entity> <entity id="M98-1028.53">Entity</entity> <entity id="M98-1028.54">task</entity> is to be <entity id="M98-1028.55">performed</entity> within the <entity id="M98-1028.56">text</entity> delimited by the SLUG, <entity id="M98-1028.57">DATE</entity>, NWORDS, PREAMBLE, <entity id="M98-1028.58">TEXT</entity>, and TRAILER <entity id="M98-1028.59">tags</entity>. The <entity id="M98-1028.60">system</entity> must produce a single, unambiguous <entity id="M98-1028.61">output</entity> for any relevant <entity id="M98-1028.62">string</entity> in the <entity id="M98-1028.63">text</entity>; thus, this <entity id="M98-1028.64">evaluation</entity> is not <entity id="M98-1028.65">based</entity> on a view of a pipelined <entity id="M98-1028.66">system architecture</entity> in which <entity id="M98-1028.67">Named</entity> <entity id="M98-1028.68">Entity</entity> <entity id="M98-1028.69">recognition</entity> would be completely handled as a preprocess to <entity id="M98-1028.70">sentence</entity> and <entity id="M98-1028.71">discourse analysis</entity>. The <entity id="M98-1028.72">task</entity> <entity id="M98-1028.73">requires</entity> that the <entity id="M98-1028.74">system</entity> recognize what a <entity id="M98-1028.75">string</entity> represents, not just its superficial appearance. Sometimes, the right answer is superficially apparent, as in the <entity id="M98-1028.76">case</entity> of most, if not all, NUMEX <entity id="M98-1028.77">expressions</entity>, and can be obtained by local <entity id="M98-1028.78">pattern-matching</entity> <entity id="M98-1028.79">techniques</entity>. In other <entity id="M98-1028.80">cases</entity>, the right answer is not superficially apparent, as when a single capitalized <entity id="M98-1028.81">word</entity> could represent the <entity id="M98-1028.82">name</entity> of a <entity id="M98-1028.83">location</entity>, person, or <entity id="M98-1028.84">organization</entity>, and the answer may have to be obtained using <entity id="M98-1028.85">techniques</entity> that draw <entity id="M98-1028.86">information</entity> from a larger <entity id="M98-1028.87">context</entity> or from <entity id="M98-1028.88">reference</entity> <entity id="M98-1028.89">lists</entity>. The three subtasks correspond to three SGML <entity id="M98-1028.90">tag</entity> elements: ENAMEX, TIMEX, and NUMEX. The subcategorization is captured by a SGML <entity id="M98-1028.91">tag</entity> attribute <entity id="M98-1028.92">called</entity> <entity id="M98-1028.93">TYPE</entity>, which is defined to have a different set of possible values for each <entity id="M98-1028.94">tag</entity> element. The <entity id="M98-1028.95">markup</entity> is described in <entity id="M98-1028.96">section</entity> 2, below. "
</abstract>

</text>

<text id="W07-0209">
<title>Semi-supervised Algorithm for Human-Computer Dialogue Mining</title>
<abstract>
This <entity id="W07-0209.1">paper</entity> describes the <entity id="W07-0209.2">analysis</entity> of weak local <entity id="W07-0209.3">coherence</entity> <entity id="W07-0209.4">utterances</entity> during <entity id="W07-0209.5">human-computer</entity> <entity id="W07-0209.6">conversation</entity> through the <entity id="W07-0209.7">application</entity> of an emergent<entity id="W07-0209.8">data</entity> mining <entity id="W07-0209.9">technique</entity>,<entity id="W07-0209.10">data</entity> crystallization.
</abstract>

</text>

<text id="W07-0802">
<title>Implementation of the Arabic Numerals and their Syntax in GF</title>
<abstract>
The numeral <entity id="W07-0802.1">system</entity> of Arabic is rich in its morphosyntactic <entity id="W07-0802.2">variety</entity> yet suffers from the <entity id="W07-0802.3">lack</entity> of a good <entity id="W07-0802.4">computational</entity> <entity id="W07-0802.5">resource</entity> that describes it in a reusable way. This implies that <entity id="W07-0802.6">applications</entity> that <entity id="W07-0802.7">require</entity> the use of <entity id="W07-0802.8">rules</entity> of the Arabic numeral <entity id="W07-0802.9">system</entity> have to either reimplement them each <entity id="W07-0802.10">time</entity>, which implies wasted <entity id="W07-0802.11">resources</entity>, or use simplified, imprecise <entity id="W07-0802.12">rules</entity> that <entity id="W07-0802.13">result</entity> in low <entity id="W07-0802.14">quality</entity> <entity id="W07-0802.15">applications</entity>. A <entity id="W07-0802.16">solution</entity> has been devised with Grammatical <entity id="W07-0802.17">Framework</entity> (GF) to use <entity id="W07-0802.18">language</entity> <entity id="W07-0802.19">constructs</entity> and grammars as <entity id="W07-0802.20">libraries</entity> that can be written once and reused in various <entity id="W07-0802.21">applications</entity>. In this <entity id="W07-0802.22">paper</entity>, we describe our <entity id="W07-0802.23">implementation</entity> of the Arabic numeral <entity id="W07-0802.24">system</entity>, as an <entity id="W07-0802.25">example</entity> of a bigger <entity id="W07-0802.26">implementation</entity> of a grammar <entity id="W07-0802.27">library</entity> for Arabic. We show that <entity id="W07-0802.28">users</entity> can reuse our <entity id="W07-0802.29">system</entity> by <entity id="W07-0802.30">accessing</entity> a <entity id="W07-0802.31">simple</entity> <entity id="W07-0802.32">language-independent</entity> API <entity id="W07-0802.33">rule</entity>.
</abstract>

</text>

<text id="W07-1219">
<title>Local Ambiguity Packing and Discontinuity in German</title>
<abstract>
We <entity id="W07-1219.1">report</entity> on recent <entity id="W07-1219.2">advances</entity> in HPSG <entity id="W07-1219.3">parsing</entity> of German with local <entity id="W07-1219.4">ambiguity</entity> packing ( Oepen and Carroll, 2000 ), achieving a <entity id="W07-1219.5">speed-up</entity> <entity id="W07-1219.6">factor</entity> of 2 on a balanced <entity id="W07-1219.7">test-suite</entity>. In <entity id="W07-1219.8">contrast</entity> to earlier <entity id="W07-1219.9">studies</entity> carried out for <entity id="W07-1219.10">English</entity> using the same packing <entity id="W07-1219.11">algorithm</entity>, we show that restricting <entity id="W07-1219.12">semantic features</entity> only is insufficient for achieving acceptable runtime <entity id="W07-1219.13">performance</entity> with a German HPSG grammar. In a <entity id="W07-1219.14">series</entity> of <entity id="W07-1219.15">experiments</entity> relating to the three different <entity id="W07-1219.16">types</entity> of discontinuities in German (head movement, <entity id="W07-1219.17">extraction</entity>, extraposition), we examine the <entity id="W07-1219.18">effects</entity> of re-strictor <entity id="W07-1219.19">choice</entity>, ultimately showing that <entity id="W07-1219.20">extraction</entity> and head movement <entity id="W07-1219.21">require</entity> <entity id="W07-1219.22">partial</entity> <entity id="W07-1219.23">restriction</entity> of the respective <entity id="W07-1219.24">features</entity> encoding the <entity id="W07-1219.25">dependency</entity>, whereas full <entity id="W07-1219.26">restriction</entity> gives best <entity id="W07-1219.27">results</entity> for extraposition.
</abstract>

</text>

<text id="W07-1506">
<title>Discontinuity Revisited: An Improved Conversion to Context-Free Representations</title>
<abstract>
This <entity id="W07-1506.1">paper</entity> introduces a new, reversible <entity id="W07-1506.2">method</entity> for converting <entity id="W07-1506.3">syntactic structures</entity> with discontinuous <entity id="W07-1506.4">constituents</entity> into traditional <entity id="W07-1506.5">syntax</entity> <entity id="W07-1506.6">trees</entity>. The <entity id="W07-1506.7">method</entity> is <entity id="W07-1506.8">applied</entity> to the <entity id="W07-1506.9">Tiger</entity> <entity id="W07-1506.10">Corpus</entity> of German and <entity id="W07-1506.11">results</entity> for PCFG <entity id="W07-1506.12">parsing</entity> <entity id="W07-1506.13">requiring</entity> such <entity id="W07-1506.14">context-free</entity> <entity id="W07-1506.15">trees</entity> are <entity id="W07-1506.16">provided</entity>. A labeled <entity id="W07-1506.17">dependency</entity> <entity id="W07-1506.18">evaluation</entity> shows that the new <entity id="W07-1506.19">conversion</entity> <entity id="W07-1506.20">method</entity> leads to better <entity id="W07-1506.21">results</entity> by preserving local <entity id="W07-1506.22">relationships</entity> and introducing fewer <entity id="W07-1506.23">inconsistencies</entity> into the <entity id="W07-1506.24">training</entity><entity id="W07-1506.25">data</entity>.
</abstract>

</text>

<text id="W07-1528">
<title>IGT-XML: An XML Format for Interlinearized Glossed Text</title>
<abstract>
We <entity id="W07-1528.1">propose</entity> a new XML <entity id="W07-1528.2">format</entity> for representing interlinearized glossed <entity id="W07-1528.3">text</entity> (IGT), particularly in the <entity id="W07-1528.4">context</entity> of the <entity id="W07-1528.5">documentation</entity> and <entity id="W07-1528.6">description</entity> of endangered <entity id="W07-1528.7">languages</entity>. The <entity id="W07-1528.8">proposed</entity> <entity id="W07-1528.9">representation</entity>, which we <entity id="W07-1528.10">call</entity> IGT-XML, builds on previous <entity id="W07-1528.11">models</entity> but <entity id="W07-1528.12">provides</entity> a more loosely coupled and flexible <entity id="W07-1528.13">representation</entity> of different annotation <entity id="W07-1528.14">layers</entity>. Designed to accommodate both selective <entity id="W07-1528.15">manual</entity> reannotation of <entity id="W07-1528.16">individual</entity> <entity id="W07-1528.17">layers</entity> and <entity id="W07-1528.18">semi-automatic</entity> <entity id="W07-1528.19">extension</entity> of annotation, IGT-XML is a <entity id="W07-1528.20">first step</entity> toward <entity id="W07-1528.21">partial</entity> <entity id="W07-1528.22">automation</entity> of the production of IGT.
</abstract>

</text>

<text id="D07-1066">
<title>Treebank Annotation Schemes and Parser Evaluation for German</title>
<abstract>
Recent <entity id="D07-1066.1">studies</entity> focussed on the <entity id="D07-1066.2">question</entity> whether less-configurational <entity id="D07-1066.3">languages</entity> like German are harder to <entity id="D07-1066.4">parse</entity> than <entity id="D07-1066.5">English</entity>, or whether the lower <entity id="D07-1066.6">parsing</entity> scores are an artefact of treebank encoding <entity id="D07-1066.7">schemes</entity> and<entity id="D07-1066.8">data</entity> <entity id="D07-1066.9">structures</entity>, as <entity id="D07-1066.10">claimed</entity> by Kiibler et al. (2006). This <entity id="D07-1066.11">claim</entity> is <entity id="D07-1066.12">based</entity> on the <entity id="D07-1066.13">assumption</entity> that PARSEVAL <entity id="D07-1066.14">metrics</entity> fully reflect <entity id="D07-1066.15">parse</entity> <entity id="D07-1066.16">quality</entity> across treebank encoding <entity id="D07-1066.17">schemes</entity>. In this <entity id="D07-1066.18">paper</entity> we present new <entity id="D07-1066.19">experiments</entity> to <entity id="D07-1066.20">test</entity> this <entity id="D07-1066.21">claim</entity>. We use the PARSEVAL <entity id="D07-1066.22">metric</entity>, the Leaf-Ancestor <entity id="D07-1066.23">metric</entity> as well as a <entity id="D07-1066.24">dependency-based</entity> <entity id="D07-1066.25">evaluation</entity>, and present novel <entity id="D07-1066.26">approaches</entity> measuring the <entity id="D07-1066.27">effect</entity> of controlled <entity id="D07-1066.28">error</entity> <entity id="D07-1066.29">insertion</entity> on treebank <entity id="D07-1066.30">trees</entity> and <entity id="D07-1066.31">parser</entity> <entity id="D07-1066.32">output</entity>. We also <entity id="D07-1066.33">provide</entity> extensive <entity id="D07-1066.34">past-parsing</entity> <entity id="D07-1066.35">cross-treebank</entity> <entity id="D07-1066.36">conversion</entity>. The <entity id="D07-1066.37">results</entity> of the <entity id="D07-1066.38">experiments</entity> show that, contrary to Kubler et al. (2006), the <entity id="D07-1066.39">question</entity> whether or not German is harder to <entity id="D07-1066.40">parse</entity> than <entity id="D07-1066.41">English</entity> remains undecided.
</abstract>

</text>

<text id="D07-1072">
<title>The Infinite PCFG Using Hierarchical Dirichlet Processes</title>
<abstract>
We present a nonparametric Bayesian <entity id="D07-1072.1">model</entity> of <entity id="D07-1072.2">tree structures</entity> <entity id="D07-1072.3">based</entity> on the hierarchical Dirichlet <entity id="D07-1072.4">process</entity> (HDP). Our HDP-PCFG <entity id="D07-1072.5">model</entity> allows the <entity id="D07-1072.6">complexity</entity> of the grammar to grow as more <entity id="D07-1072.7">training</entity><entity id="D07-1072.8">data</entity> is available. In <entity id="D07-1072.9">addition</entity> to presenting a fully Bayesian <entity id="D07-1072.10">model</entity> for the PCFG, we also <entity id="D07-1072.11">develop</entity> an efficient variational <entity id="D07-1072.12">inference</entity> <entity id="D07-1072.13">procedure</entity>. On synthetic data, we recover the correct grammar without having to specify its <entity id="D07-1072.14">complexity</entity> in <entity id="D07-1072.15">advance</entity>. We also show that our <entity id="D07-1072.16">techniques</entity> can be <entity id="D07-1072.17">applied</entity> to <entity id="D07-1072.18">full-scale</entity> <entity id="D07-1072.19">parsing</entity> <entity id="D07-1072.20">applications</entity> by demonstrating its <entity id="D07-1072.21">effectiveness</entity> in learning state-split grammars.
</abstract>

</text>

<text id="W06-1701">
<title>Web-Based Frequency Dictionaries For Medium Density Languages</title>
<abstract><entity id="W06-1701.1">Frequency</entity> <entity id="W06-1701.2">dictionaries</entity> play an important <entity id="W06-1701.3">role</entity> both in psycholinguistic <entity id="W06-1701.4">experiment</entity> <entity id="W06-1701.5">design</entity> and in <entity id="W06-1701.6">language technology</entity>. The <entity id="W06-1701.7">paper</entity> describes a new, freely available, web-based <entity id="W06-1701.8">frequency</entity> <entity id="W06-1701.9">dictionary</entity> of Hungarian that is being used for both <entity id="W06-1701.10">purposes</entity>, and the <entity id="W06-1701.11">language-independent</entity> <entity id="W06-1701.12">techniques</entity> used for creating it.
</abstract>

</text>

<text id="W06-2110">
<title>Automatic Identification Of English Verb Particle Constructions Using Linguistic Features</title>
<abstract>
This <entity id="W06-2110.1">paper</entity> presents a <entity id="W06-2110.2">method</entity> for identifying token <entity id="W06-2110.3">instances</entity> of <entity id="W06-2110.4">verb</entity> <entity id="W06-2110.5">particle</entity> <entity id="W06-2110.6">constructions</entity> (VPCs) automatically, <entity id="W06-2110.7">based</entity> on the <entity id="W06-2110.8">output</entity> of the RASP <entity id="W06-2110.9">parser</entity>. The <entity id="W06-2110.10">proposed</entity> <entity id="W06-2110.11">method</entity> pools together <entity id="W06-2110.12">instances</entity> of VPCs and <entity id="W06-2110.13">verb-</entity>PPs from the <entity id="W06-2110.14">parser</entity> <entity id="W06-2110.15">output</entity> and uses the sentential <entity id="W06-2110.16">context</entity> of each such <entity id="W06-2110.17">instance</entity> to differentiate VPCs from <entity id="W06-2110.18">verb-</entity>PPs. We show our <entity id="W06-2110.19">technique</entity> to <entity id="W06-2110.20">perform</entity> at an F-score of 97.4% at identifying VPCs in <entity id="W06-2110.21">Wall Street Journal</entity> and <entity id="W06-2110.22">Brown Corpus</entity><entity id="W06-2110.23">data</entity> taken from the Penn <entity id="W06-2110.24">Tree-bank</entity>.
</abstract>

</text>

<text id="P07-1067">
<title>Coreference Resolution Using Semantic Relatedness Information from Automatically Discovered Patterns</title>
<abstract><entity id="P07-1067.1">Semantic</entity> <entity id="P07-1067.2">relatedness</entity> is a very important <entity id="P07-1067.3">factor</entity> for the <entity id="P07-1067.4">coreference resolution</entity> <entity id="P07-1067.5">task</entity>. To obtain this <entity id="P07-1067.6">semantic information</entity>, <entity id="P07-1067.7">corpus-based approaches</entity> commonly <entity id="P07-1067.8">leverage</entity> <entity id="P07-1067.9">patterns</entity> that can express a specific <entity id="P07-1067.10">semantic relation</entity>. The <entity id="P07-1067.11">patterns</entity>, however, are <entity id="P07-1067.12">designed</entity> manually and thus are not necessarily the most effective ones in <entity id="P07-1067.13">terms</entity> of <entity id="P07-1067.14">accuracy</entity> and breadth. To <entity id="P07-1067.15">deal</entity> with this <entity id="P07-1067.16">problem</entity>, in this <entity id="P07-1067.17">paper</entity> we <entity id="P07-1067.18">propose</entity> an <entity id="P07-1067.19">approach</entity> that can automatically find the effective <entity id="P07-1067.20">patterns</entity> for <entity id="P07-1067.21">coreference resolution</entity>. We explore how to automatically discover and <entity id="P07-1067.22">evaluate</entity> <entity id="P07-1067.23">patterns</entity>, and how to exploit the <entity id="P07-1067.24">patterns</entity> to obtain the <entity id="P07-1067.25">semantic</entity> <entity id="P07-1067.26">relatedness</entity> <entity id="P07-1067.27">information</entity>. The <entity id="P07-1067.28">evaluation</entity> on <entity id="P07-1067.29">ACE</entity><entity id="P07-1067.30">data</entity> set shows that the <entity id="P07-1067.31">pattern</entity> <entity id="P07-1067.32">based</entity> <entity id="P07-1067.33">semantic information</entity> is helpful for <entity id="P07-1067.34">coreference resolution</entity>.
</abstract>

</text>

<text id="P07-1114">
<title>Words and Echoes: Assessing and Mitigating the Non-Randomness Problem in Word Frequency Distribution Modeling</title>
<abstract><entity id="P07-1114.1">Frequency</entity> <entity id="P07-1114.2">distribution</entity> <entity id="P07-1114.3">models</entity> tuned to <entity id="P07-1114.4">words</entity> and other linguistic <entity id="P07-1114.5">events</entity> can predict the <entity id="P07-1114.6">number</entity> of distinct <entity id="P07-1114.7">types</entity> and their <entity id="P07-1114.8">frequency</entity> <entity id="P07-1114.9">distribution</entity> in <entity id="P07-1114.10">samples</entity> of arbitrary <entity id="P07-1114.11">sizes</entity>. We conduct, for the first <entity id="P07-1114.12">time</entity>, a rigorous <entity id="P07-1114.13">evaluation</entity> of these <entity id="P07-1114.14">models</entity> <entity id="P07-1114.15">based</entity> on <entity id="P07-1114.16">cross-validation</entity> and separation of <entity id="P07-1114.17">training</entity> and <entity id="P07-1114.18">test</entity><entity id="P07-1114.19">data</entity>. Our <entity id="P07-1114.20">experiments</entity> reveal that the <entity id="P07-1114.21">prediction</entity> <entity id="P07-1114.22">accuracy</entity> of the <entity id="P07-1114.23">models</entity> is <entity id="P07-1114.24">marred</entity> by serious overfitting <entity id="P07-1114.25">problems</entity>, <entity id="P07-1114.26">due</entity> to violations of the random <entity id="P07-1114.27">sampling</entity> <entity id="P07-1114.28">assumption</entity> in <entity id="P07-1114.29">corpus</entity><entity id="P07-1114.30">data</entity>. We then <entity id="P07-1114.31">propose</entity> a <entity id="P07-1114.32">simple</entity> <entity id="P07-1114.33">pre-processing</entity> <entity id="P07-1114.34">method</entity> to alleviate such non-randomness <entity id="P07-1114.35">problems</entity>. Further <entity id="P07-1114.36">evaluation</entity> confirms the <entity id="P07-1114.37">effectiveness</entity> of the <entity id="P07-1114.38">method</entity>, which compares favourably to more <entity id="P07-1114.39">complex</entity> <entity id="P07-1114.40">correction</entity> <entity id="P07-1114.41">techniques</entity>. different <entity id="P07-1114.42">degrees</entity> in the<entity id="P07-1114.43">data</entity> ( Baayen, 1992 ). Consider for <entity id="P07-1114.44">example</entity> a very <entity id="P07-1114.45">common</entity> <entity id="P07-1114.46">prefix</entity> such as re-meta-. metametahow many typos are there on the Internet?)
</abstract>

</text>

<text id="P07-2051">
<title>Shallow Dependency Labeling</title>
<abstract>
We present a <entity id="P07-2051.1">formalization</entity> of <entity id="P07-2051.2">dependency</entity> labeling with <entity id="P07-2051.3">Integer Linear Programming</entity>. We <entity id="P07-2051.4">focus</entity> on the <entity id="P07-2051.5">integration</entity> of subcatego-rization into the <entity id="P07-2051.6">decision</entity> making <entity id="P07-2051.7">process</entity>, where the various subcategorization <entity id="P07-2051.8">frames</entity> of a <entity id="P07-2051.9">verb</entity> compete with each other. A <entity id="P07-2051.10">maximum entropy model</entity> <entity id="P07-2051.11">provides</entity> the <entity id="P07-2051.12">weights</entity> for ILP <entity id="P07-2051.13">optimization</entity>.
</abstract>

</text>

<text id="P08-1085">
<title>EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)</title>
<abstract>
We address the <entity id="P08-1085.1">task</entity> of unsupervised <entity id="P08-1085.2">POS tagging</entity>. We demonstrate that good <entity id="P08-1085.3">results</entity> can be obtained using the <entity id="P08-1085.4">robust</entity> EM-HMM learner when <entity id="P08-1085.5">provided</entity> with good initial conditions, even with incomplete <entity id="P08-1085.6">dictionaries</entity>. We present a family of <entity id="P08-1085.7">algorithms</entity> to <entity id="P08-1085.8">compute</entity> effective initial <entity id="P08-1085.9">estimations</entity> p(t\w).
</abstract>

</text>

<text id="P08-2056">
<title>Adapting a WSJ-Trained Parser to Grammatically Noisy Text</title>
<abstract>
We present a <entity id="P08-2056.1">robust</entity> <entity id="P08-2056.2">parser</entity> which is <entity id="P08-2056.3">trained</entity> on a treebank of ungrammatical <entity id="P08-2056.4">sentences</entity>. The treebank is created automatically by modifying <entity id="P08-2056.5">Penn treebank</entity> <entity id="P08-2056.6">sentences</entity> so that they contain one or more <entity id="P08-2056.7">syntactic</entity> <entity id="P08-2056.8">errors</entity>. We <entity id="P08-2056.9">evaluate</entity> an existing Penn-treebank-trained <entity id="P08-2056.10">parser</entity> on the ungrammatical treebank to see how it reacts to <entity id="P08-2056.11">noise</entity> in the <entity id="P08-2056.12">form</entity> of grammatical <entity id="P08-2056.13">errors</entity>. We <entity id="P08-2056.14">re-train</entity> this <entity id="P08-2056.15">parser</entity> on the <entity id="P08-2056.16">training</entity> <entity id="P08-2056.17">section</entity> of the ungrammatical treebank, leading to an significantly <entity id="P08-2056.18">improved</entity> <entity id="P08-2056.19">performance</entity> on the ungrammatical <entity id="P08-2056.20">test sets</entity>. We show how a <entity id="P08-2056.21">classifier</entity> can be used to prevent <entity id="P08-2056.22">performance</entity> <entity id="P08-2056.23">degradation</entity> on the original grammatical<entity id="P08-2056.24">data</entity>.
</abstract>

</text>

<text id="J08-1005">
<title>Book Reviews: Incremental Conceptualization for Language Production by Markus Guhe </title>
<abstract>
"For the past ten years or more, most work in the <entity id="J08-1005.1">field</entity> of <entity id="J08-1005.2">Natural Language Generation</entity> (NLG) has shied away from <entity id="J08-1005.3">considerations</entity> regarding the <entity id="J08-1005.4">processes</entity> underlying <entity id="J08-1005.5">human language</entity> production. Rather, the <entity id="J08-1005.6">focus</entity> has been on <entity id="J08-1005.7">systems</entity> that automatically produce <entity id="J08-1005.8">language</entity> usually <entity id="J08-1005.9">text</entity> from non-linguistic <entity id="J08-1005.10">representations</entity>, with the <entity id="J08-1005.11">main</entity> <entity id="J08-1005.12">objective</entity> being <entity id="J08-1005.13">generation</entity> of a <entity id="J08-1005.14">text</entity> that faithfully captures the meaning of those non-linguistic <entity id="J08-1005.15">representations</entity> (see, e.g., Reiter and Dale 's 2000 textbook on NLG). There is, however, also a different take on NLG "as not just competent <entity id="J08-1005.16">performance</entity> by a <entity id="J08-1005.17">computer</entity> but the <entity id="J08-1005.18">development</entity> of a <entity id="J08-1005.19">computational</entity> <entity id="J08-1005.20">theory</entity> of the human <entity id="J08-1005.21">capacity</entity> for <entity id="J08-1005.22">language</entity> and <entity id="J08-1005.23">processes</entity> that engage it" ( McDonald 1987, <entity id="J08-1005.24">page</entity> 642). Guhe 's <entity id="J08-1005.25">research</entity> monograph, <entity id="J08-1005.26">based</entity> on his 2003 Ph.D. <entity id="J08-1005.27">thesis</entity>, is firmly situated in the latter tradition. One of his <entity id="J08-1005.28">main</entity> <entity id="J08-1005.29">goals</entity> is to work out a <entity id="J08-1005.30">computational</entity> <entity id="J08-1005.31">architecture</entity> for Levelt 's (1989) psycholinguistically motivated <entity id="J08-1005.32">model</entity> of <entity id="J08-1005.33">language production</entity>. According to Levelt 's <entity id="J08-1005.34">model</entity>, speaking involves three <entity id="J08-1005.35">main</entity> activities: conceptualizing (deciding what to say), formulating (deciding how to say it), and articulating (saying it). Guhe 's book <entity id="J08-1005.36">focuses</entity> on the mental activity of conceptualizing. Conceptualizing is a recalcitrant <entity id="J08-1005.37">object</entity> of <entity id="J08-1005.38">study</entity>, partly because of the <entity id="J08-1005.39">problem</entity> of the "initial spark"; the <entity id="J08-1005.40">decision</entity> to say something appears to be the <entity id="J08-1005.41">result</entity> of volitional conscious <entity id="J08-1005.42">decisions</entity>, which largely elude scientific <entity id="J08-1005.43">study</entity>. Guhe avoids this <entity id="J08-1005.44">problem</entity> by investigating conceptualization in settings where the <entity id="J08-1005.45">main</entity> intention is already fixed: a speaker witnesses several <entity id="J08-1005.46">events</entity> unfold and is instructed to describe what happens "
</abstract>

</text>

<text id="W08-1906">
<title>First ideas of user-adapted views of lexicographicdata exemplified on OWID and elexiko</title>
<abstract>
This <entity id="W08-1906.1">paper</entity> is a <entity id="W08-1906.2">project</entity> <entity id="W08-1906.3">report</entity> of the lexicographic Internet portal OWID, an Online <entity id="W08-1906.4">Vocabulary</entity> <entity id="W08-1906.5">Information System</entity> of German which is being built at the Institute of German <entity id="W08-1906.6">Language</entity> in Mannheim (IDS). Overall, the contents of the portal and its technical <entity id="W08-1906.7">approaches</entity> will be presented. The <entity id="W08-1906.8">lexical database</entity> is <entity id="W08-1906.9">structured</entity> in a granular way which allows to extend possible <entity id="W08-1906.10">search</entity> <entity id="W08-1906.11">options</entity> for lexicographers. Against the <entity id="W08-1906.12">background</entity> of <entity id="W08-1906.13">current</entity> <entity id="W08-1906.14">research</entity> on using <entity id="W08-1906.15">electronic dictionaries</entity>, the <entity id="W08-1906.16">project</entity> OWID is also working on first ideas of <entity id="W08-1906.17">user-adapted</entity> <entity id="W08-1906.18">access</entity> and <entity id="W08-1906.19">user-adapted</entity> views of the lexicographic<entity id="W08-1906.20">data</entity>. <entity id="W08-1906.21">Due</entity> to the fact that the portal OWID comprises <entity id="W08-1906.22">dictionaries</entity> which are available online it is possible to change the <entity id="W08-1906.23">design</entity> and <entity id="W08-1906.24">functions</entity> of the website easily (in <entity id="W08-1906.25">comparison</entity> to printed <entity id="W08-1906.26">dictionaries</entity>). Ideas of <entity id="W08-1906.27">implementing</entity> <entity id="W08-1906.28">user-adapted</entity> views of the lexicographic<entity id="W08-1906.29">data</entity> will be demonstrated by using an <entity id="W08-1906.30">example</entity> taken from one of the <entity id="W08-1906.31">dictionaries</entity> of the portal, namely elexiko.
</abstract>

</text>

<text id="W08-2139">
<title>Probabilistic Model for Syntactic and Semantic Dependency Parsing</title>
<abstract>
This <entity id="W08-2139.1">paper</entity> <entity id="W08-2139.2">proposes</entity> a novel <entity id="W08-2139.3">method</entity> to analyze <entity id="W08-2139.4">syntactic</entity> <entity id="W08-2139.5">dependencies</entity> and label <entity id="W08-2139.6">semantic</entity> <entity id="W08-2139.7">dependencies</entity> around both the <entity id="W08-2139.8">verbal</entity> predicates and the <entity id="W08-2139.9">nouns</entity>. In this <entity id="W08-2139.10">method</entity>, a <entity id="W08-2139.11">probabilistic model</entity> is <entity id="W08-2139.12">designed</entity> to obtain a global <entity id="W08-2139.13">optimal</entity> <entity id="W08-2139.14">result</entity>. Moreover, a predicate <entity id="W08-2139.15">identification</entity> <entity id="W08-2139.16">model</entity> and a <entity id="W08-2139.17">disambiguation</entity> <entity id="W08-2139.18">model</entity> are <entity id="W08-2139.19">proposed</entity> to label predicates and their senses. The <entity id="W08-2139.20">experimental</entity> <entity id="W08-2139.21">results</entity> obtained on the wsj and brown <entity id="W08-2139.22">test sets</entity> show that our <entity id="W08-2139.23">system</entity> obtains 77% of labeled macro F1 score for the whole <entity id="W08-2139.24">task</entity>, 84.47% of labeled <entity id="W08-2139.25">attachment</entity> score for <entity id="W08-2139.26">syntactic</entity> <entity id="W08-2139.27">dependency</entity> <entity id="W08-2139.28">task</entity>, and 69.45% of labeled F1 score for <entity id="W08-2139.29">semantic</entity> <entity id="W08-2139.30">dependency</entity> <entity id="W08-2139.31">task</entity>.
</abstract>

</text>

<text id="N07-2015">
<title>Are Very Large N-Best Lists Useful for SMT?</title>
<abstract>
This <entity id="N07-2015.1">paper</entity> describes an efficient <entity id="N07-2015.2">method</entity> to <entity id="N07-2015.3">extract</entity> large n-best <entity id="N07-2015.4">lists</entity> from a <entity id="N07-2015.5">word</entity> graph produced by a <entity id="N07-2015.6">statistical machine translation system</entity>. The <entity id="N07-2015.7">extraction</entity> is <entity id="N07-2015.8">based</entity> on the k shortest <entity id="N07-2015.9">paths</entity> <entity id="N07-2015.10">algorithm</entity> which is efficient even for very large k. We show that, although we can <entity id="N07-2015.11">generate</entity> large <entity id="N07-2015.12">amounts</entity> of distinct <entity id="N07-2015.13">translation</entity> <entity id="N07-2015.14">hypotheses</entity>, these numerous <entity id="N07-2015.15">candidates</entity> are not able to significantly <entity id="N07-2015.16">improve</entity> overall <entity id="N07-2015.17">system</entity> <entity id="N07-2015.18">performance</entity>. We conclude that large n-best <entity id="N07-2015.19">lists</entity> would <entity id="N07-2015.20">benefit</entity> from better discriminating <entity id="N07-2015.21">models</entity>.
</abstract>

</text>

<text id="P03-1053">
<title>A Word-Order Database For Testing Computational Models Of Language Acquisition</title>
<abstract>
An investment of <entity id="P03-1053.1">effort</entity> over the last two years has begun to produce a wealth of<entity id="P03-1053.2">data</entity> <entity id="P03-1053.3">concerning</entity> <entity id="P03-1053.4">computational</entity> psycholin-guistic <entity id="P03-1053.5">models</entity> of <entity id="P03-1053.6">syntax</entity> <entity id="P03-1053.7">acquisition</entity>. The<entity id="P03-1053.8">data</entity> is <entity id="P03-1053.9">generated</entity> by running <entity id="P03-1053.10">simulations</entity> on a recently completed <entity id="P03-1053.11">database</entity> of <entity id="P03-1053.12">word</entity> <entity id="P03-1053.13">order</entity> <entity id="P03-1053.14">patterns</entity> from over 3,000 <entity id="P03-1053.15">abstract</entity> <entity id="P03-1053.16">languages</entity>. This article presents the <entity id="P03-1053.17">design</entity> of the <entity id="P03-1053.18">database</entity> which contains <entity id="P03-1053.19">sentence</entity> <entity id="P03-1053.20">patterns</entity>, grammars and <entity id="P03-1053.21">derivations</entity> that can be used to <entity id="P03-1053.22">test</entity> <entity id="P03-1053.23">acquisition</entity> <entity id="P03-1053.24">models</entity> from widely divergent <entity id="P03-1053.25">paradigms</entity>. The <entity id="P03-1053.26">domain</entity> is <entity id="P03-1053.27">generated</entity> from grammars that are linguistically motivated by <entity id="P03-1053.28">current</entity> <entity id="P03-1053.29">syntactic</entity> <entity id="P03-1053.30">theory</entity> and the <entity id="P03-1053.31">sentence</entity> <entity id="P03-1053.32">patterns</entity> have been validated as psychologically/developmen-tally plausible by <entity id="P03-1053.33">checking</entity> their <entity id="P03-1053.34">frequency</entity> of <entity id="P03-1053.35">occurrence</entity> in <entity id="P03-1053.36">corpora</entity> of child-directed <entity id="P03-1053.37">speech</entity>. A small <entity id="P03-1053.38">case-study</entity> <entity id="P03-1053.39">simulation</entity> is also presented.
</abstract>

</text>

<text id="P03-2035">
<title>Deverbal Compound Noun Analysis Based On Lexical Conceptual Structure</title>
<abstract>
This <entity id="P03-2035.1">paper</entity> <entity id="P03-2035.2">proposes</entity> a principled <entity id="P03-2035.3">approach</entity> for <entity id="P03-2035.4">analysis</entity> of <entity id="P03-2035.5">semantic relations</entity> between <entity id="P03-2035.6">constituents</entity> in compound <entity id="P03-2035.7">nouns</entity> <entity id="P03-2035.8">based</entity> on <entity id="P03-2035.9">lexical</entity> <entity id="P03-2035.10">semantic structure</entity>. One of the <entity id="P03-2035.11">difficulties</entity> of compound <entity id="P03-2035.12">noun</entity> <entity id="P03-2035.13">analysis</entity> is that the <entity id="P03-2035.14">mechanisms</entity> governing the <entity id="P03-2035.15">decision</entity> <entity id="P03-2035.16">system</entity> of <entity id="P03-2035.17">semantic relations</entity> and the <entity id="P03-2035.18">representation</entity> <entity id="P03-2035.19">method</entity> of <entity id="P03-2035.20">semantic relations</entity> associated with <entity id="P03-2035.21">lexical</entity> and contextual meaning are not obvious. The aim of our <entity id="P03-2035.22">research</entity> is to clarify how <entity id="P03-2035.23">lexical semantics</entity> contribute to the <entity id="P03-2035.24">relations</entity> in compound <entity id="P03-2035.25">nouns</entity> since such <entity id="P03-2035.26">nouns</entity> are very productive and are supposed to be governed by systematic <entity id="P03-2035.27">mechanisms</entity>. The <entity id="P03-2035.28">results</entity> of <entity id="P03-2035.29">applying</entity> our <entity id="P03-2035.30">approach</entity> to the <entity id="P03-2035.31">analysis</entity> of <entity id="P03-2035.32">noun-deverbal</entity> compounds in <entity id="P03-2035.33">Japanese</entity> and <entity id="P03-2035.34">English</entity> show that <entity id="P03-2035.35">lexical</entity> <entity id="P03-2035.36">conceptual structure</entity> contributes to the re-strictional <entity id="P03-2035.37">rules</entity> in compounds.
</abstract>

</text>

<text id="P04-1051">
<title>Computing Locally Coherent Discourses</title>
<abstract>
We present the first <entity id="P04-1051.1">algorithm</entity> that <entity id="P04-1051.2">computes</entity> <entity id="P04-1051.3">optimal</entity> orderings of <entity id="P04-1051.4">sentences</entity> into a locally coherent <entity id="P04-1051.5">discourse</entity>. The <entity id="P04-1051.6">algorithm</entity> runs very efficiently on a <entity id="P04-1051.7">variety</entity> of <entity id="P04-1051.8">coherence</entity> measures from the <entity id="P04-1051.9">literature</entity>. We also show that the <entity id="P04-1051.10">discourse</entity> <entity id="P04-1051.11">ordering</entity> <entity id="P04-1051.12">problem</entity> is NP-complete and cannot be approximated.
</abstract>

</text>

<text id="P04-1063">
<title>Multi-Engine Machine Translation With Voted Language Model</title>
<abstract>
The <entity id="P04-1063.1">paper</entity> describes a particular <entity id="P04-1063.2">approach</entity> to <entity id="P04-1063.3">multi-engine</entity> <entity id="P04-1063.4">machine translation</entity> (MEMT), where we make use of voted <entity id="P04-1063.5">language models</entity> to selectively combine <entity id="P04-1063.6">translation</entity> <entity id="P04-1063.7">outputs</entity> from multiple off-the-shelf <entity id="P04-1063.8">MT systems</entity>. <entity id="P04-1063.9">Experiments</entity> are done using large <entity id="P04-1063.10">corpora</entity> from three distinct <entity id="P04-1063.11">domains</entity>. The <entity id="P04-1063.12">study</entity> found that the use of voted <entity id="P04-1063.13">language models</entity> leads to an <entity id="P04-1063.14">improved</entity> <entity id="P04-1063.15">performance</entity> of MEMT <entity id="P04-1063.16">systems</entity>.
</abstract>

</text>

<text id="P04-1078">
<title>A Unified Framework For Automatic Evaluation Using N-Gram Co-Occurrence Statistics</title>
<abstract>
In this <entity id="P04-1078.1">paper</entity> we <entity id="P04-1078.2">propose</entity> a unified <entity id="P04-1078.3">framework</entity> for <entity id="P04-1078.4">automatic evaluation</entity> of <entity id="P04-1078.5">NLP applications</entity> using N-<entity id="P04-1078.6">gram</entity> <entity id="P04-1078.7">co-occurrence</entity> <entity id="P04-1078.8">statistics</entity>. The automatic <entity id="P04-1078.9">evaluation metrics</entity> <entity id="P04-1078.10">proposed</entity> to <entity id="P04-1078.11">date</entity> for <entity id="P04-1078.12">Machine Translation</entity> and <entity id="P04-1078.13">Automatic Summarization</entity> are particular <entity id="P04-1078.14">instances</entity> from the family of <entity id="P04-1078.15">metrics</entity> we <entity id="P04-1078.16">propose</entity>. We show that different members of the same family of <entity id="P04-1078.17">metrics</entity> explain best the <entity id="P04-1078.18">variations</entity> obtained with human <entity id="P04-1078.19">evaluations</entity>, according to the <entity id="P04-1078.20">application</entity> being <entity id="P04-1078.21">evaluated</entity> (<entity id="P04-1078.22">Machine Translation</entity>, <entity id="P04-1078.23">Automatic Summarization</entity>, and <entity id="P04-1078.24">Automatic</entity> <entity id="P04-1078.25">Question Answering</entity>) and the <entity id="P04-1078.26">evaluation</entity> <entity id="P04-1078.27">guidelines</entity> used by humans for <entity id="P04-1078.28">evaluating</entity> such <entity id="P04-1078.29">applications</entity>.
</abstract>

</text>

<text id="P06-2003">
<title>MT Evaluation: Human-Like Vs. Human Acceptable</title>
<abstract>
We present a comparative <entity id="P06-2003.1">study</entity> on <entity id="P06-2003.2">Machine Translation Evaluation</entity> according to two different <entity id="P06-2003.3">criteria</entity>: Human Likeness and Human Acceptability. We <entity id="P06-2003.4">provide</entity> empirical <entity id="P06-2003.5">evidence</entity> that there is a <entity id="P06-2003.6">relationship</entity> between these two <entity id="P06-2003.7">kinds</entity> of <entity id="P06-2003.8">evaluation</entity>: Human Likeness implies Human Acceptability but the reverse is not true. From the <entity id="P06-2003.9">point of view</entity> of <entity id="P06-2003.10">automatic evaluation</entity> this implies that <entity id="P06-2003.11">metrics</entity> <entity id="P06-2003.12">based</entity> on Human Likeness are more reliable for <entity id="P06-2003.13">system</entity> tuning. Our <entity id="P06-2003.14">results</entity> also show that <entity id="P06-2003.15">current</entity> <entity id="P06-2003.16">evaluation metrics</entity> are not always able to distinguish between <entity id="P06-2003.17">automatic</entity> and human <entity id="P06-2003.18">translations</entity>. In <entity id="P06-2003.19">order</entity> to <entity id="P06-2003.20">improve</entity> the descriptive power of <entity id="P06-2003.21">current</entity> <entity id="P06-2003.22">metrics</entity> we <entity id="P06-2003.23">propose</entity> the use of additional <entity id="P06-2003.24">syntax-based</entity> <entity id="P06-2003.25">metrics</entity>, and <entity id="P06-2003.26">metric</entity> <entity id="P06-2003.27">combinations</entity> inside the QARLA <entity id="P06-2003.28">Framework</entity>.
</abstract>

</text>

<text id="P06-2069">
<title>Examining The Content Load Of Part Of Speech Blocks For Information Retrieval</title>
<abstract>
We investigate the connection between <entity id="P06-2069.1">part of speech</entity> (POS) <entity id="P06-2069.2">distribution</entity> and <entity id="P06-2069.3">content</entity> in <entity id="P06-2069.4">language</entity>. We define POS blocks to be groups of <entity id="P06-2069.5">parts of speech</entity>. We hypothesise that there exists a directly proportional <entity id="P06-2069.6">relation</entity> between the <entity id="P06-2069.7">frequency</entity> of POS blocks and their <entity id="P06-2069.8">content</entity> salience. We also hypothesise that the <entity id="P06-2069.9">class</entity> membership of the <entity id="P06-2069.10">parts of speech</entity> within such blocks reflects the <entity id="P06-2069.11">content</entity> load of the blocks, on the <entity id="P06-2069.12">basis</entity> that open <entity id="P06-2069.13">class</entity> <entity id="P06-2069.14">parts of speech</entity> are more <entity id="P06-2069.15">content-bearing</entity> than closed <entity id="P06-2069.16">class</entity> <entity id="P06-2069.17">parts of speech</entity>. We <entity id="P06-2069.18">test</entity> these <entity id="P06-2069.19">hypotheses</entity> in the <entity id="P06-2069.20">context</entity> of <entity id="P06-2069.21">Information Retrieval</entity>, by syntactically representing <entity id="P06-2069.22">queries</entity>, and removing from them <entity id="P06-2069.23">content-poor</entity> blocks, in line with the aforementioned <entity id="P06-2069.24">hypotheses</entity>. For our first <entity id="P06-2069.25">hypothesis</entity>, we induce POS <entity id="P06-2069.26">distribution</entity> <entity id="P06-2069.27">information</entity> from a <entity id="P06-2069.28">corpus</entity>, and approximate the <entity id="P06-2069.29">probability</entity> of <entity id="P06-2069.30">occurrence</entity> of POS blocks as per two <entity id="P06-2069.31">statistical</entity> estimators separately. For our second <entity id="P06-2069.32">hypothesis</entity>, we use <entity id="P06-2069.33">simple</entity> heuristics to estimate the <entity id="P06-2069.34">content</entity> load within POS blocks. We use the <entity id="P06-2069.35">Text REtrieval Conference</entity> (TREC) <entity id="P06-2069.36">queries</entity> of 1999 and 2000 to retrieve <entity id="P06-2069.37">documents</entity> from the WT2G and WT10G <entity id="P06-2069.38">test</entity> <entity id="P06-2069.39">collections</entity>, with five different <entity id="P06-2069.40">retrieval</entity> <entity id="P06-2069.41">strategies</entity>. <entity id="P06-2069.42">Experimental</entity> <entity id="P06-2069.43">outcomes</entity> confirm that our <entity id="P06-2069.44">hypotheses</entity> hold in the <entity id="P06-2069.45">context</entity> of <entity id="P06-2069.46">Information Retrieval</entity>.
</abstract>

</text>

<text id="P06-3009">
<title>Integrated Morphological And Syntactic Disambiguation For Modern Hebrew</title>
<abstract><entity id="P06-3009.1">Current</entity> <entity id="P06-3009.2">parsing</entity> <entity id="P06-3009.3">models</entity> are not immediately applicable for <entity id="P06-3009.4">languages</entity> that exhibit strong <entity id="P06-3009.5">interaction</entity> between <entity id="P06-3009.6">morphology</entity> and <entity id="P06-3009.7">syntax</entity>, e.g., Modern Hebrew (MH), Arabic and other Semitic <entity id="P06-3009.8">languages</entity>. This work represents a first attempt at <entity id="P06-3009.9">modeling</entity> <entity id="P06-3009.10">morphological-syntactic</entity> <entity id="P06-3009.11">interaction</entity> in a generative probabilistic <entity id="P06-3009.12">framework</entity> to allow for MH <entity id="P06-3009.13">parsing</entity>. We show that morphological <entity id="P06-3009.14">information</entity> selected in tandem with <entity id="P06-3009.15">syntactic categories</entity> is instrumental for <entity id="P06-3009.16">parsing</entity> Semitic <entity id="P06-3009.17">languages</entity>. We further show that redundant morphological <entity id="P06-3009.18">information</entity> <entity id="P06-3009.19">helps</entity> <entity id="P06-3009.20">syntactic</entity> <entity id="P06-3009.21">disambiguation</entity>.
</abstract>

</text>

<text id="C82-1055">
<title>Lexical Parallelism In Text Structure Determination And Content Analysis</title>
<abstract>
In this <entity id="C82-1055.1">paper</entity> the <entity id="C82-1055.2">problem</entity> is discussed about the <entity id="C82-1055.3">text structure</entity> <entity id="C82-1055.4">determination</entity> and <entity id="C82-1055.5">content</entity> <entity id="C82-1055.6">analysis</entity> by <entity id="C82-1055.7">lexical</entity> parallelism, or the <entity id="C82-1055.8">repetition</entity> of <entity id="C82-1055.9">lexical items</entity>. Intersentential <entity id="C82-1055.10">relations</entity> are determined through the identical, partly identical or <entity id="C82-1055.11">lexico-semantic</entity> <entity id="C82-1055.12">repetition</entity> in <entity id="C82-1055.13">Japanese</entity> scientific <entity id="C82-1055.14">texts</entity>. <entity id="C82-1055.15">Lexical</entity> <entity id="C82-1055.16">parallelism</entity> <entity id="C82-1055.17">ratio</entity> and <entity id="C82-1055.18">lexical</entity> <entity id="C82-1055.19">parallelism</entity> <entity id="C82-1055.20">indicator</entity> <entity id="C82-1055.21">distance</entity> are obtained on <entity id="C82-1055.22">computer</entity> and by <entity id="C82-1055.23">hand</entity>. And the <entity id="C82-1055.24">application</entity> of the <entity id="C82-1055.25">characteristics</entity> to <entity id="C82-1055.26">automatic</entity> <entity id="C82-1055.27">content</entity> <entity id="C82-1055.28">analysis</entity> is dicsussed.
</abstract>

</text>

<text id="C82-2008">
<title>Toward A Parsing Method For Free Word Order Languages</title>
<abstract>
"Free <entity id="C82-2008.1">word</entity> <entity id="C82-2008.2">order</entity>" is a traditional <entity id="C82-2008.3">term</entity> that should not be taken literally. However, we shall retain the <entity id="C82-2008.4">term</entity> for its conciseness.Formal descriptionsof <entity id="C82-2008.5">syntax</entity> have been usually <entity id="C82-2008.6">based</entity> either on the immediate <entity id="C82-2008.7">constituents</entity> or on the <entity id="C82-2008.8">dependency</entity> philosophy. Neither of them seems directly applicable to <entity id="C82-2008.9">free word order languages</entity>. The intertwining <entity id="C82-2008.10">phrases</entity> cannot be described naturally by IC <entity id="C82-2008.11">rules</entity>. Some coordinate <entity id="C82-2008.12">constructions</entity> are difficult to describe by means of <entity id="C82-2008.13">dependency relations</entity>. In our <entity id="C82-2008.14">opinion</entity>, <entity id="C82-2008.15">parsers</entity> for <entity id="C82-2008.16">free word order languages</entity> should not be <entity id="C82-2008.17">based</entity> on the <entity id="C82-2008.18">methods</entity> <entity id="C82-2008.19">developed</entity> within the IC <entity id="C82-2008.20">framework</entity>. Scarce <entity id="C82-2008.21">experiments</entity> with <entity id="C82-2008.22">parsers</entity> <entity id="C82-2008.23">based</entity> on the <entity id="C82-2008.24">dependency</entity> <entity id="C82-2008.25">formalism</entity>, eg. /5/, do not seem promising. Therefore, we decided to take a fresh start and to attack the <entity id="C82-2008.26">problem</entity> by reanalys-ing the <entity id="C82-2008.27">basic</entity> <entity id="C82-2008.28">notions</entity> of <entity id="C82-2008.29">syntax</entity> and <entity id="C82-2008.30">parsing</entity>. We <entity id="C82-2008.31">focus</entity> our attention on those formal <entity id="C82-2008.32">aspects</entity> of a <entity id="C82-2008.33">language system</entity> which might be most useful for <entity id="C82-2008.34">automatic</entity> <entity id="C82-2008.35">text processing</entity>. We assume that the morphological <entity id="C82-2008.36">level</entity> is described along the lines of /2A"
</abstract>

</text>

<text id="P01-1052">
<title>Logic Form Transformation Of WordNet And Its Applicability To Question Answering</title>
<abstract>
WordNet is a rich <entity id="P01-1052.1">source</entity> of <entity id="P01-1052.2">world knowledge</entity> from which formal <entity id="P01-1052.3">axioms</entity> can be derived. In this <entity id="P01-1052.4">paper</entity> we present a <entity id="P01-1052.5">method</entity> for transforming the WordNet glosses into <entity id="P01-1052.6">logic</entity> <entity id="P01-1052.7">forms</entity> and further into <entity id="P01-1052.8">axioms</entity>. The <entity id="P01-1052.9">transformation</entity> of <entity id="P01-1052.10">Word-</entity>Net glosses into <entity id="P01-1052.11">logic</entity> <entity id="P01-1052.12">forms</entity> is useful for <entity id="P01-1052.13">theorem</entity> proving and other <entity id="P01-1052.14">applications</entity>. The <entity id="P01-1052.15">paper</entity> demonstrates the <entity id="P01-1052.16">utility</entity> of the WordNet <entity id="P01-1052.17">axioms</entity> in a <entity id="P01-1052.18">question</entity> answering <entity id="P01-1052.19">system</entity> to <entity id="P01-1052.20">rank</entity> and <entity id="P01-1052.21">extract</entity> answers.
</abstract>

</text>

<text id="P02-1015">
<title>Parsing Non-Recursive CFGs</title>
<abstract>
We consider the <entity id="P02-1015.1">problem</entity> of <entity id="P02-1015.2">parsing</entity> non-recursive <entity id="P02-1015.3">context-free</entity> grammars, i.e., <entity id="P02-1015.4">context-free</entity> grammars that <entity id="P02-1015.5">generate</entity> finite <entity id="P02-1015.6">languages</entity>. In <entity id="P02-1015.7">natural language processing</entity>, this <entity id="P02-1015.8">problem</entity> arises in several <entity id="P02-1015.9">areas</entity> of <entity id="P02-1015.10">application</entity>, <entity id="P02-1015.11">including</entity> <entity id="P02-1015.12">natural language generation</entity>, <entity id="P02-1015.13">speech recognition</entity> and <entity id="P02-1015.14">machine translation</entity>. We present two tabular <entity id="P02-1015.15">algorithms</entity> for <entity id="P02-1015.16">parsing</entity> ofnon-recursive <entity id="P02-1015.17">context-free</entity> grammars, and show that they <entity id="P02-1015.18">perform</entity> well in practical settings, despite the fact that this <entity id="P02-1015.19">problem</entity> is PSPACE-complete.
</abstract>

</text>

<text id="C88-2143">
<title>Massive Disambiguation Of Large Text Corpora With Flexible Categorial Grammar</title>
<abstract>
A new <entity id="C88-2143.1">method</entity> of <entity id="C88-2143.2">automatic</entity> <entity id="C88-2143.3">lexical</entity> <entity id="C88-2143.4">disambiguation</entity> of big <entity id="C88-2143.5">texts</entity> is described, using recent proof-theoretical <entity id="C88-2143.6">results</entity> from the <entity id="C88-2143.7">theory</entity> of <entity id="C88-2143.8">categorial grammar</entity>.
</abstract>

</text>

<text id="C88-2151">
<title>Using Constraints In A Constructive Version Of GPSG</title>
<abstract><entity id="C88-2151.1">Complex</entity> <entity id="C88-2151.2">categories</entity> are caracteristic of <entity id="C88-2151.3">unification</entity> grammars as for <entity id="C88-2151.4">example</entity> GPSG [Shieber86a]. They are sets of <entity id="C88-2151.5">pairs</entity> of <entity id="C88-2151.6">features</entity> and values. The <entity id="C88-2151.7">unification</entity>, which can be <entity id="C88-2151.8">applied</entity> to two or more <entity id="C88-2151.9">categories</entity>, is the essential <entity id="C88-2151.10">operation</entity>. The <entity id="C88-2151.11">papers</entity> of [Shieber85], [Barton85] and [Ristad86] <entity id="C88-2151.12">deal</entity> with the <entity id="C88-2151.13">influence</entity> of <entity id="C88-2151.14">complex</entity> <entity id="C88-2151.15">categories</entity> on the <entity id="C88-2151.16">efficiency</entity> of the <entity id="C88-2151.17">parsing</entity> <entity id="C88-2151.18">algorithm</entity>. This is one <entity id="C88-2151.19">problem</entity> from using <entity id="C88-2151.20">complex</entity> <entity id="C88-2151.21">categories</entity>, another one arises when using a constructive <entity id="C88-2151.22">version</entity> of GPSG (see [Busemann/Hauenschild88] in this <entity id="C88-2151.23">volume</entity>). Namely that the <entity id="C88-2151.24">application</entity> of admissibility conditions, e.g. LP statements and FCRs to 
m a local <entity id="C88-2151.25">tree</entity> t is prevented because particular <entity id="C88-2151.26">feature values</entity> of <entity id="C88-2151.27">categories</entity> in t are not yet specified, but they will be instantiated later somewhere else in the complete <entity id="C88-2151.28">tree</entity>. Similar <entity id="C88-2151.29">problems</entity> are described. in [Karttunen86] for D-PATR. This work describes the latter <entity id="C88-2151.30">problem</entity> and will present a <entity id="C88-2151.31">solution</entity> working with <entity id="C88-2151.32">computation</entity>, <entity id="C88-2151.33">evaluation</entity> and <entity id="C88-2151.34">propagation</entity> of <entity id="C88-2151.35">constraints</entity> within local <entity id="C88-2151.36">trees</entity> (depth 1). The <entity id="C88-2151.37">constraint</entity> <entity id="C88-2151.38">evaluation</entity> will reject local <entity id="C88-2151.39">trees</entity> if the <entity id="C88-2151.40">constraints</entity> of the subtrees of the daughters are violated.
</abstract>

</text>

<text id="C90-3004">
<title>Phonological Processing Of Speech Variants</title>
<abstract>
This <entity id="C90-3004.1">paper</entity> describes a <entity id="C90-3004.2">strategy</entity> for the <entity id="C90-3004.3">extension</entity> of the phonological <entity id="C90-3004.4">lexicon</entity> in <entity id="C90-3004.5">order</entity> that nonstandard <entity id="C90-3004.6">forms</entity> which arise in fast <entity id="C90-3004.7">speech</entity> may be <entity id="C90-3004.8">processed</entity> by a <entity id="C90-3004.9">speech recognition system</entity>. By way of illustration, an <entity id="C90-3004.10">outline</entity> of the phonological <entity id="C90-3004.11">processing</entity> of <entity id="C90-3004.12">standard</entity> wordforms by the phonological <entity id="C90-3004.13">parser</entity> (PhoPa) is given and then the <entity id="C90-3004.14">extension</entity> <entity id="C90-3004.15">procedure</entity> which is <entity id="C90-3004.16">based</entity> on this phonological <entity id="C90-3004.17">parser</entity> is discussed. The <entity id="C90-3004.18">lexicon</entity> <entity id="C90-3004.19">extension</entity> <entity id="C90-3004.20">procedure</entity> has two stages: phonotactic <entity id="C90-3004.21">extension</entity> which involves the <entity id="C90-3004.22">introduction</entity> of additional <entity id="C90-3004.23">restrictions</entity> into the phonotactic <entity id="C90-3004.24">network</entity> for the <entity id="C90-3004.25">standard</entity> <entity id="C90-3004.26">language</entity> in the <entity id="C90-3004.27">form</entity> of metarules describing phonological <entity id="C90-3004.28">processes</entity>, and specialised <entity id="C90-3004.29">word model</entity> <entity id="C90-3004.30">construction</entity> whereby for each <entity id="C90-3004.31">standard</entity> phonemic wordform a <entity id="C90-3004.32">verification</entity> net which contains all <entity id="C90-3004.33">variants</entity> of this <entity id="C90-3004.34">standard</entity> <entity id="C90-3004.35">form</entity> is compiled. The complete <entity id="C90-3004.36">system</entity> serves as a phonologically oriented <entity id="C90-3004.37">lexicon</entity> <entity id="C90-3004.38">development</entity> <entity id="C90-3004.39">tool</entity>, and its theoretical interest lies in its <entity id="C90-3004.40">contribution</entity> to the <entity id="C90-3004.41">field</entity> of <entity id="C90-3004.42">speech</entity> <entity id="C90-3004.43">variant</entity> <entity id="C90-3004.44">learning</entity>.
</abstract>

</text>

<text id="C92-1051">
<title>Zero Pronouns As Experiencer In Japanese Discourse</title>
<abstract>
The <entity id="C92-1051.1">process</entity> of finding the antecedent of zero pronoun, that is indispensable to <entity id="C92-1051.2">Japanese</entity> <entity id="C92-1051.3">language understanding</entity>, is the <entity id="C92-1051.4">topic</entity> of this <entity id="C92-1051.5">paper</entity>. Here we mainly <entity id="C92-1051.6">concern</entity> with <entity id="C92-1051.7">discourses</entity> comprising two <entity id="C92-1051.8">sentences</entity> that are in a subordinate <entity id="C92-1051.9">relation</entity>, especially one of them describes the <entity id="C92-1051.10">agent</entity>'s volitional <entity id="C92-1051.11">action</entity> and the other describes the <entity id="C92-1051.12">reason</entity> of the <entity id="C92-1051.13">action</entity>. We <entity id="C92-1051.14">propose</entity> basically two new <entity id="C92-1051.15">principles</entity>: (1) The <entity id="C92-1051.16">agent</entity> of an <entity id="C92-1051.17">action</entity> should <entity id="C92-1051.18">experience</entity> a certain psychological <entity id="C92-1051.19">reason</entity>, (2) Predicates <entity id="C92-1051.20">reporting</entity> someone's psychological state are categorized into 1) weakly or 2) strongly bound to the expected <entity id="C92-1051.21">point of view</entity>. <entity id="C92-1051.22">Combination</entity> of these <entity id="C92-1051.23">principles</entity> accounts for some problematic <entity id="C92-1051.24">Japanese</entity> zero anaphora, which cannot be accounted for by the <entity id="C92-1051.25">theories</entity> so far <entity id="C92-1051.26">proposed</entity>.
</abstract>

</text>

<text id="C92-4173">
<title>Tokenization As The Initial Phase In NLP</title>
<abstract>
In this <entity id="C92-4173.1">paper</entity>, the authors address the <entity id="C92-4173.2">significance</entity> and <entity id="C92-4173.3">complexity</entity> of tokenization, the beginning <entity id="C92-4173.4">step</entity> of NLP. Notions of <entity id="C92-4173.5">word</entity> and token <entity id="C92-4173.6">arc</entity> discussed and defined from the viewpoints of lexicography and pragmatic <entity id="C92-4173.7">implementation</entity>, respectively. <entity id="C92-4173.8">Automatic</entity> segmentation of <entity id="C92-4173.9">Chinese</entity> <entity id="C92-4173.10">words</entity> is presented as an illustration of tokenization. Practical <entity id="C92-4173.11">approaches</entity> to <entity id="C92-4173.12">identification</entity> of compound tokens in <entity id="C92-4173.13">English</entity>, such as <entity id="C92-4173.14">idioms</entity>, phrasal <entity id="C92-4173.15">verbs</entity> and fixed <entity id="C92-4173.16">expressions</entity>, are <entity id="C92-4173.17">developed</entity>.
</abstract>

</text>

<text id="C94-1020">
<title>An English-To-Korean Machine Translator: MATES/EK</title>
<abstract>
This <entity id="C94-1020.1">note</entity> introduces an EnglishKorean <entity id="C94-1020.2">Machine Translation System</entity> MATES/EK, which has been de veloped as a <entity id="C94-1020.3">research</entity> <entity id="C94-1020.4">prototype</entity> and still under upgrading KAIST (Korea Advanced Institute of <entity id="C94-1020.5">Science</entity> and <entity id="C94-1020.6">Technology</entity>). MATES/EK a <entity id="C94-1020.7">transfer-based system</entity> and has several subsystems that can be used <entity id="C94-1020.8">support</entity> other -<entity id="C94-1020.9">developments</entity>. They are grammar <entity id="C94-1020.10">developing</entity> <entity id="C94-1020.11">environment</entity> <entity id="C94-1020.12">systems</entity>, <entity id="C94-1020.13">dictionary</entity> developing <entity id="C94-1020.14">tools</entity>, a set augmented <entity id="C94-1020.15">context</entity> free grain <entity id="C94-1020.16">mars</entity> <entity id="C94-1020.17">English</entity> <entity id="C94-1020.18">syntactic analysis</entity>, and so on.
</abstract>

</text>

<text id="C94-1062">
<title>Notes On LR Parser Design</title>
<abstract>
This <entity id="C94-1062.1">paper</entity> discusses the <entity id="C94-1062.2">design</entity> of an LR <entity id="C94-1062.3">parser</entity> for a specific <entity id="C94-1062.4">high-coverage</entity> <entity id="C94-1062.5">English</entity> grammar. The <entity id="C94-1062.6">design</entity> <entity id="C94-1062.7">principles</entity>, though, are applicable to a large <entity id="C94-1062.8">class</entity> of <entity id="C94-1062.9">unification-based</entity> grammars where the <entity id="C94-1062.10">constraints</entity> are realized as Prolog <entity id="C94-1062.11">terms</entity> and <entity id="C94-1062.12">applied</entity> monotonically through instantiation, where there is no right movement, and where left movement is handled by <entity id="C94-1062.13">gap</entity> <entity id="C94-1062.14">threading</entity>. The LR <entity id="C94-1062.15">parser</entity> was <entity id="C94-1062.16">constructed</entity> for <entity id="C94-1062.17">experiments</entity> on probabilistic <entity id="C94-1062.18">parsing</entity> and speedup <entity id="C94-1062.19">learning</entity>, see [10]. LR <entity id="C94-1062.20">parsers</entity> are suitable for probabilistic <entity id="C94-1062.21">parsing</entity> since they contain a <entity id="C94-1062.22">representation</entity> of the <entity id="C94-1062.23">current</entity> <entity id="C94-1062.24">parsing</entity> state, namely the stack and the <entity id="C94-1062.25">input</entity> <entity id="C94-1062.26">string</entity>, and since the <entity id="C94-1062.27">actions</entity> of the <entity id="C94-1062.28">parsing</entity> <entity id="C94-1062.29">tables</entity> are easily attributed <entity id="C94-1062.30">probabilities</entity> conditional on this <entity id="C94-1062.31">parsing</entity> state. LR <entity id="C94-1062.32">parsers</entity> are suitable for the speedup learning <entity id="C94-1062.33">application</entity> since the learneu grammar is much larger than the original grammar, and the <entity id="C94-1062.34">prefixes</entity> of the learned <entity id="C94-1062.35">rules</entity> overlap to a very high <entity id="C94-1062.36">degree</entity>, circumstances that are far from ideal for the <entity id="C94-1062.37">system</entity>'s original <entity id="C94-1062.38">parser</entity>. Even though these ends <entity id="C94-1062.39">influenced</entity> the <entity id="C94-1062.40">design</entity> of the <entity id="C94-1062.41">parser</entity>, this article does not <entity id="C94-1062.42">focus</entity> on these <entity id="C94-1062.43">applications</entity> but rather on the <entity id="C94-1062.44">design</entity> and testing of the <entity id="C94-1062.45">parser</entity> itself.
</abstract>

</text>

<text id="C86-1119">
<title>Towards Discourse-Oriented Nonmonotonic System</title>
<abstract>
The <entity id="C86-1119.1">purpose</entity> of this <entity id="C86-1119.2">paper</entity> is to analyse the <entity id="C86-1119.3">phenomenon</entity> of nonmonotonicity in a <entity id="C86-1119.4">natural language</entity> and to formulate a <entity id="C86-1119.5">number</entity> of general <entity id="C86-1119.6">principles</entity> which should be taken into <entity id="C86-1119.7">consideration</entity> while <entity id="C86-1119.8">constructing</entity> a <entity id="C86-1119.9">discourse</entity> oriented nonmonotonic <entity id="C86-1119.10">formalism</entity>.
</abstract>

</text>

<text id="C88-1029">
<title>Morphology And Cross Dependencies In The Synthesis Of Personal Pronouns In Romance Languages</title>
<abstract>
"This <entity id="C88-1029.1">paper</entity> describes some of the <entity id="C88-1029.2">problems</entity> that arise from the <entity id="C88-1029.3">synthesis</entity> of personal pronouns in a <entity id="C88-1029.4">system</entity> that <entity id="C88-1029.5">generates</entity> <entity id="C88-1029.6">texts</entity> in Romance <entity id="C88-1029.7">languages</entity>. It puts the emphasis first on the fact that the morphological <entity id="C88-1029.8">level</entity> has to be taken into account early in the <entity id="C88-1029.9">generation process</entity>, second on the numerous "<entity id="C88-1029.10">cross</entity> <entity id="C88-1029.11">dependency</entity>" <entity id="C88-1029.12">phenomena</entity> which are to be found when the <entity id="C88-1029.13">synthesis</entity> of an element X depends upon that of another element Y and when the <entity id="C88-1029.14">synthesis</entity> of Y depends upon that of X. The linguistic <entity id="C88-1029.15">examples</entity> are taken from French and Italian <entity id="C88-1029.16">languages</entity>, for which a <entity id="C88-1029.17">robust</entity> <entity id="C88-1029.18">generation system</entity> has been <entity id="C88-1029.19">implemented</entity>."
</abstract>

</text>

<text id="C88-1072">
<title>A News Analysis System</title>
<abstract>
This <entity id="C88-1072.1">paper</entity> describes a <entity id="C88-1072.2">prototype</entity> <entity id="C88-1072.3">news</entity> <entity id="C88-1072.4">analysis</entity> <entity id="C88-1072.5">system</entity> which classifies and <entity id="C88-1072.6">indexes</entity> <entity id="C88-1072.7">news</entity> stories in <entity id="C88-1072.8">real time</entity>. The <entity id="C88-1072.9">system</entity> <entity id="C88-1072.10">extracts</entity> stories from newswire, <entity id="C88-1072.11">parses</entity> the <entity id="C88-1072.12">sentences</entity> of the story, and then <entity id="C88-1072.13">maps</entity> the <entity id="C88-1072.14">syntactic structures</entity> into <entity id="C88-1072.15">concept</entity> <entity id="C88-1072.16">base</entity>. This <entity id="C88-1072.17">process</entity> <entity id="C88-1072.18">results</entity> in an <entity id="C88-1072.19">index</entity> containing both general <entity id="C88-1072.20">categories</entity> and specific <entity id="C88-1072.21">details</entity>. Central to this <entity id="C88-1072.22">system</entity> is a Government-Binding <entity id="C88-1072.23">parser</entity> which <entity id="C88-1072.24">processes</entity> each <entity id="C88-1072.25">sentence</entity> of a <entity id="C88-1072.26">news</entity> <entity id="C88-1072.27">item</entity>. The <entity id="C88-1072.28">system</entity> is completely modular and can be interfaced with different <entity id="C88-1072.29">news</entity> feeds or <entity id="C88-1072.30">concept</entity> <entity id="C88-1072.31">bases</entity>.
</abstract>

</text>

<text id="C88-2085">
<title>Error Diagnosing And Selection In A Training System For Second Language Learning</title>
<abstract>
A diagnosing <entity id="C88-2085.1">procedure</entity> to be used in intelligent <entity id="C88-2085.2">systems</entity> for <entity id="C88-2085.3">language</entity> <entity id="C88-2085.4">instruction</entity> is presented. <entity id="C88-2085.5">Based</entity> on a <entity id="C88-2085.6">knowledge representation</entity> <entity id="C88-2085.7">scheme</entity> for a certain <entity id="C88-2085.8">class</entity> of <entity id="C88-2085.9">syntactic</entity> <entity id="C88-2085.10">correctness</entity> conditions the <entity id="C88-2085.11">system</entity> carries out a thorough <entity id="C88-2085.12">analysis</entity> of possible <entity id="C88-2085.13">error</entity> <entity id="C88-2085.14">hypotheses</entity> and their consequences. A <entity id="C88-2085.15">comparison</entity> with earlier attempts shows a clearly <entity id="C88-2085.16">improved</entity> <entity id="C88-2085.17">precision</entity> of diagnostic <entity id="C88-2085.18">results</entity>. First of all, the <entity id="C88-2085.19">procedure</entity> concentrates on an exact <entity id="C88-2085.20">localization</entity> of <entity id="C88-2085.21">rule</entity> violations, but -if desired - is able to infer <entity id="C88-2085.22">information</entity> about factual faults as well.
</abstract>

</text>

<text id="L08-1560">
<title>Tapping Huge Temporally Indexed Textual Resources with WCTAnalyze</title>
<abstract>
WCTAnalyze is a <entity id="L08-1560.1">tool</entity> for storing, <entity id="L08-1560.2">accessing</entity> and visually analyzing huge <entity id="L08-1560.3">collections</entity> of temporally <entity id="L08-1560.4">indexed</entity><entity id="L08-1560.5">data</entity>. It is motivated by <entity id="L08-1560.6">applications</entity> in media <entity id="L08-1560.7">analysis</entity>, business intelligence etc. where higher <entity id="L08-1560.8">level</entity> <entity id="L08-1560.9">analysis</entity> is <entity id="L08-1560.10">performed</entity> on top of linguistically and statistically <entity id="L08-1560.11">processed</entity> unstructured textual<entity id="L08-1560.12">data</entity>. WCTAnalyze combines fast <entity id="L08-1560.13">access</entity> with economically <entity id="L08-1560.14">storage</entity> <entity id="L08-1560.15">behaviour</entity> and appropriates a lot of built in <entity id="L08-1560.16">visualization</entity> <entity id="L08-1560.17">options</entity> for <entity id="L08-1560.18">result</entity> <entity id="L08-1560.19">presentation</entity> in <entity id="L08-1560.20">detail</entity> as well as in <entity id="L08-1560.21">contrast</entity>. So it enables an efficient and effective way to explore chronological <entity id="L08-1560.22">text</entity> <entity id="L08-1560.23">patterns</entity> of <entity id="L08-1560.24">word</entity> <entity id="L08-1560.25">forms</entity>, their <entity id="L08-1560.26">co-occurrence</entity> sets and <entity id="L08-1560.27">co-occurrence</entity> set <entity id="L08-1560.28">intersections</entity>. Digging deep into co-occurrences of the same <entity id="L08-1560.29">semantic</entity> or <entity id="L08-1560.30">syntactic</entity> describing wordforms, some <entity id="L08-1560.31">entities</entity> can be recognized as to be temporal related, whereas other differ significantly. This <entity id="L08-1560.32">behaviour</entity> motivates <entity id="L08-1560.33">approaches</entity> in interactive discovering <entity id="L08-1560.34">events</entity> <entity id="L08-1560.35">based</entity> on <entity id="L08-1560.36">co-occurrence</entity> subsets.
</abstract>

</text>

<text id="L08-1581">
<title>Linguistic Structure and Bilingual Informants Help Induce Machine Translation of Lesser-Resourced Languages</title>
<abstract>
Producing <entity id="L08-1581.1">machine translation</entity> (MT) for the many minority <entity id="L08-1581.2">languages</entity> in the world is a serious <entity id="L08-1581.3">challenge</entity>. Minority <entity id="L08-1581.4">languages</entity> typically have few <entity id="L08-1581.5">resources</entity> for <entity id="L08-1581.6">building</entity> <entity id="L08-1581.7">MT systems</entity>. For many minor <entity id="L08-1581.8">languages</entity> there is little <entity id="L08-1581.9">machine</entity> readable <entity id="L08-1581.10">text</entity>, few knowledgeable <entity id="L08-1581.11">linguists</entity>, and little money available for MT <entity id="L08-1581.12">development</entity>. For these <entity id="L08-1581.13">reasons</entity>, our <entity id="L08-1581.14">research</entity> <entity id="L08-1581.15">programs</entity> on minority <entity id="L08-1581.16">language</entity> MT have <entity id="L08-1581.17">focused</entity> on <entity id="L08-1581.18">leveraging</entity> to the maximum <entity id="L08-1581.19">extent</entity> two <entity id="L08-1581.20">resources</entity> that are available for minority <entity id="L08-1581.21">languages</entity>: <entity id="L08-1581.22">linguistic structure</entity> and bilingual informants. All <entity id="L08-1581.23">natural languages</entity> contain <entity id="L08-1581.24">linguistic structure</entity>. And although the <entity id="L08-1581.25">details</entity> of that <entity id="L08-1581.26">linguistic structure</entity> vary from <entity id="L08-1581.27">language</entity> to <entity id="L08-1581.28">language</entity>, <entity id="L08-1581.29">language</entity> universals such as <entity id="L08-1581.30">context-free</entity> <entity id="L08-1581.31">syntactic structure</entity> and the paradigmatic <entity id="L08-1581.32">structure</entity> of inflectional <entity id="L08-1581.33">morphology</entity>, allow us to learn the specific <entity id="L08-1581.34">details</entity> of a minority <entity id="L08-1581.35">language</entity>. Similarly, most minority <entity id="L08-1581.36">languages</entity> possess speakers who are bilingual with the major <entity id="L08-1581.37">language</entity> of the <entity id="L08-1581.38">area</entity>. This <entity id="L08-1581.39">paper</entity> discusses our <entity id="L08-1581.40">efforts</entity> to utilize <entity id="L08-1581.41">linguistic structure</entity> and the <entity id="L08-1581.42">translation</entity> <entity id="L08-1581.43">information</entity> that bilingual informants can <entity id="L08-1581.44">provide</entity> in three sub-areas of our rapid <entity id="L08-1581.45">development</entity> MT <entity id="L08-1581.46">program</entity>: <entity id="L08-1581.47">morphology</entity> <entity id="L08-1581.48">induction</entity>, <entity id="L08-1581.49">syntactic</entity> <entity id="L08-1581.50">transfer</entity> <entity id="L08-1581.51">rule</entity> <entity id="L08-1581.52">learning</entity>, and <entity id="L08-1581.53">refinement</entity> of imperfect learned <entity id="L08-1581.54">rules</entity>.
</abstract>

</text>

<text id="D08-1082">
<title>A Generative Model for Parsing Natural Language to Meaning Representations</title>
<abstract>
In this <entity id="D08-1082.1">paper</entity>, we present an <entity id="D08-1082.2">algorithm</entity> for learning a <entity id="D08-1082.3">generative model</entity> of <entity id="D08-1082.4">natural language sentences</entity> together with their formal <entity id="D08-1082.5">meaning representations</entity> with <entity id="D08-1082.6">hierarchical structures</entity>. The <entity id="D08-1082.7">model</entity> is <entity id="D08-1082.8">applied</entity> to the <entity id="D08-1082.9">task</entity> of <entity id="D08-1082.10">mapping</entity> <entity id="D08-1082.11">sentences</entity> to hierarchical <entity id="D08-1082.12">representations</entity> of their underlying meaning. We introduce <entity id="D08-1082.13">dynamic programming</entity> <entity id="D08-1082.14">techniques</entity> for efficient <entity id="D08-1082.15">training</entity> and decoding. In <entity id="D08-1082.16">experiments</entity>, we demonstrate that the <entity id="D08-1082.17">model</entity>, when coupled with a discriminative reranking <entity id="D08-1082.18">technique</entity>, achieves state-of-the-art <entity id="D08-1082.19">performance</entity> when <entity id="D08-1082.20">tested</entity> on two publicly available <entity id="D08-1082.21">corpora</entity>. The <entity id="D08-1082.22">generative model</entity> degrades robustly when presented with <entity id="D08-1082.23">instances</entity> that are different from those seen in <entity id="D08-1082.24">training</entity>. This allows a notable <entity id="D08-1082.25">improvement</entity> in <entity id="D08-1082.26">recall</entity> compared to previous <entity id="D08-1082.27">models</entity>.
</abstract>

</text>

<text id="L08-1118">
<title>A Comparative Study on Language Identification Methods</title>
<abstract>
In this <entity id="L08-1118.1">paper</entity> we present two <entity id="L08-1118.2">experiments</entity> conducted for <entity id="L08-1118.3">comparison</entity> of different <entity id="L08-1118.4">language</entity> <entity id="L08-1118.5">identification</entity> <entity id="L08-1118.6">algorithms</entity>. Short <entity id="L08-1118.7">words-</entity>, frequent <entity id="L08-1118.8">words-</entity> and <entity id="L08-1118.9">n-gram-based approaches</entity> are considered and combined with the Ad-Hoc Ranking <entity id="L08-1118.10">classification</entity> <entity id="L08-1118.11">method</entity>. The <entity id="L08-1118.12">language</entity> <entity id="L08-1118.13">identification</entity> <entity id="L08-1118.14">process</entity> can be subdivided into two <entity id="L08-1118.15">main</entity> <entity id="L08-1118.16">steps</entity>: First a <entity id="L08-1118.17">document</entity> <entity id="L08-1118.18">model</entity> is <entity id="L08-1118.19">generated</entity> for the <entity id="L08-1118.20">document</entity> and a <entity id="L08-1118.21">language model</entity> for the <entity id="L08-1118.22">language</entity>; second the <entity id="L08-1118.23">language</entity> of the <entity id="L08-1118.24">document</entity> is determined on the <entity id="L08-1118.25">basis</entity> of the <entity id="L08-1118.26">language model</entity> and is added to the <entity id="L08-1118.27">document</entity> as additional <entity id="L08-1118.28">information</entity>. In this work we present our <entity id="L08-1118.29">evaluation results</entity> and discuss the <entity id="L08-1118.30">importance</entity> of a dynamic value for the out-of-place measure.
</abstract>

</text>

<text id="E03-1070">
<title>QUALIFIER: Question Answering By Lexical Fabric And External Resources</title>
<abstract>
One of the major <entity id="E03-1070.1">challenges</entity> in TREC-style <entity id="E03-1070.2">question-answering</entity> (QA) is to overcome the <entity id="E03-1070.3">mismatch</entity> in the <entity id="E03-1070.4">lexical</entity> <entity id="E03-1070.5">representations</entity> in the <entity id="E03-1070.6">query</entity> <entity id="E03-1070.7">space</entity> and <entity id="E03-1070.8">document</entity> <entity id="E03-1070.9">space</entity>. This is particularly severe in QA as exact answers, rather than <entity id="E03-1070.10">documents</entity>, are <entity id="E03-1070.11">required</entity> in response to <entity id="E03-1070.12">questions</entity>. Most <entity id="E03-1070.13">current</entity> <entity id="E03-1070.14">approaches</entity> overcome the <entity id="E03-1070.15">mismatch</entity> <entity id="E03-1070.16">problem</entity> by employing either<entity id="E03-1070.17">data</entity> <entity id="E03-1070.18">redundancy</entity> <entity id="E03-1070.19">strategy</entity> through the use of Web or <entity id="E03-1070.20">linguistic resources</entity>. This <entity id="E03-1070.21">paper</entity> investigates the <entity id="E03-1070.22">integration</entity> of <entity id="E03-1070.23">lexical</entity> <entity id="E03-1070.24">relations</entity> and Web <entity id="E03-1070.25">knowledge</entity> to tackle this <entity id="E03-1070.26">problem</entity>. The <entity id="E03-1070.27">results</entity> obtained on TREC11 QA <entity id="E03-1070.28">corpus</entity> indicate that our <entity id="E03-1070.29">approach</entity> is both feasible and effective.
</abstract>

</text>

<text id="C96-2174">
<title>CALL: The Potential Of LINGWARE And The Use Of Empirical Linguistic Data</title>
<abstract><entity id="C96-2174.1">Language technology</entity> has significantly evolved during the last decade. However, the <entity id="C96-2174.2">community</entity> of <entity id="C96-2174.3">language</entity> learning seems to ignore this <entity id="C96-2174.4">development</entity>, most of the existing <entity id="C96-2174.5">language</entity> learning <entity id="C96-2174.6">systems</entity> drawing their <entity id="C96-2174.7">enhancements</entity> from other <entity id="C96-2174.8">sources</entity>, such as <entity id="C96-2174.9">hypertext</entity>, multimedia, interactive <entity id="C96-2174.10">video</entity>, <entity id="C96-2174.11">information retrieval</entity>. Despite some spectacular <entity id="C96-2174.12">progress</entity> made at the <entity id="C96-2174.13">level</entity> of <entity id="C96-2174.14">interface</entity>, several fundamental <entity id="C96-2174.15">language</entity> learning <entity id="C96-2174.16">principles</entity>, are only partially met. Nevertheless, the hypermedia <entity id="C96-2174.17">technology</entity> did <entity id="C96-2174.18">solve</entity> one very important <entity id="C96-2174.19">aspect</entity> of <entity id="C96-2174.20">computer-assisted</entity> <entity id="C96-2174.21">learning</entity> by putting the student in a visual <entity id="C96-2174.22">environment</entity>. Minimizing cultural <entity id="C96-2174.23">differences</entity> they've been able to draw on shared <entity id="C96-2174.24">background knowledge</entity> (microworld immersiveness). Other important <entity id="C96-2174.25">aspects</entity> of typical <entity id="C96-2174.26">immersion-based approaches</entity>, i.e. <entity id="C96-2174.27">natural</entity> <entity id="C96-2174.28">learning</entity>, such as <entity id="C96-2174.29">mixed-initiative</entity>, fault-tolerance, <entity id="C96-2174.30">dialogue</entity> <entity id="C96-2174.31">repair</entity>, cooperative <entity id="C96-2174.32">behaviour</entity>, etc. are still in their infancy. In real settings learners freely interact with their <entity id="C96-2174.33">environment</entity> (parents, tutors), taking turns, asking for <entity id="C96-2174.34">explanations</entity>, <entity id="C96-2174.35">shifting</entity> <entity id="C96-2174.36">topics</entity>, etc. The <entity id="C96-2174.37">language</entity> produced by the learner is more often than not agrammatical, yet this does not prevent the tutor to proceed with the <entity id="C96-2174.38">dialog</entity>. <entity id="C96-2174.39">Error</entity> <entity id="C96-2174.40">correction</entity> is usually done contextually, by drawing either explicitly attention to the <entity id="C96-2174.41">deviation</entity>, by producing a similar but correct <entity id="C96-2174.42">sentence</entity>, or by simply ignoring the mistake leaving its <entity id="C96-2174.43">correction</entity> for later. There are many AI and CL <entity id="C96-2174.44">programs</entity> <entity id="C96-2174.45">solving</entity> various specific <entity id="C96-2174.46">CALL-relevant</entity> <entity id="C96-2174.47">problems</entity>. If assembled properly, these pieces could <entity id="C96-2174.48">result</entity> in very powerful <entity id="C96-2174.49">language</entity> learning <entity id="C96-2174.50">systems</entity>. <entity id="C96-2174.51">Lexical</entity> <entity id="C96-2174.52">thesauri</entity> Parsers Generators <entity id="C96-2174.53">Semantic</entity> <entity id="C96-2174.54">interpreters</entity>/<entity id="C96-2174.55">generators</entity></abstract>

</text>

<text id="C00-2102">
<title>Named Entity Chunking Techniques In Supervised Learning For Japanese Named Entity Recognition</title>
<abstract>
This <entity id="C00-2102.1">paper</entity> <entity id="C00-2102.2">focuses</entity> on the <entity id="C00-2102.3">issue</entity> of <entity id="C00-2102.4">named</entity> <entity id="C00-2102.5">entity</entity> <entity id="C00-2102.6">chunking</entity> in <entity id="C00-2102.7">Japanese</entity> <entity id="C00-2102.8">named</entity> <entity id="C00-2102.9">entity</entity> <entity id="C00-2102.10">recognition</entity>. We <entity id="C00-2102.11">apply</entity> the supervised <entity id="C00-2102.12">decision</entity> <entity id="C00-2102.13">list</entity> learning <entity id="C00-2102.14">method</entity> to <entity id="C00-2102.15">Japanese</entity> <entity id="C00-2102.16">named</entity> <entity id="C00-2102.17">entity</entity> <entity id="C00-2102.18">recognition</entity>. We also investigate and incorporate several <entity id="C00-2102.19">named-entity</entity> <entity id="C00-2102.20">noun phrase</entity> <entity id="C00-2102.21">chunking</entity> <entity id="C00-2102.22">techniques</entity> and experimentally <entity id="C00-2102.23">evaluate</entity> and compare their <entity id="C00-2102.24">performance</entity>. In <entity id="C00-2102.25">addition</entity>, we <entity id="C00-2102.26">propose</entity> a <entity id="C00-2102.27">method</entity> for incorporating richer <entity id="C00-2102.28">contextual information</entity> as well as <entity id="C00-2102.29">patterns</entity> of <entity id="C00-2102.30">constituent</entity> morphemes within a <entity id="C00-2102.31">named</entity> <entity id="C00-2102.32">entity</entity>, which have not been considered in previous <entity id="C00-2102.33">research</entity>, and show that the <entity id="C00-2102.34">proposed</entity> <entity id="C00-2102.35">method</entity> outperforms these previous <entity id="C00-2102.36">approaches</entity>.
</abstract>

</text>

<text id="C02-1067">
<title>An Inference-Based Approach To Dialogue System Design</title>
<abstract>
We present an <entity id="C02-1067.1">architecture</entity> for spoken <entity id="C02-1067.2">dialogue systems</entity> where <entity id="C02-1067.3">first-order</entity> <entity id="C02-1067.4">inference</entity> (both <entity id="C02-1067.5">theorem</entity> proving and <entity id="C02-1067.6">model</entity> <entity id="C02-1067.7">building</entity>) plays a crucial <entity id="C02-1067.8">role</entity> in interpreting <entity id="C02-1067.9">utterances</entity> of <entity id="C02-1067.10">dialogue</entity> <entity id="C02-1067.11">participants</entity> and deciding how the <entity id="C02-1067.12">system</entity> should respond and carry out <entity id="C02-1067.13">instructions</entity>. The <entity id="C02-1067.14">dialogue</entity> itself is represented <entity id="C02-1067.15">SIS</entity> St DRS which is <entity id="C02-1067.16">translated</entity> into <entity id="C02-1067.17">first-order</entity> <entity id="C02-1067.18">logic</entity> for <entity id="C02-1067.19">inference</entity> <entity id="C02-1067.20">tasks</entity>. The <entity id="C02-1067.21">system</entity> is <entity id="C02-1067.22">implemented</entity> <entity id="C02-1067.23">SIS</entity> St society of OAA-agents, and <entity id="C02-1067.24">evaluated</entity> against a specific <entity id="C02-1067.25">application</entity> (home <entity id="C02-1067.26">automation</entity>).
</abstract>

</text>

<text id="C02-1095">
<title>Compilation Of Unification Grammars With Compositional Semantics To Speech Recognition Packages</title>
<abstract>
In this <entity id="C02-1095.1">paper</entity> a <entity id="C02-1095.2">method</entity> to compile <entity id="C02-1095.3">unification</entity> grammars into <entity id="C02-1095.4">speech recognition</entity> <entity id="C02-1095.5">packages</entity> is presented, and in particular, <entity id="C02-1095.6">rules</entity> are specified to <entity id="C02-1095.7">transfer</entity> the compositional <entity id="C02-1095.8">semantics</entity> stated in <entity id="C02-1095.9">unification</entity> grammars into <entity id="C02-1095.10">speech recognition</entity> grammars. The <entity id="C02-1095.11">resulting</entity> compiler creates a <entity id="C02-1095.12">context-free</entity> <entity id="C02-1095.13">backbone</entity> of the <entity id="C02-1095.14">unification</entity> grammar, eliminates left-recursive productions and removes redundant <entity id="C02-1095.15">grammar rules</entity>. The <entity id="C02-1095.16">method</entity> was <entity id="C02-1095.17">tested</entity> on a medium-sized <entity id="C02-1095.18">unification</entity> grammar for <entity id="C02-1095.19">English</entity> using Nuance <entity id="C02-1095.20">speech recognition</entity> <entity id="C02-1095.21">software</entity> on a <entity id="C02-1095.22">corpus</entity> of 131 <entity id="C02-1095.23">utterances</entity> of 12 different speakers. <entity id="C02-1095.24">Results</entity> showed no significant <entity id="C02-1095.25">computational</entity> overhead with <entity id="C02-1095.26">respect</entity> to <entity id="C02-1095.27">speech recognition</entity> <entity id="C02-1095.28">performances</entity> for <entity id="C02-1095.29">speech recognition</entity> grammar with compositional <entity id="C02-1095.30">semantics</entity> compared to grammars without.
</abstract>

</text>

<text id="C04-1115">
<title>Feature Weighting For Co-Occurrence-Based Classification Of Words</title>
<abstract>
The <entity id="C04-1115.1">paper</entity> comparatively <entity id="C04-1115.2">studies</entity> <entity id="C04-1115.3">methods</entity> of <entity id="C04-1115.4">feature</entity> <entity id="C04-1115.5">weighting</entity> in <entity id="C04-1115.6">application</entity> to the <entity id="C04-1115.7">task</entity> of cooccurrence-based <entity id="C04-1115.8">classification</entity> of <entity id="C04-1115.9">words</entity> according to their meaning. We explore <entity id="C04-1115.10">parameter</entity> <entity id="C04-1115.11">optimization</entity> of several <entity id="C04-1115.12">weighting</entity> <entity id="C04-1115.13">methods</entity> frequently used for similar <entity id="C04-1115.14">problems</entity> such as <entity id="C04-1115.15">text classification</entity>. We find that successful <entity id="C04-1115.16">application</entity> of all the <entity id="C04-1115.17">methods</entity> crucially depends on a <entity id="C04-1115.18">number</entity> of <entity id="C04-1115.19">parameters</entity>; only a carefully chosen <entity id="C04-1115.20">weighting</entity> <entity id="C04-1115.21">procedure</entity> allows to obtain consistent <entity id="C04-1115.22">improvement</entity> on a <entity id="C04-1115.23">classifier</entity> learned from non-weighted<entity id="C04-1115.24">data</entity>.
</abstract>

</text>

<text id="C04-1130">
<title>Trajectory Based Word Sense Disambiguation</title>
<abstract><entity id="C04-1130.1">Classifier</entity> <entity id="C04-1130.2">combination</entity> is a promising way to <entity id="C04-1130.3">improve</entity> <entity id="C04-1130.4">performance</entity> of <entity id="C04-1130.5">word sense disambiguation</entity>. We <entity id="C04-1130.6">propose</entity> a new combinational <entity id="C04-1130.7">method</entity> in this <entity id="C04-1130.8">paper</entity>. We first <entity id="C04-1130.9">construct</entity> a <entity id="C04-1130.10">series</entity> of Na
ve Bayesian <entity id="C04-1130.11">classifiers</entity> along a <entity id="C04-1130.12">sequence</entity> of orderly varying <entity id="C04-1130.13">sized</entity> <entity id="C04-1130.14">windows</entity> of <entity id="C04-1130.15">context</entity>, and <entity id="C04-1130.16">perform</entity> <entity id="C04-1130.17">sense</entity> <entity id="C04-1130.18">selection</entity> for both <entity id="C04-1130.19">training</entity> <entity id="C04-1130.20">samples</entity> and <entity id="C04-1130.21">test</entity> <entity id="C04-1130.22">samples</entity> using these <entity id="C04-1130.23">classifiers</entity>. We thus get a <entity id="C04-1130.24">sense</entity> <entity id="C04-1130.25">selection</entity> trajectory along the <entity id="C04-1130.26">sequence</entity> of <entity id="C04-1130.27">context</entity> <entity id="C04-1130.28">windows</entity> for each <entity id="C04-1130.29">sample</entity>. Then we make use of these trajectories to make final k-nearest-neighbors-based <entity id="C04-1130.30">sense</entity> <entity id="C04-1130.31">selection</entity> for <entity id="C04-1130.32">test</entity> <entity id="C04-1130.33">samples</entity>. This <entity id="C04-1130.34">method</entity> aims to lower the <entity id="C04-1130.35">uncertainty</entity> brought by <entity id="C04-1130.36">classifiers</entity> using different <entity id="C04-1130.37">context</entity> <entity id="C04-1130.38">windows</entity> and make more <entity id="C04-1130.39">robust</entity> utilization of <entity id="C04-1130.40">context</entity> while <entity id="C04-1130.41">perform</entity> well. <entity id="C04-1130.42">Experiments</entity> show that our <entity id="C04-1130.43">approach</entity> outperforms some other <entity id="C04-1130.44">algorithms</entity> on both <entity id="C04-1130.45">robustness</entity> and <entity id="C04-1130.46">performance</entity>.
</abstract>

</text>

<text id="C04-1133">
<title>Automated Induction Of Sense In Context</title>
<abstract>
In this <entity id="C04-1133.1">paper</entity>, we introduce a <entity id="C04-1133.2">model</entity> for <entity id="C04-1133.3">sense</entity> <entity id="C04-1133.4">assignment</entity> which relies on assigning senses to the <entity id="C04-1133.5">contexts</entity> within which <entity id="C04-1133.6">words</entity> appear, rather than to the <entity id="C04-1133.7">words</entity> themselves. We argue that <entity id="C04-1133.8">word</entity> senses as such are not directly encoded in the <entity id="C04-1133.9">lexicon</entity> of the <entity id="C04-1133.10">language</entity>. Rather, each <entity id="C04-1133.11">word</entity> is associated with one or more stereotypical syntagmatic <entity id="C04-1133.12">patterns</entity>, which we <entity id="C04-1133.13">call</entity> <entity id="C04-1133.14">selection</entity> <entity id="C04-1133.15">contexts</entity>.
</abstract>

</text>

<text id="E87-1041">
<title>Situations And Prepositional Phrases</title>
<abstract>
This <entity id="E87-1041.1">paper</entity> presents a <entity id="E87-1041.2">format</entity> for representing the linguistic <entity id="E87-1041.3">form</entity> of <entity id="E87-1041.4">utterances</entity>, <entity id="E87-1041.5">called</entity> <entity id="E87-1041.6">situation</entity> <entity id="E87-1041.7">schemata</entity>, which is rooted in the <entity id="E87-1041.8">situation</entity> <entity id="E87-1041.9">semantics</entity> of Barwise and Perry. A <entity id="E87-1041.10">treatment</entity> of locative <entity id="E87-1041.11">prepositional phrases</entity> is given, thus illustrating the <entity id="E87-1041.12">generation</entity> of the <entity id="E87-1041.13">situation</entity> <entity id="E87-1041.14">schemata</entity> and their <entity id="E87-1041.15">interpretation</entity> in <entity id="E87-1041.16">situation</entity> <entity id="E87-1041.17">semantics</entity>. 
</abstract>

</text>

<text id="L08-1223">
<title>The U.S. Policy Agenda Legislation Corpus Volume 1 - a Language Resource from 1947 - 1998</title>
<abstract>
We introduce the <entity id="L08-1223.1">corpus</entity> of United States Congressional bills from 1947 to 1998 for use by <entity id="L08-1223.2">language</entity> <entity id="L08-1223.3">research</entity> <entity id="L08-1223.4">communities</entity>. The U.S. Policy <entity id="L08-1223.5">Agenda</entity> Legislation <entity id="L08-1223.6">Corpus</entity> <entity id="L08-1223.7">Volume</entity> 1 (USPALCV1) <entity id="L08-1223.8">includes</entity> more than 375,000 legislative bills annotated with a hierarchical policy <entity id="L08-1223.9">area</entity> <entity id="L08-1223.10">category</entity>. The human annotations in USPALCV1 have been reliably <entity id="L08-1223.11">applied</entity> over <entity id="L08-1223.12">time</entity> to enable social <entity id="L08-1223.13">science</entity> <entity id="L08-1223.14">analysis</entity> of legislative <entity id="L08-1223.15">trends</entity>. The <entity id="L08-1223.16">corpus</entity> is a member of an emerging family of <entity id="L08-1223.17">corpora</entity> that are annotated by policy <entity id="L08-1223.18">area</entity> to enable comparative parallel <entity id="L08-1223.19">trend</entity> <entity id="L08-1223.20">recognition</entity> across countries and <entity id="L08-1223.21">domains</entity> (legislation, political <entity id="L08-1223.22">speeches</entity>, newswire articles, budgetary expenditures, <entity id="L08-1223.23">web sites</entity>, etc.). This <entity id="L08-1223.24">paper</entity> describes the origins of the <entity id="L08-1223.25">corpus</entity>, its <entity id="L08-1223.26">creation</entity>, ways to <entity id="L08-1223.27">access</entity> it, <entity id="L08-1223.28">design</entity> <entity id="L08-1223.29">criteria</entity>, and an <entity id="L08-1223.30">analysis</entity> with <entity id="L08-1223.31">common</entity> supervised <entity id="L08-1223.32">machine</entity> learning <entity id="L08-1223.33">methods</entity>. The use of <entity id="L08-1223.34">machine</entity> learning <entity id="L08-1223.35">methods</entity> establishes a baseline <entity id="L08-1223.36">proposed</entity> <entity id="L08-1223.37">modeling</entity> for the <entity id="L08-1223.38">topic</entity> <entity id="L08-1223.39">classification</entity> of legal <entity id="L08-1223.40">documents</entity>.
</abstract>

</text>

<text id="L08-1341">
<title>JMWNL: an Extensible Multilingual Library for Accessing Wordnets in Different Languages</title>
<abstract>
In this <entity id="L08-1341.1">paper</entity> we present JMWNL, a multilingual <entity id="L08-1341.2">extension</entity> of the JWNL java <entity id="L08-1341.3">library</entity>, which was originally <entity id="L08-1341.4">developed</entity> for <entity id="L08-1341.5">accessing</entity> Princeton WordNet <entity id="L08-1341.6">dictionaries</entity>. JMWNL broadens the range of JWNL's accessible <entity id="L08-1341.7">resources</entity> by covering also <entity id="L08-1341.8">dictionaries</entity> produced inside the EuroWordNet <entity id="L08-1341.9">project</entity>. Specific <entity id="L08-1341.10">resources</entity>, such as <entity id="L08-1341.11">language-dependent</entity> algorithmic stemmers, have been adopted to cover the <entity id="L08-1341.12">diversities</entity> in the morphological <entity id="L08-1341.13">nature</entity> of <entity id="L08-1341.14">words</entity> in the addressed <entity id="L08-1341.15">idioms</entity>. New <entity id="L08-1341.16">semantic</entity> and <entity id="L08-1341.17">lexical</entity> <entity id="L08-1341.18">relations</entity> have been <entity id="L08-1341.19">included</entity> to maximize <entity id="L08-1341.20">compatibility</entity> with new <entity id="L08-1341.21">versions</entity> of the original Princeton WordNet and to <entity id="L08-1341.22">include</entity> the whole range of <entity id="L08-1341.23">relations</entity> from EuroWordNet. <entity id="L08-1341.24">Relations</entity> from Princeton WordNet on one <entity id="L08-1341.25">side</entity> and EuroWordNet on the other one have in some <entity id="L08-1341.26">cases</entity> been <entity id="L08-1341.27">mapped</entity> to <entity id="L08-1341.28">provide</entity> a uniform <entity id="L08-1341.29">reference</entity> for coherent <entity id="L08-1341.30">cross-linguistic</entity> use of the <entity id="L08-1341.31">library</entity>.
</abstract>

</text>

<text id="L08-1386">
<title>Methodology for Evaluating the Usability of User Interfaces in Mobile Services</title>
<abstract>
In this <entity id="L08-1386.1">paper</entity> we present a usability measure <entity id="L08-1386.2">adapted</entity> to mobile services, which is <entity id="L08-1386.3">based</entity> on the well-known theoretical <entity id="L08-1386.4">framework</entity> defined in the ISO 9241-11 [ISO 9241 (1988)] <entity id="L08-1386.5">standard</entity>. This measure is then <entity id="L08-1386.6">applied</entity> to a <entity id="L08-1386.7">representative</entity> set of services of the Telefonica's portfolio for residential <entity id="L08-1386.8">customers</entity>. The <entity id="L08-1386.9">user</entity> <entity id="L08-1386.10">tests</entity> that we present were carried out by a total of 327 people. Additionally, in <entity id="L08-1386.11">section</entity> 3 we describe the <entity id="L08-1386.12">application</entity> of the <entity id="L08-1386.13">methodology</entity> to a particular service and <entity id="L08-1386.14">section</entity> 4 presents the <entity id="L08-1386.15">results</entity> of the <entity id="L08-1386.16">experiments</entity>. These <entity id="L08-1386.17">results</entity> show highly significant <entity id="L08-1386.18">differences</entity> in the three usability measures considered, though all of them have the same <entity id="L08-1386.19">trend</entity>. The worst performers in all <entity id="L08-1386.20">cases</entity> were the WAP and <entity id="L08-1386.21">i-mode</entity> <entity id="L08-1386.22">user interfaces</entity> (UI), while the best performers were the SMS and web <entity id="L08-1386.23">based</entity> UIs closely followed by the voice UI. Finally, in <entity id="L08-1386.24">section</entity> 5 we analyse the <entity id="L08-1386.25">results</entity> and present our <entity id="L08-1386.26">conclusions</entity>.
</abstract>

</text>

<text id="L08-1423">
<title>The 2008 Oriental COCOSDA Book Project: in Commemoration of the First Decade of Sustained Activities in Asia</title>
<abstract>
The <entity id="L08-1423.1">purpose</entity> of Oriental COCOSDA is to <entity id="L08-1423.2">provide</entity> the Asian <entity id="L08-1423.3">community</entity> a <entity id="L08-1423.4">platform</entity> to exchange ideas, to share <entity id="L08-1423.5">information</entity> and to discuss regional matters on <entity id="L08-1423.6">creation</entity>, utilization, dissemination of spoken <entity id="L08-1423.7">language</entity> <entity id="L08-1423.8">corpora</entity> of oriental <entity id="L08-1423.9">languages</entity> and also on the <entity id="L08-1423.10">assessment</entity> <entity id="L08-1423.11">methods</entity> of <entity id="L08-1423.12">speech recognition</entity>/<entity id="L08-1423.13">synthesis</entity> <entity id="L08-1423.14">systems</entity> as well as to promote <entity id="L08-1423.15">speech</entity> <entity id="L08-1423.16">research</entity> on oriental <entity id="L08-1423.17">languages</entity>. Since its preparatory <entity id="L08-1423.18">meeting</entity> in <entity id="L08-1423.19">Hong</entity> Kong in 1997, annual <entity id="L08-1423.20">workshops</entity> have been organized and held in Japan, Taiwan, China, Korea, Thailand, Singapore, India, Indonesia, Malaysia, and Vietnam from 1998. The <entity id="L08-1423.21">organization</entity> is managed by a convener, three advisory members, and 26 committee members from 13 <entity id="L08-1423.22">regions</entity> in Oriental <entity id="L08-1423.23">area</entity>. In <entity id="L08-1423.24">order</entity> to commemorate 10 years of continued activities, the members have decided to publish a book which covers a wide range of <entity id="L08-1423.25">speech</entity> <entity id="L08-1423.26">research</entity>. Special <entity id="L08-1423.27">focus</entity> will be on <entity id="L08-1423.28">speech</entity> <entity id="L08-1423.29">resources</entity> or <entity id="L08-1423.30">speech corpora</entity> in Oriental countries and standardization of <entity id="L08-1423.31">speech</entity> <entity id="L08-1423.32">input</entity>/<entity id="L08-1423.33">output</entity> <entity id="L08-1423.34">systems</entity> <entity id="L08-1423.35">performance</entity> <entity id="L08-1423.36">evaluation methods</entity> on which key <entity id="L08-1423.37">technologies</entity> for <entity id="L08-1423.38">speech</entity> <entity id="L08-1423.39">systems</entity> <entity id="L08-1423.40">development</entity> are <entity id="L08-1423.41">based</entity>. The book will also <entity id="L08-1423.42">include</entity> linguistic <entity id="L08-1423.43">outlines</entity> of oriental <entity id="L08-1423.44">languages</entity>, annotation, labeling, and <entity id="L08-1423.45">software</entity> <entity id="L08-1423.46">tools</entity> for <entity id="L08-1423.47">speech processing</entity>.
</abstract>

</text>

<text id="C96-1068">
<title>A CD-ROM Retrieval System With Multiple Dialogue Agents</title>
<abstract>
In this <entity id="C96-1068.1">paper</entity>, we <entity id="C96-1068.2">proposed</entity> a new <entity id="C96-1068.3">dialogue system</entity> with multiple <entity id="C96-1068.4">dialogue</entity> <entity id="C96-1068.5">agents</entity>. In our new <entity id="C96-1068.6">system</entity>, three <entity id="C96-1068.7">types</entity> of <entity id="C96-1068.8">agents</entity>: a) <entity id="C96-1068.9">domain</entity> <entity id="C96-1068.10">agents</entity>, b) <entity id="C96-1068.11">strategy</entity> <entity id="C96-1068.12">agents</entity>, and c) <entity id="C96-1068.13">context</entity> <entity id="C96-1068.14">agents</entity> were realized. They give the following <entity id="C96-1068.15">advantages</entity> to the <entity id="C96-1068.16">user</entity>: 
 the <entity id="C96-1068.17">domain</entity> <entity id="C96-1068.18">agents</entity> make the <entity id="C96-1068.19">user</entity> aware of the <entity id="C96-1068.20">boundary</entity> between the <entity id="C96-1068.21">domains</entity>. 
 the <entity id="C96-1068.22">strategy</entity> <entity id="C96-1068.23">agents</entity> make the <entity id="C96-1068.24">user</entity> aware of the <entity id="C96-1068.25">difference</entity> between the <entity id="C96-1068.26">strategies</entity>. 
 the <entity id="C96-1068.27">context</entity> <entity id="C96-1068.28">agents</entity> <entity id="C96-1068.29">help</entity> the <entity id="C96-1068.30">user</entity> to <entity id="C96-1068.31">deal</entity> with multiple <entity id="C96-1068.32">goals</entity>. We expect that the <entity id="C96-1068.33">complex</entity> <entity id="C96-1068.34">behaviors</entity> of the <entity id="C96-1068.35">system</entity> will become more visible to the <entity id="C96-1068.36">user</entity> in different <entity id="C96-1068.37">situations</entity>. The <entity id="C96-1068.38">experimental</entity> <entity id="C96-1068.39">results</entity> show that the <entity id="C96-1068.40">user</entity> can retrieve effectively and obtain the expected <entity id="C96-1068.41">goals</entity> easily by using these multiple <entity id="C96-1068.42">agents</entity>.
</abstract>

</text>

<text id="C96-2117">
<title>Computation Of Relative Social Status On The Basis Of Honorification In Korean</title>
<abstract>
This <entity id="C96-2117.1">paper</entity> presents a way to <entity id="C96-2117.2">compute</entity> <entity id="C96-2117.3">relative</entity> social <entity id="C96-2117.4">status</entity> of the <entity id="C96-2117.5">individuals</entity> involved in Korean <entity id="C96-2117.6">dialogue</entity>. Every Korean <entity id="C96-2117.7">sentence</entity> indicates whether honorification occurs in it. The <entity id="C96-2117.8">occurrence</entity> of honorification in a <entity id="C96-2117.9">sentence</entity> is constr ained by <entity id="C96-2117.10">relative</entity> social <entity id="C96-2117.11">status</entity> of the <entity id="C96-2117.12">individuals</entity> involved in the <entity id="C96-2117.13">sentence</entity>. By using the <entity id="C96-2117.14">information</entity> about social <entity id="C96-2117.15">status</entity> and the <entity id="C96-2117.16">information</entity> about <entity id="C96-2117.17">sentence-external</entity> <entity id="C96-2117.18">individuals</entity> such as speaker and addressee, we can explain why a <entity id="C96-2117.19">sentence</entity> is felicitous in a restricted <entity id="C96-2117.20">context</entity> and whether a <entity id="C96-2117.21">dialogue</entity> is coherent or not. Since it is possible and easy to <entity id="C96-2117.22">include</entity> such <entity id="C96-2117.23">contextual information</entity> in the HPSG <entity id="C96-2117.24">formalism</entity>, that <entity id="C96-2117.25">formalism</entity> is adopted here. The <entity id="C96-2117.26">implementation</entity> of Korean <entity id="C96-2117.27">dialogue</entity> <entity id="C96-2117.28">processing</entity> and the <entity id="C96-2117.29">computation</entity> of social <entity id="C96-2117.30">status</entity> is made <entity id="C96-2117.31">based</entity> on ALE <entity id="C96-2117.32">system</entity>.
</abstract>

</text>

<text id="C96-2130">
<title>Learning Part-Of-Speech Guessing Rules From Lexicon: Extension To Non-Concatenative Operations</title>
<abstract>
One of the <entity id="C96-2130.1">problems</entity> in <entity id="C96-2130.2">part-of-speech</entity> <entity id="C96-2130.3">tagging</entity> of <entity id="C96-2130.4">real-word</entity> <entity id="C96-2130.5">texts</entity> is that of unknown to the <entity id="C96-2130.6">lexicon</entity> <entity id="C96-2130.7">words</entity>. In ( Mikheev, 1996 ), a <entity id="C96-2130.8">technique</entity> for fully unsupervised <entity id="C96-2130.9">statistical</entity> <entity id="C96-2130.10">acquisition</entity> of <entity id="C96-2130.11">rules</entity> which guess possible <entity id="C96-2130.12">parts-of-speech</entity> for <entity id="C96-2130.13">unknown words</entity> was <entity id="C96-2130.14">proposed</entity>. One of the <entity id="C96-2130.15">over-simplification</entity> assumed by this learning <entity id="C96-2130.16">technique</entity> was the <entity id="C96-2130.17">acquisition</entity> of morphological <entity id="C96-2130.18">rules</entity> which obey only <entity id="C96-2130.19">simple</entity> concatenative <entity id="C96-2130.20">regularities</entity> of the <entity id="C96-2130.21">main</entity> <entity id="C96-2130.22">word</entity> with an affix. In this <entity id="C96-2130.23">paper</entity> wc extend this <entity id="C96-2130.24">technique</entity> to the non-concatenative <entity id="C96-2130.25">cases</entity> of suffixation and assess the <entity id="C96-2130.26">gain</entity> in the <entity id="C96-2130.27">performance</entity>.
</abstract>

</text>

<text id="C88-2167">
<title>An Efficient Execution Method For Rule-Based Machine Translation</title>
<abstract>
A <entity id="C88-2167.1">rule</entity> <entity id="C88-2167.2">based</entity> <entity id="C88-2167.3">system</entity> is an effective way to <entity id="C88-2167.4">implement</entity> a <entity id="C88-2167.5">machine translation system</entity> because of its extensibility and maintainability. However, it is disadvantageous in <entity id="C88-2167.6">processing</entity> <entity id="C88-2167.7">efficiency</entity>. In a <entity id="C88-2167.8">rule</entity> <entity id="C88-2167.9">based</entity> <entity id="C88-2167.10">machine translation system</entity>, the grammar consists of a lot of rewriting <entity id="C88-2167.11">rules</entity>. While the <entity id="C88-2167.12">translation</entity> is carried out by repeating <entity id="C88-2167.13">pattern matching</entity> and <entity id="C88-2167.14">transformation</entity> of graph <entity id="C88-2167.15">structures</entity>, most <entity id="C88-2167.16">rules</entity> fail in <entity id="C88-2167.17">pattern matching</entity>. It is to be desired that <entity id="C88-2167.18">pattern matching</entity> of the unfruitful <entity id="C88-2167.19">rules</entity> should be avoided. This <entity id="C88-2167.20">paper</entity> <entity id="C88-2167.21">proposes</entity> a <entity id="C88-2167.22">method</entity> to restrict the <entity id="C88-2167.23">rule application</entity> by activating <entity id="C88-2167.24">rules</entity> dynamically. The logical <entity id="C88-2167.25">relationship</entity> among <entity id="C88-2167.26">rules</entity> are pre-analyzed and a set of antecedent <entity id="C88-2167.27">actions</entity>, which are prerequisite for the condition of the <entity id="C88-2167.28">rule</entity> being satisfied, is determined for each <entity id="C88-2167.29">rule</entity>. In <entity id="C88-2167.30">execution</entity> <entity id="C88-2167.31">time</entity>, a <entity id="C88-2167.32">rule</entity> is activated only when one of the antecedent <entity id="C88-2167.33">actions</entity> are carried out. The <entity id="C88-2167.34">probability</entity> of a <entity id="C88-2167.35">rule</entity> being activated is reduced to near the <entity id="C88-2167.36">occurrence</entity> <entity id="C88-2167.37">probability</entity> of its relevant linguistic <entity id="C88-2167.38">phenomenon</entity>. As most <entity id="C88-2167.39">rules</entity> relate to linguistic <entity id="C88-2167.40">phenomena</entity> that rarely occur, the <entity id="C88-2167.41">processing</entity> <entity id="C88-2167.42">efficiency</entity> is drastically <entity id="C88-2167.43">improved</entity>.
</abstract>

</text>

<text id="C92-1021">
<title>
Hopfield <entity id="C92-1021.1">Models</entity> As Nondeterministic Finite-State Machines</title>
<abstract>
The use of <entity id="C92-1021.2">neural networks</entity> for integrated <entity id="C92-1021.3">linguistic analysis</entity> may be profitable. This <entity id="C92-1021.4">paper</entity> presents the first <entity id="C92-1021.5">results</entity> of our <entity id="C92-1021.6">research</entity> on that subject: a <entity id="C92-1021.7">Hop-field</entity> <entity id="C92-1021.8">model</entity> for syntactical <entity id="C92-1021.9">analysis</entity>. We <entity id="C92-1021.10">construct</entity> a <entity id="C92-1021.11">neural network</entity> as an <entity id="C92-1021.12">implementation</entity> of a bounded push-down automaton, which can accept <entity id="C92-1021.13">context-free</entity> <entity id="C92-1021.14">languages</entity> with limited <entity id="C92-1021.15">center-embedding</entity>. The <entity id="C92-1021.16">network</entity>'s <entity id="C92-1021.17">behavior</entity> can be predicted a priori, so the presented <entity id="C92-1021.18">theory</entity> can be <entity id="C92-1021.19">tested</entity>. The <entity id="C92-1021.20">operation</entity> of the <entity id="C92-1021.21">network</entity> as an <entity id="C92-1021.22">implementation</entity> of the acceptor is prov-ably correct. Furthermore we found a <entity id="C92-1021.23">solution</entity> to the <entity id="C92-1021.24">problem</entity> of spurious states in Hopfield <entity id="C92-1021.25">models</entity>: we use them as dynamically <entity id="C92-1021.26">constructed</entity> <entity id="C92-1021.27">representations</entity> of sets of states of the <entity id="C92-1021.28">implemented</entity> acceptor. The so-called <entity id="C92-1021.29">neural-network</entity> acceptor we <entity id="C92-1021.30">propose</entity>, is fast but large.
</abstract>

</text>

<text id="C92-1041">
<title>Generation Of Accent In Nominally Premodified Noun Phrases</title>
<abstract>
The primary <entity id="C92-1041.1">purpose</entity> of this <entity id="C92-1041.2">paper</entity> is to present a set of conditions that constrain accent placement in <entity id="C92-1041.3">focused</entity> nominally premodified Selkirk (1984) argues that if the premodifier is an <entity id="C92-1041.4">argument</entity> of the head, then the head can be deaccented. I agree with Selkirk 's <entity id="C92-1041.5">proposal</entity> and argue that what essential is not whether the premodifier is a grammatical <entity id="C92-1041.6">argument</entity> of the head <entity id="C92-1041.7">noun</entity>, but rather, whether it is a 6-<entity id="C92-1041.8">complement</entity> in <entity id="C92-1041.9">lexical</entity> <entity id="C92-1041.10">conceptual structure</entity>. This <entity id="C92-1041.11">proposal</entity> is <entity id="C92-1041.12">evaluated</entity> by <entity id="C92-1041.13">testing</entity> it against a <entity id="C92-1041.14">corpus</entity> of naturally occurring<entity id="C92-1041.15">data</entity>.
</abstract>

</text>


</doc>